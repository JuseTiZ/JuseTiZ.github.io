<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Juse&#39;s Blog</title>
  
  <subtitle>积跬步 至千里</subtitle>
  <link href="https://biojuse.com/atom.xml" rel="self"/>
  
  <link href="https://biojuse.com/"/>
  <updated>2024-11-09T09:47:37.476Z</updated>
  <id>https://biojuse.com/</id>
  
  <author>
    <name>Juse</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于 JCVI 共线性分析确定一对一直系同源基因</title>
    <link href="https://biojuse.com/2024/11/09/%E5%9F%BA%E4%BA%8E%20JCVI%20%E5%85%B1%E7%BA%BF%E6%80%A7%E5%88%86%E6%9E%90%E7%A1%AE%E5%AE%9A%E4%B8%80%E5%AF%B9%E4%B8%80%E7%9B%B4%E7%B3%BB%E5%90%8C%E6%BA%90%E5%9F%BA%E5%9B%A0/"/>
    <id>https://biojuse.com/2024/11/09/%E5%9F%BA%E4%BA%8E%20JCVI%20%E5%85%B1%E7%BA%BF%E6%80%A7%E5%88%86%E6%9E%90%E7%A1%AE%E5%AE%9A%E4%B8%80%E5%AF%B9%E4%B8%80%E7%9B%B4%E7%B3%BB%E5%90%8C%E6%BA%90%E5%9F%BA%E5%9B%A0/</id>
    <published>2024-11-09T09:30:00.000Z</published>
    <updated>2024-11-09T09:47:37.476Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>前段时间因为课题需求，需要获取两个物种间的所有一对一直系同源基因对（One-to-One ortholog, 为避免文章内容冗余后面统称 OTO）。由于两个目标物种在 Ensembl 上都有，所以可以直接通过 BioMart 下载其基因的同源关系并提取出来。</p><p>但经过筛选和过滤后，发现 BioMart 中能确定的 OTO 数量极低，不太能满足分析需求。考虑到我所分析的目标基因类型在不同物种间具有高顺序保守性和序列保守性，因此想看看能不能通过 JCVI 的共线性结果来获取更多的 OTO 作为补充，发现效果还挺不错，固有此文。</p><h2 id="JCVI-介绍"><a href="#JCVI-介绍" class="headerlink" title="JCVI 介绍"></a>JCVI 介绍</h2><p>JCVI 是一个用于比较基因组学分析的多功能工具包，<strong>共线性分析只是它其中的一个子功能</strong>。</p><p>JCVI  的安装，最简单的方式是通过 <code>pip</code>：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jcvi</span><br></pre></td></tr></tbody></table></figure><p>也可通过 <code>conda</code> 或 <code>mamba</code> 安装：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install bioconda::jcvi</span><br></pre></td></tr></tbody></table></figure><p>此外也需要安装用于比对的 LAST：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install bioconda::last</span><br></pre></td></tr></tbody></table></figure><p>关于 JCVI 的运行，更多细节可见：<a href="https://github.com/tanghaibao/jcvi/wiki/MCscan-%28Python-version%29">JCVI tutorial</a></p><p>请注意，JCVI 的 wiki 写的并不全面，<strong>很多参数需要灵活调整以适配自身需求</strong>。本文所用到的文件（如注释等）均来自 Ensembl 数据库，如果使用其他来源的文件，请灵活调整参数（或调整源文件格式）以达成相同目的。</p><h2 id="JCVI-寻找-OTO-示例"><a href="#JCVI-寻找-OTO-示例" class="headerlink" title="JCVI 寻找 OTO 示例"></a>JCVI 寻找 OTO 示例</h2><p>本文使用到的文件下载（人和黑猩猩 Ensembl V110）：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget ftp://ftp.ensembl.org/pub/release-110/fasta/pan_troglodytes/cds/Pan_troglodytes.Pan_tro_3.0.cds.all.fa.gz</span><br><span class="line">wget ftp://ftp.ensembl.org/pub/release-110/gff3/pan_troglodytes/Pan_troglodytes.Pan_tro_3.0.110.gff3.gz</span><br><span class="line">wget ftp://ftp.ensembl.org/pub/release-110/fasta/homo_sapiens/cds/Homo_sapiens.GRCh38.cds.all.fa.gz</span><br><span class="line">wget ftp://ftp.ensembl.org/pub/release-110/gff3/homo_sapiens/Homo_sapiens.GRCh38.110.gff3.gz</span><br></pre></td></tr></tbody></table></figure><p>下文中的命令请根据自身情况修改相关输入和输出文件名，同时检查是否需要修改参数。</p><p>①、首先使用 <code>jcvi.formats.gff</code> 将其 <code>gff</code> 文件中特定的注释类型转为 <code>bed</code> 文件格式：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python -m jcvi.formats.gff bed --type=mRNA --key=transcript_id --primary_only Pan_troglodytes.Pan_tro_3.0.110.gff3.gz -o chimpanzee.bed</span><br><span class="line">python -m jcvi.formats.gff bed --type=mRNA --key=transcript_id --primary_only Homo_sapiens.GRCh38.110.gff3.gz -o human.bed</span><br><span class="line">python -m jcvi.formats.bed uniq chimpanzee.bed</span><br><span class="line">python -m jcvi.formats.bed uniq human.bed</span><br><span class="line">mv chimpanzee.uniq.bed chimpanzee.bed</span><br><span class="line">mv human.uniq.bed human.bed</span><br></pre></td></tr></tbody></table></figure><p>参数说明：</p><ul><li><code>--type</code>：指定想要提取的注释类型，可以通过逗号分隔同时提取多种信息。这里指定提取蛋白编码基因的 mRNA 序列区间范围。</li><li><code>--key</code>：指定提取时使用的标志信息。由于 Ensembl CDS 文件中序列以 transcipt id 开头，因此此处使用 <code>transcript_id</code> 以和 CDS 文件对应。</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Ensembl CDS 文件示例</span><br><span class="line">&gt;ENST00000633790.1 cds scaffold:GRCh38:HSCHR7_2_CTG6:503684:504150:1 gene:ENSG00000282748.1 gene_biotype:TR_V_gene transcript_biotype:TR_V_gene gene_symbol:TRBV5-7 description:T cell receptor beta variable 5-7 (non-functional) [Source:HGNC Symbol;Acc:HGNC:12224]</span><br><span class="line">ATGGGCCCCGGGCTCCTCTGCTGGGTGCTGCTTTGTCCCCTAGGAGAAGGCCCAGTGGAC</span><br><span class="line">GCTGGAGTCACCCAAAGTCCCACACACCTGATCAAAACGAGAGGACAGCACGTGACTCTG</span><br><span class="line">AGATGCTCTCCTATCTCTGGGCACACCAGTGTGTCCTCGTACCAACAGGCCCTGGGTCAG</span><br><span class="line">GGGCCCCAGTTTATCTTTCAGTATTATGAGAAAGAAGAGAGAGGAAGAGGAAACTTCCCT</span><br><span class="line">GATCAATTCTCAGGTCACCAGTTCCCTAACTATAGCTCTGAGCTGAATGTGAACGCCTTG</span><br><span class="line">TTGCTAGGGGACTCGGCCCTCTATCTCTGTGCCAGCAGCTTGG</span><br></pre></td></tr></tbody></table></figure><ul><li><code>python -m jcvi.formats.bed uniq</code>：此步将提取的注释中存在重叠的部分删除，最终保留更长的那一个转录本。</li><li><code>--primary</code>：此步仅保留每个 transcript id 下最长的 mRNA。<strong>该步骤和 uniq 步骤的目的都是为了尽可能去除冗余性以提高后续的共线性得分</strong>。</li></ul><p>②、对于 CDS，使用 <code>jcvi.formats.fasta</code> 将其 ID 处理为不带版本尾缀的格式：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m jcvi.formats.fasta format --sep=. Pan_troglodytes.Pan_tro_3.0.cds.all.fa.gz chimpanzee.cds</span><br><span class="line">python -m jcvi.formats.fasta format --sep=. Homo_sapiens.GRCh38.cds.all.fa.gz human.cds</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 处理前</span><br><span class="line">&gt;ENSPTRT00000098376.1</span><br><span class="line"># 处理后</span><br><span class="line">&gt;ENSPTRT00000098376</span><br></pre></td></tr></tbody></table></figure><p>进行该步骤的原因主要是 <code>--key</code> 提取的 bed 文件中 <code>transcript_id</code> 里并不携带 version，因此这里通过 <code>--seq=.</code> 去除 CDS  ID 中的 version 尾缀使 bed 文件和 cds 文件能够相吻合（<strong>在 Ensembl 同一版本的注释信息中不会出现一个 transcript id 同时具有多个 version 的情况</strong>）。</p><p>③、进行成对共线性搜索：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m jcvi.compara.catalog ortholog human chimpanzee --cscore=.99 --no_strip_names</span><br></pre></td></tr></tbody></table></figure><p>该步骤通过 LAST 进行比对，并通过 <code>--cscore=.99</code> 过滤 hit 以保留 reciprocal best hit (RBH)。</p><p>请注意 <code>jcvi.compara.catalog ortholog</code> 后紧接两个需要进行分析的物种文件名，此处应与前文输出的文件前缀对应，例如 human 对应 <code>human.bed</code> 及 <code>human.cds</code>。分析后输出的 <code>.anchors</code> 中即包含保留下的高质量共线性块。</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># anchors 文件内容</span><br><span class="line">###</span><br><span class="line">ENST00000616016 ENSPTRT00000090571      2030</span><br><span class="line">ENST00000338591 ENSPTRT00000103781      3000</span><br><span class="line">ENST00000428771 ENSPTRT00000073784      871</span><br><span class="line">ENST00000624697 ENSPTRT00000076196      742</span><br><span class="line">ENST00000453464 ENSPTRT00000098436      1010</span><br><span class="line">ENST00000379339 ENSPTRT00000106256      459</span><br></pre></td></tr></tbody></table></figure><p>④、筛选 OTO：</p><p><code>.anchors</code> 文件中绝大多数都为 one-to-one 的对应关系，但也<strong>依然存在少数为多对一或一对多</strong>，因此需要进行进一步的过滤，该步骤可以通过 Python 实现：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">jcvi_result_df = pd.read_csv(sys.argv[<span class="number">1</span>], sep=<span class="string">"\t"</span>, comment=<span class="string">"#"</span>, header=<span class="literal">None</span>)</span><br><span class="line">jcvi_result_df.columns = [<span class="string">"spe1"</span>, <span class="string">"spe2"</span>, <span class="string">"score"</span>]</span><br><span class="line"></span><br><span class="line">spe1_counts = jcvi_result_df[<span class="string">"spe1"</span>].value_counts()</span><br><span class="line">spe1_only_once = spe1_counts[spe1_counts == <span class="number">1</span>].index.to_list()</span><br><span class="line"></span><br><span class="line">spe2_counts = jcvi_result_df[<span class="string">"spe2"</span>].value_counts()</span><br><span class="line">spe2_only_once = spe2_counts[spe2_counts == <span class="number">1</span>].index.to_list()</span><br><span class="line"></span><br><span class="line">jcvi_result_filter_df = jcvi_result_df.loc[</span><br><span class="line">    (jcvi_result_df[<span class="string">"spe1"</span>].isin(spe1_only_once)) &amp;</span><br><span class="line">    (jcvi_result_df[<span class="string">"spe2"</span>].isin(spe2_only_once))</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">jcvi_result_filter_df[[<span class="string">"spe1"</span>, <span class="string">"spe2"</span>]].to_csv(<span class="string">f"<span class="subst">{sys.argv[<span class="number">1</span>]}</span>.one2one"</span>, sep=<span class="string">"\t"</span>, index=<span class="literal">False</span>, header=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><p>将上述代码保存到一个名为 <code>jcvi_filter.py</code> 的文件中，运行以下命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python jcvi_filter.py human.chimpanzee.anchors</span><br></pre></td></tr></tbody></table></figure><p>随后将生成一个名为 <code>human.chimpanzee.anchors.one2one</code> 的文件，其中每一行都为一个 human 和 chimpanzee 间的共线性一对一直系同源基因。</p><h2 id="其他需要注意的地方"><a href="#其他需要注意的地方" class="headerlink" title="其他需要注意的地方"></a>其他需要注意的地方</h2><p>由于 JCVI 除了考虑比对情况以外，也考虑了这些基因在基因组位置上的线性关系，因此使用 JCVI 有利于回收在两种物种基因组上<strong>位置顺序保守</strong>的直系同源基因对，但相对的也掩盖了一些在不同物种间位置不一致的直系同源关系。如果你并不关心基因位置信息并想要得到更多的 OTO，可以考虑直接使用 RBH 方法进行确定。</p></body></html>]]></content>
    
    
    <summary type="html">本文介绍了一种使用 JCVI 分析结果确定一对一直系同源基因的方法，以期通过这种方式寻找到更多的 One-to-One ortholog。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生信" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E4%BF%A1/"/>
    
    
  </entry>
  
  <entry>
    <title>获取 vcf 文件中每个个体携带的突变数量</title>
    <link href="https://biojuse.com/2024/10/10/%E8%8E%B7%E5%8F%96%20vcf%20%E6%96%87%E4%BB%B6%E4%B8%AD%E6%AF%8F%E4%B8%AA%E4%B8%AA%E4%BD%93%E6%90%BA%E5%B8%A6%E7%9A%84%E7%AA%81%E5%8F%98%E6%95%B0%E9%87%8F/"/>
    <id>https://biojuse.com/2024/10/10/%E8%8E%B7%E5%8F%96%20vcf%20%E6%96%87%E4%BB%B6%E4%B8%AD%E6%AF%8F%E4%B8%AA%E4%B8%AA%E4%BD%93%E6%90%BA%E5%B8%A6%E7%9A%84%E7%AA%81%E5%8F%98%E6%95%B0%E9%87%8F/</id>
    <published>2024-10-10T06:30:00.000Z</published>
    <updated>2024-10-10T06:29:49.056Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>最近的工作涉及到处理大量种群数据，其中有个分析需要统计每个个体携带的突变数量，在网上搜寻了诸多方法后确定了一些比较高效的策略，固有此文记录。</p><blockquote><p>本文的部分流程参考源：</p><p><a href="https://www.biostars.org/p/336206/">BioStars "how to count variants par sample per chromosome in a vcf file?"</a></p></blockquote><h2 id="提取每个个体携带的突变数量"><a href="#提取每个个体携带的突变数量" class="headerlink" title="提取每个个体携带的突变数量"></a>提取每个个体携带的突变数量</h2><p>该流程主要通过 plink2 实现：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plink2 --vcf [vcf file] --out [plink output1]</span><br><span class="line">plink2 --pfile [plink output1] --sample-counts --out [plink output2]</span><br></pre></td></tr></tbody></table></figure><ul><li><code>[vcf file]</code>：此处填入 vcf 文件的路径。</li><li><code>[plink output1]</code>：此处填入 plink2 输出的文件前缀。</li><li><code>[plink output2]</code>：此处填入 plink2 输出的文件前缀。</li></ul><p>运行结束后，可以得到以 <code>.scount</code> 结尾的文件，其中各列的含义如下：</p><ol><li><strong>#IID</strong>：标识每个样本的编号。</li><li><strong>HOM_REF_CT</strong>：纯合参考基因型计数，个体纯合参考基因型的位点数量。</li><li><strong>HOM_ALT_SNP_CT</strong>：纯合变异基因型计数，个体纯合变异基因型的单核苷酸多态性（SNP）数量。</li><li><strong>HET_SNP_CT</strong>：杂合基因型计数，个体在该位点上是杂合基因型的SNP数量（一条染色体是参考基因型，另一条是变异基因型）。</li><li><strong>DIPLOID_TRANSITION_CT</strong>：二倍体转换变异的数量（转换：嘌呤与嘌呤或嘧啶与嘧啶之间的替换）。</li><li><strong>DIPLOID_TRANSVERSION_CT</strong>：二倍体颠换变异的数量（颠换：嘌呤与嘧啶之间的替换）。</li><li><strong>DIPLOID_NONSNP_NONSYMBOLIC_CT</strong>：二倍体非SNP非符号变异计数。包括一些插入、缺失（INDELs）等。</li><li><strong>DIPLOID_SINGLETON_CT</strong>：二倍体单例变异计数，表示在整个样本集中只出现在该个体中的变异数量。</li><li><strong>HAP_REF_INCL_FEMALE_Y_CT</strong>：单倍体参考基因型计数，即在单倍体区域（Y 染色体）的参考等位基因数量。</li><li><strong>HAP_ALT_INCL_FEMALE_Y_CT</strong>：单倍体变异基因型计数，在单倍体区域中出现的变异等位基因的数量。</li><li><strong>MISSING_INCL_FEMALE_Y_CT</strong>：缺失基因型计数，在样本中缺失的基因型数据计数。</li></ol><p>更多可输出列的详细信息可见：<a href="https://www.cog-genomics.org/plink/2.0/formats#scount">https://www.cog-genomics.org/plink/2.0/formats#scount</a></p><p>具体参数的用法可见：<a href="https://www.cog-genomics.org/plink/2.0/basic_stats#sample_counts">https://www.cog-genomics.org/plink/2.0/basic_stats#sample_counts</a></p><p>通过 <code>HOM_ALT_SNP_CT</code> <code>HET_SNP_CT</code> <code>DIPLOID_NONSNP_NONSYMBOLIC_CT</code> 计算每个个体携带的突变数量。</p><h2 id="提取每个突变对应的携带者信息"><a href="#提取每个突变对应的携带者信息" class="headerlink" title="提取每个突变对应的携带者信息"></a>提取每个突变对应的携带者信息</h2><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bcftools norm -m -any [vcf file] | bcftools query -i 'GT="alt"' -f '%CHROM\t%POS\t%REF\t%ALT[\t%SAMPLE:%GT]\n' - &gt; [output]</span><br></pre></td></tr></tbody></table></figure><ul><li><code>[vcf file]</code>：此处填入 vcf 文件的路径。</li><li> <code>[output]</code>：此处填入输出文件的路径。</li></ul><p>此处通过 <code>bcftools norm -m -any</code> 对多等位基因进行了拆分，可视自身需求去除该部分，此处 <code>GT="alt"</code> 将所有携带替代等位基因的个体提取出并打印。</p><p>输出结果示例：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chr1    228424930       G       A       4922792:0/1     4884261:0/1     3988378:0/1     2479308:0/1     4414951:0/1     3862053:0/1</span><br></pre></td></tr></tbody></table></figure><p>前四列用于表示特定突变（分别指示染色体编号、位置、ref 和 alt），后续以制表符分隔记录该突变的携带者及基因型。</p></body></html>]]></content>
    
    
    <summary type="html">这篇文章介绍了如何获取 vcf 中每个个体携带的突变数量（及每个突变对应的携带者 id）。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="杂项" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E6%9D%82%E9%A1%B9/"/>
    
    
  </entry>
  
  <entry>
    <title>vscode 在远程服务器上通过 jupyter notebook 使用 R</title>
    <link href="https://biojuse.com/2024/09/19/vscode%20%E5%9C%A8%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E9%80%9A%E8%BF%87%20jupyter%20notebook%20%E4%BD%BF%E7%94%A8%20R/"/>
    <id>https://biojuse.com/2024/09/19/vscode%20%E5%9C%A8%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E9%80%9A%E8%BF%87%20jupyter%20notebook%20%E4%BD%BF%E7%94%A8%20R/</id>
    <published>2024-09-19T09:00:00.000Z</published>
    <updated>2024-09-20T02:18:17.541Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>最近一段时间的关键分析要用到 R，但是办公电脑的机械硬盘前段时间出了问题，现在用 Rstudio 会非常卡顿，加上相关分析对计算资源的要求也比较高，就想着是否有什么办法能在服务器上也弄一个类似于 Rstudio 的可视化界面出来作为辅助。</p><p>不过看了一圈发现 Rstudio 的 server 版本似乎局限性不少，而且还要用到 root 权限。考虑到平时运行 Python 进行分析时大多数时候都在 jupyter notebook 上进行，就想着是否能套用过来，于是就有了这篇文章。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>该文章的具体实现在 Linux + mamba 环境下完成，如果你没有 conda/mamba 环境，可以参考网上资料进行安装。</p><p>首先确认你的环境中有 R，如果没有可以通过以下命令安装：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mamba install conda-forge::r-base</span><br><span class="line">R # 确认可以进入 R 终端，使用 q() 退出</span><br></pre></td></tr></tbody></table></figure><p>通过 mamba 安装 <a href="https://github.com/IRkernel/IRkernel">IRkernel</a> 和 jupyter（使用 conda 也行，本文章以 mamba 为准）：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mamba install r::r-irkernel</span><br><span class="line">mamba install anaconda::jupyter</span><br></pre></td></tr></tbody></table></figure><p>完成上述步骤后，进入 R 终端，输入 <code>IRkernel::installspec()</code>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ R</span><br><span class="line">&gt; IRkernel::installspec()</span><br></pre></td></tr></tbody></table></figure><p>如果在 VScode 中尚未安装 jupyter 扩展，则点开侧栏的 <strong><u>拓展</u></strong> 页面并搜索 jupyter 安装：</p><img src="/pic2/vscode_jupyter1.png" height="400px"><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>新建一个 jupyter notebook 文件，使用 vscode 打开它：</p><img src="/pic2/vscode_jupyter2.png" height="300px"><p>点击右上角的 <code>select kernel</code>，此时如果仅存在 Python 相关的 kernel，则先在服务器上运行 jupyter notebook：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></tbody></table></figure><p>此时屏幕输出里会返回 server 的链接，例如：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:8888/tree?token=11d321403f5a4d62d94d07ddae56e48017cf641c059573b0</span><br></pre></td></tr></tbody></table></figure><p>在 <code>select kernel</code> 里，点击 <code>Select Another Kernel</code>，此后选择 <code>Existing Jupyter Server</code>，在弹出的窗口里输入上面得到的 server 链接，并选择弹出的 R。图示：</p><img src="/pic2/vscode_jupyter3.png" height="500px"><p>此后即可在 jupyter notebook 中使用 R 进行分析：</p><img src="/pic2/vscode_jupyter4.png" height="400px"></body></html>]]></content>
    
    
    <summary type="html">这篇文章介绍了 vscode 上如何在远程服务器中通过 jupyter notebook 使用 R（无需 root 权限），包括详细的图文步骤。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生信" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E4%BF%A1/"/>
    
    
  </entry>
  
  <entry>
    <title>簇突变的类型简介及识别方法（Clustered mutation classification）</title>
    <link href="https://biojuse.com/2024/08/23/%E7%B0%87%E7%AA%81%E5%8F%98%E7%9A%84%E7%B1%BB%E5%9E%8B%E7%AE%80%E4%BB%8B%E5%8F%8A%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95/"/>
    <id>https://biojuse.com/2024/08/23/%E7%B0%87%E7%AA%81%E5%8F%98%E7%9A%84%E7%B1%BB%E5%9E%8B%E7%AE%80%E4%BB%8B%E5%8F%8A%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95/</id>
    <published>2024-08-23T12:30:00.000Z</published>
    <updated>2024-08-23T12:29:12.593Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>基因组中，某些特殊的突变过程可能会导致一小段区域内连续的突变发生。这些突变在基因组中呈现成簇分布，因此被称作 <strong><u>clustered mutation</u><strong>（本文将称其为</strong>簇突变</strong>）。研究这些突变的分布及组成有助于我们揭示导致其发生的内源性和外源性过程。该文章将主要讲述簇突变的类别并列举一些会导致特定类别的生物学原因，同时介绍该领域中的某些生物信息学工具等。</p><h2 id="簇突变类型"><a href="#簇突变类型" class="headerlink" title="簇突变类型"></a>簇突变类型</h2><p>簇突变类型主要被分作六类，其中五类针对碱基水平的变化，一类针对插入及缺失（InDel）：</p><ul><li><p><strong>DSB（Doublet Base Substitutions）</strong>：双碱基替换，表示这两个突变<strong>发生在相邻的碱基上</strong>。某些外源性过程例如紫外线损伤会导致双碱基替换的发生（CC&gt;TT），此外 DNA 修复缺陷和聚合酶功能突变也会导致 DSB 发生。</p></li><li><p><strong>MBS (Multiple Base Substitutions)<strong>：多碱基替换，表示在</strong>很短的序列范围内发生多个碱基突变且这些突变彼此相邻</strong>。由于该簇突变的出现数量很有限，因此尚未得到全面研究。</p></li><li><p><strong>Omikli</strong>：源自希腊语，意为 <u>雾</u> 或 <u>薄雾</u>，也被称作 <strong>diffuse hypermutation（弥漫性超突变）</strong>，表现为 IMD（Inter-Mutation Distance，突变间距）小于预期的少数几个碱基突变（两到三个）。多种外源过程可导致 Omikli 事件，内源过程则有单链错配修复等。</p></li><li><p><strong>Kataegis</strong>：源自希腊语，意为 <u>暴雨</u>。其本质上是 <strong>Omikli 的更大规模事件</strong>。表现为 IMD 小于预期的多个碱基突变聚集在一起。当前认为大多数导致 Kataegis 的过程都归因于 <a href="https://en.wikipedia.org/wiki/APOBEC">AID/APOBEC 家族</a>（Omikli 也受部分影响），与双链断裂具有一定关系。</p></li><li><p><strong>Other：</strong>IMD 小于预期但是 <strong>VAF（Variant allele frequency，突变等位基因频率）不相等的一些碱基突变聚集在一起</strong>。而<strong>上述几种突变类型中，一个 clustered mutation group 里所包含的突变 VAF 都相同</strong>。</p><ul><li><blockquote><p>如果簇突变在同一个事件中发生（也可以当作在同一个细胞中发生），那么这些突变的细胞谱系树将完全相同，因此最后得到的 VAF 预期一致。不同则表明这些突变是在多次突变事件中独立发生的。<strong>Other 类型的突变可能表明其所处的基因组区域本身突变率高</strong>，所以严格意义上讲它并不算主流研究中关注的簇突变。</p></blockquote></li></ul></li><li><p><strong>indels</strong>：IMD 小于预期的插入或缺失事件。与微卫星不稳定性有关并常见于错配修复缺陷的细胞中。</p></li></ul><img src="/pic2/clustered_mutation.jpeg" height="500px"><center>Refer: https://osf.io/qpmzw/wiki/2.%20Workflow/</center><h2 id="簇突变的确定"><a href="#簇突变的确定" class="headerlink" title="簇突变的确定"></a>簇突变的确定</h2><p>一般而言，确定簇突变的步骤如下：</p><ol><li>计算一个 IMD 阈值。</li><li>对基因组上的突变进行判断，如果存在与其距离低于 IMD 阈值的突变，则认为其是簇突变。</li></ol><p>在一些比较久之前的文章中，IMD 的阈值是直接人为确定的。例如 <a href="https://www.cell.com/cell/fulltext/S0092-8674(17)30774-2">Supek 等人</a>采用了以下判断标准：</p><p>①、突变之间的距离 &lt;= 500bp。</p><p>②、突变的类型必须相同（例如都为 C&gt;T）且链对应（例如 C 都在正链上）。</p><p>这种划分方法虽然在一定程度上可靠，但是并不适用于所有应用场景，且不同研究采取的标准也不完全相同（例如阈值大小），导致不方便整合比较。</p><p>随着期刊对分析的可靠性要求越来越严格，现在已经有许多用于分析簇突变的生物信息学工具被开发出来，它们采用了更加严谨的策略并且保证了可再现性，例如突变特征分析领域的主流工具套件 SigProfiler 中也提供了用于分析簇突变的 <a href="https://osf.io/qpmzw/wiki/home/">SigProfilerClusters</a>。其运行过程大致如下：</p><p>①、根据突变的上下文（周边序列）等信息，对突变的位置进行打乱重排，以模拟随机情况下的突变分布情况。</p><p>②、根据真实数据与模拟数据的突变分布计算出每个样本的 IMD 阈值。</p><p>③、根据基因组各个窗口内的突变率对 IMD 阈值进行校正。</p><p>④、根据簇突变的 VAF 等信息对其进行分类。</p><p>该团队使用该工具对 PCAWG 项目的数据进行了分析，系统阐述了各癌症类型中的簇突变景观，并发现了 APOBEC3 Kataegis 对 Extrachromosomal DNA (ecDNA) 的作用（<a href="https://www.nature.com/articles/s41586-022-04398-6#Sec8">Bergstrom et al. 2022, Nature</a>）。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li>常见的 DSB 特征及生物学成因：<a href="https://cancer.sanger.ac.uk/signatures/dbs/">https://cancer.sanger.ac.uk/signatures/dbs/</a></li><li>Clustered Mutation Signatures Reveal that Error-Prone DNA Repair Targets Mutations to Active Genes, Cell 2017</li><li>Examining clustered somatic mutations with SigProfilerClusters, Bioinformatics 2022</li><li>Mapping clustered mutations in cancer reveals APOBEC3 mutagenesis of ecDNA, Nature 2022</li></ol></body></html>]]></content>
    
    
    <summary type="html">这篇文章详细介绍了 clustered mutation（簇突变）的各个类型及其划分标准，并描述了用于确定簇突变的方法。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="杂项" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E6%9D%82%E9%A1%B9/"/>
    
    
  </entry>
  
  <entry>
    <title>rsync 中排除文件的规则详解（rsync --exclude）</title>
    <link href="https://biojuse.com/2024/08/15/rsync%20%E4%B8%AD%E6%8E%92%E9%99%A4%E6%96%87%E4%BB%B6%E7%9A%84%E8%A7%84%E5%88%99%E8%AF%A6%E8%A7%A3/"/>
    <id>https://biojuse.com/2024/08/15/rsync%20%E4%B8%AD%E6%8E%92%E9%99%A4%E6%96%87%E4%BB%B6%E7%9A%84%E8%A7%84%E5%88%99%E8%AF%A6%E8%A7%A3/</id>
    <published>2024-08-15T08:50:00.000Z</published>
    <updated>2024-08-17T13:13:22.167Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>前段时间在备份服务器时，由于备份策略的变化，对 rsync 的 exclude patterns 进行了一些更加深入的探索。</p><p>这篇文章将介绍如何设计 rsync 的 exclude patterns 以排除特定目录或文件。</p><h2 id="命令介绍"><a href="#命令介绍" class="headerlink" title="命令介绍"></a>命令介绍</h2><p>rsync 是 Linux 和 Unix 中常用的文件复制和同步命令，相比 cp 命令而言更加高效和完善，其优点包括但不限于：</p><ul><li><strong>增量复制</strong>：rsync 只会复制那些自上次同步后更改的文件部分，极大地减少了数据传输量。例如 a 文件已经同步过一次并且没有发生变化，则 rsync 会跳过。同样，rsync 也会进行完整性检查以确定传输的数据无误。</li><li><strong>保留属性</strong>：rsync 可以保持文件的权限、时间戳、软硬链接、所有权等属性。</li><li><strong>灵活的过滤规则</strong>：支持使用包括和排除规则来选择同步的文件或目录。</li></ul><img src="https://tse3-mm.cn.bing.net/th/id/OIP-C.m0CLH5eqL6kvqiFjEVu6VQHaD4?rs=1&amp;pid=ImgDetMain" height="200px"><h2 id="排除规则"><a href="#排除规则" class="headerlink" title="排除规则"></a>排除规则</h2><p>rsync 通过参数 <code>--exclude</code> 或 <code>--exclude-from</code> 指定排除的文件模式。其中 <code>--exclude-from</code> 用于指定包含排除模式的文件（包含多个排除规则）。</p><h3 id="排除指定文件"><a href="#排除指定文件" class="headerlink" title="排除指定文件"></a>排除指定文件</h3><p>输入文件名称（如 <code>123.txt</code>）即可排除所有命名为 <code>123.txt</code> 的文件。</p><h3 id="排除特定尾缀的文件"><a href="#排除特定尾缀的文件" class="headerlink" title="排除特定尾缀的文件"></a><strong>排除特定尾缀的文件</strong></h3><p>通过 <code>*.xxx</code> 排除包含特定尾缀的文件，例如：</p><ul><li><code>*.txt</code> 排除所有以 <code>.txt</code> 结尾的文件。</li><li><code>*vcf</code> 排除所有以 <code>vcf</code> 结尾的文件。注意这里<strong>并不推荐这样做</strong>，因为这<strong>也会排除掉所有以 vcf 结尾的目录</strong>，例如：<ul><li><code>source/dir1/dir_vcf/xxx</code>，这里 <code>dir_vcf</code> 及其中的所有文件也会被排除。</li></ul></li></ul><p>因此在根据尾缀排除文件时，最好添加尾缀前的 <code>.</code> 以避免其他文件夹的意外排除。</p><p>同理也可通过前缀进行文件排除，例如 <code>SRR*</code> 等，但同样地，这会排除符合该条件的所有目录。因此请<strong>谨慎使用</strong>。</p><h3 id="排除隐藏文件及文件夹"><a href="#排除隐藏文件及文件夹" class="headerlink" title="排除隐藏文件及文件夹"></a>排除隐藏文件及文件夹</h3><p>通过 <code>.*</code> <strong>排除所有隐藏文件及隐藏文件夹</strong>。</p><h3 id="排除特定文件夹"><a href="#排除特定文件夹" class="headerlink" title="排除特定文件夹"></a>排除特定文件夹</h3><p>直接使用文件夹名称，例如 <code>anaconda3</code> 等。但注意，<strong>如果有与该文件夹名同名的文件，那么该文件也会被排除</strong>，例如：</p><ul><li>使用 <code>juse</code> 时，<code>source/juse/</code>（目录）会被排除，此外 <code>source/other/juse</code>（文件名为 <code>juse</code>）也会被排除。</li></ul><p>因此如果你想排除特定文件夹但又担心具有同名文件，两个好的权衡方式是：</p><ul><li>添加该目录先前的信息，例如你已经知道其位于其他文件夹中：<code>dir1/dir2</code>（排除位于 dir1 下的 dir2）。</li><li>排除该目录中的所有文件，并保留空文件夹：<code>dir2/**</code>，该做法更加推荐，因为你也可以保留下 ‘哪些文件夹被排除’ 这个信息。</li></ul><p>这里使用两个星号是为了和之后的跨多目录匹配相一致，但其实只用一个 <code>*</code> 的效果是一样的。</p><h3 id="跨目录排除匹配的文件夹"><a href="#跨目录排除匹配的文件夹" class="headerlink" title="跨目录排除匹配的文件夹"></a>跨目录排除匹配的文件夹</h3><p>该方法的应用场景如下：</p><p>用户 <code>juse</code> 的目录里存在多个 <code>work</code> 文件夹，<code>juse</code> 想将它们全部排除掉，但是他认为仅使用 <code>work/**</code> 并不妥当，因为其他人的 <code>work</code> 文件夹中可能存在需要备份的文件，而根据单个目录路径进行排除也存在困难，因为 <code>juse</code> 目录中的 <code>work</code> 文件夹很多且位置不一。</p><ul><li><code>/juse/dir1/work/</code></li><li><code>/juse/work/</code></li><li><code>/juse/dir2/dir3/work/</code></li></ul><p>此时可以通过使用 <code>juse**/work</code> 排除 <code>juse</code> 目录下所有 <code>work</code> 子目录。请注意<strong>仅使用单个 <code>*</code> 时无法实现跨目录匹配</strong>：</p><ul><li><code>juse/*/work</code> 仅能识别到 <code>/juse/dir1/work/</code>，无法识别 <code>/juse/dir2/dir3/work/</code>。</li></ul><p>而使用 <code>juse/**/work</code> 则无法识别到 <code>juse</code> 目录下的 <code>work</code> 文件夹，可以通过去除第一个斜杠避免该问题。</p><p>相比之下 du 命令可以通过单个星号实现跨多个目录匹配，同时两个星号也不影响 du 的使用。</p><h2 id="rsync-使用建议"><a href="#rsync-使用建议" class="headerlink" title="rsync 使用建议"></a>rsync 使用建议</h2><p>首先需要明确一点，rsync <strong>以 source 目录作为根目录进行同步操作，因此并不会因为先前的绝对路径中存在排除模式就导致同步失败</strong>，例如：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync -av --exclude 'juse' /juse/dir1/ /pathto/destination/</span><br></pre></td></tr></tbody></table></figure><p>上述命令并不会导致 <code>/juse/dir1/</code> 中的文件全部无法同步，因为其并不会考虑源目录本身的路径信息。</p><p>此外，在使用 rsync 推荐使用以下一些参数：</p><ul><li><code>-av</code>：归档模式 + 输出更多信息，前者保证 rsync 递归复制目录并保留文件的各种属性，后者有助于监控同步过程。</li><li><code>-z</code>：在数据传输过程中进行压缩，在慢速网络中进行同步时建议启用。</li><li><code>--bwlimit</code>：限制带宽（传输速度），避免影响其他进程。</li><li><code>--delete</code> &amp; <code>--delete-excluded</code>：前者删除目标文件夹中源文件夹里不存在的文件，后者删除目标文件夹中被排除模式匹配的文件，适合需要完全同步时使用。</li></ul><p>注意 rsync 可以通过 <code>--exclude-from</code> 使用多个排除模式。</p></body></html>]]></content>
    
    
    <summary type="html">这篇文章详细介绍了如何设置 rsync 中的 exclude patterns，并说明了在实际应用过程中的一些可选参数。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="杂项" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E6%9D%82%E9%A1%B9/"/>
    
    
  </entry>
  
  <entry>
    <title>Snakemake pipeline 搭建的进阶教程</title>
    <link href="https://biojuse.com/2024/07/06/Snakemake%20pipeline%20%E6%90%AD%E5%BB%BA%E7%9A%84%E8%BF%9B%E9%98%B6%E6%95%99%E7%A8%8B/"/>
    <id>https://biojuse.com/2024/07/06/Snakemake%20pipeline%20%E6%90%AD%E5%BB%BA%E7%9A%84%E8%BF%9B%E9%98%B6%E6%95%99%E7%A8%8B/</id>
    <published>2024-07-06T13:05:00.000Z</published>
    <updated>2024-07-09T04:51:15.393Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><h2 id="关于-Snakemake"><a href="#关于-Snakemake" class="headerlink" title="关于 Snakemake"></a>关于 Snakemake</h2><p>Snakemake 是一个工作流管理系统，可用于创建易于迁移和复现的数据分析流程（pipeline），因此被广泛应用于生物学分析中。虽然 Snakemake 算是一个独立的编程语言，但其本质是基于 Python 搭建的，因此如果你对 Python 有一定了解，那么上手 Snakemake 就会更容易很多。并且 Python 技能的掌握对于某些情况下 Pipeline 下游分析部分的拓展是有必要的。</p><img src="/pic2/snakemake1.png"><p>本文将结合多个近些年生物论文中公开的 Snakemake Pipeline，介绍如何搭建一个具有一定功能的 Pipeline。</p><p>Snakemake 安装：<a href="https://snakemake.readthedocs.io/en/stable/getting_started/installation.html">https://snakemake.readthedocs.io/en/stable/getting_started/installation.html</a></p><p>Snakemake Pipeline 示例教程：<a href="https://snakemake.readthedocs.io/en/stable/tutorial/basics.html">https://snakemake.readthedocs.io/en/stable/tutorial/basics.html</a></p><p>有利于提升阅读体验的一些条件：</p><ul><li>对于 Python 编程已经具备一定的了解，包括文件的一些基本读写操作和 lambda 表达式的应用。</li><li>具有初步的 Snakemake Pipeline 制作经验。</li></ul><p>阅读后有望获得的一些能力：</p><ul><li>知晓 Snakemake 中一些基本的语法和操作。</li><li>编写具有明确结构的 Snakemake Pipeline。</li></ul><h2 id="搭建-Pipeline"><a href="#搭建-Pipeline" class="headerlink" title="搭建 Pipeline"></a>搭建 Pipeline</h2><p>注意，该 Pipeline 的完整版本在以下 github repository 的 <a href="https://github.com/JuseTiZ/Blog-Snakemake-pipeline/tree/main/Tutorial-Pipeline">Tutorial-Pipeline</a> 可见，建议结合完整 Pipeline 阅读文章以避免不必要的混淆：</p><p><a href="https://github.com/JuseTiZ/Blog-Snakemake-pipeline">https://github.com/JuseTiZ/Blog-Snakemake-pipeline</a></p><p>该 Pipeline 的运行环境也可通过以上 repository 中的指引进行安装和配置。</p><h3 id="Pipeline-要求"><a href="#Pipeline-要求" class="headerlink" title="Pipeline 要求"></a>Pipeline 要求</h3><p><strong>目标：</strong>搭建一个可以根据 SRR accession number 自动下载数据、处理数据和进行表达定量的 Pipeline。</p><p><strong>流程：</strong></p><ol><li><code>prefetch</code> 下载数据，该步骤还需配置 <code>pigz</code>（多线程压缩）及 <code>fasterq-dump</code>（提取 fastq）。</li><li><code>trim_galore</code> 过滤数据</li><li><code>salmon</code> 进行定量</li></ol><p><strong>功能：</strong>①、Pipeline 可以识别来自不同样本的数据，并把同一样本的多个数据进行合并；②、Pipeline 可以分为单端和双端模式进行处理；③、可以自定义各个软件运行时使用的参数；④、下载数据时<strong>如果遇到网络错误应当可以自行重续下载</strong>。</p><p><strong>需要提前准备的：</strong>①、指定自行构建的 salmon 索引；②、已安装好的并放置于环境变量中的软件。</p><p>如果你想将软件的版本控制也考虑进 Pipeline 中，可见本文 <strong><u>Pipeline 拓展</u></strong> 部分。</p><p>你可以通过下述命令进行 hg38 salmon 索引的构建以在后文中使用，或者你可以在阅读本文后将该步骤嵌入 Pipeline 流程中（但并不建议，因为该步骤并不是每一次分析都需要复用，这种情况下最好作为参数指定）：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir hg38_salmon_index</span><br><span class="line">wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_44/gencode.v44.pc_transcripts.fa.gz</span><br><span class="line">wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_44/GRCh38.primary_assembly.genome.fa.gz</span><br><span class="line">grep "^&gt;" &lt;(gunzip -c GRCh38.primary_assembly.genome.fa.gz) | cut -d " " -f 1 &gt; decoys.txt</span><br><span class="line">sed -i.bak -e 's/&gt;//g' decoys.txt</span><br><span class="line">cat gencode.v44.pc_transcripts.fa.gz GRCh38.primary_assembly.genome.fa.gz &gt; GRCh38.gentrome.fa.gz</span><br><span class="line">salmon index -t GRCh38.gentrome.fa.gz -d decoys.txt -p 12 -i hg38_salmon_index --gencode &gt; hg38_salmon_index/salmon_index.log 2&gt;&amp;1</span><br></pre></td></tr></tbody></table></figure><h3 id="Pipeline-管理"><a href="#Pipeline-管理" class="headerlink" title="Pipeline 管理"></a>Pipeline 管理</h3><p>首先，一个清晰的文件夹结构对于 Pipeline 的管理和拓展是有帮助的，因此请 <code>cd</code> 到拟存储 Pipeline 的路径，并输入以下命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd [path]</span><br><span class="line">tutorial_path=$(pwd)</span><br><span class="line">mkdir rules</span><br><span class="line">mkdir config</span><br><span class="line">touch config/config.yaml</span><br><span class="line">mkdir envs</span><br><span class="line">mkdir logs</span><br><span class="line">touch Snakefile</span><br></pre></td></tr></tbody></table></figure><p>这些文件夹的用途将在后续搭建过程中一一介绍。</p><img src="/pic2/snakemake2.png" style="zoom:80%;"><h3 id="Pipeline-搭建"><a href="#Pipeline-搭建" class="headerlink" title="Pipeline 搭建"></a>Pipeline 搭建</h3><p>阅读时请留意每个代码块指定的文件，文件头中的 <code>#</code> 仅作文件名说明用，在实际文件中请将该标头去除。</p><h4 id="1、输出目标文件确定"><a href="#1、输出目标文件确定" class="headerlink" title="1、输出目标文件确定"></a>1、输出目标文件确定</h4><p>搭建一个 Pipeline 具有一定的前置条件，首先<strong>你需要对每一个步骤会产生什么样的文件有具体的了解</strong>，因此<strong>建议完全跑过一套流程以后再进行这一步骤</strong>。</p><p>按照上述目标，我们首先希望 Pipeline 能够做到以下事情：</p><p>①、下载 SRR id 对应的原始数据。</p><p>②、经过一系列处理以后，将这些数据在样本层级进行定量。</p><p>已知第一步我们会得到一些以 <code>.sra</code> 结尾的文件，而完成 ② 后我们会得到所有样本的 <code>quant.sf</code> 文件，这些将决定我们如何<strong>定义 Snakemake 的输出目标</strong>。</p><p>因此我们先创建一个包含我们定量所用 sra id 及其对应的样本名的文件，命名为 <code>group.txt</code>，以下分析将以一个人类海拉细胞示例数据集为例进行：    </p><figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># group.txt</span><br><span class="line">SRR25601734HeLa_Rep1</span><br><span class="line">SRR25601735HeLa_Rep1</span><br><span class="line">SRR25601736HeLa_Rep2</span><br><span class="line">SRR25601737HeLa_Rep2</span><br></pre></td></tr></tbody></table></figure><p>假设每个 sra 文件我们打算放置于 <code>rawfastq/[srrid]</code> 中，样本的定量结果我们打算放置于 <code>results/[sample]</code> 中，那我们预期得到的文件应该有：</p><figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rawfastq/SRR25601734/SRR25601734.sra</span><br><span class="line">rawfastq/SRR25601735/SRR25601735.sra</span><br><span class="line">rawfastq/SRR25601736/SRR25601736.sra</span><br><span class="line">rawfastq/SRR25601737/SRR25601737.sra</span><br><span class="line">results/HeLa_Rep1/quant.sf</span><br><span class="line">results/HeLa_Rep2/quant.sf</span><br></pre></td></tr></tbody></table></figure><p>首先，我们需要一个方法指定 <code>group.txt</code> 能让 Snakemake 识别，请在 <code>config/config.yaml</code> 文件中添加以下内容：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">inputfile:</span></span><br><span class="line">  <span class="attr">groupfile:</span> <span class="string">"group.txt"</span></span><br></pre></td></tr></tbody></table></figure><p>这是一个 yaml 数据格式的文件，我们将在 Snakefile 中读取它并确定相关参数。在<strong>之后每次 Pipeline 的复用中一般只需要调整该文件</strong>。</p><p>现在，请打开之前新建的 <code>Snakefile</code> 文件，输入以下内容：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">configfile: <span class="string">"config/config.yaml"</span></span><br><span class="line"></span><br><span class="line">srrlist = <span class="built_in">set</span>([line.strip().split()[<span class="number">0</span>] <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(config[<span class="string">"inputfile"</span>][<span class="string">"groupfile"</span>], <span class="string">'r'</span>)])</span><br><span class="line">samples = <span class="built_in">set</span>([line.strip().split()[<span class="number">1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(config[<span class="string">"inputfile"</span>][<span class="string">"groupfile"</span>], <span class="string">'r'</span>)])</span><br><span class="line"></span><br><span class="line">rule <span class="built_in">all</span>:</span><br><span class="line">    <span class="built_in">input</span>:</span><br><span class="line">        expand(<span class="string">"rawfastq/{srr}/{srr}.sra"</span>, srr=srrlist) + </span><br><span class="line">        expand(<span class="string">"results/{sample}/quant.sf"</span>, sample=samples)</span><br></pre></td></tr></tbody></table></figure><p>逐段解释：</p><ol><li><p>首先我们通过 <code>configfile</code> <strong>指定配置文件</strong>，该命令行会将 <code>config/config.yaml</code> 读取为字典并赋予给 <code>config</code> 变量。</p></li><li><p>此后我们通过 Python 语法读取了先前在 config.yaml 中指定的 <code>group.txt</code>，并分别将 SRR id 和 Sample 的集合传入给了 <code>srrlist</code> 和 <code>samples</code> 两个变量。</p></li><li><p>通过 <code>rule all</code> 语句，<strong>确定 Snakemake 最终检查的文件（用于判断该 workflow 是否运行完成）</strong>，其中：</p><ul><li><p><code>expand</code> 为 Snakemake 的特殊语法，本质是遍历并得到一个新列表，即 <code>expand("{x}", x=xlist)</code> 功能上等价于 <code>[f"{x}" for x in xlist]</code>。</p></li><li><p>此处使用 <code>srrlist</code> 和 <code>samples</code> 通过 <code>expand</code> 语法将先前预期得到的文件列表传递给了 Snakemake，以使其监控工作流的运行结果。</p></li></ul></li></ol><p>这里需要注意的是，虽然我们最终的目的仅是得到每个样本的 <code>quant.sf</code>，但是由于后续 Snakemake 的运行中，其是<strong>通过 <code>rule all</code> 中的 <code>input</code> 为不同 rule 的 input 分配通配符的</strong>，因此<strong>如果不在 input 中给定 srr 文件相关的信息，它无法判断后续我们每个 rule 运行的意图</strong>，具体可见后文介绍。</p><h4 id="2、工作流框架实现"><a href="#2、工作流框架实现" class="headerlink" title="2、工作流框架实现"></a>2、工作流框架实现</h4><p>到这里，我们已经为工作流指定了最终的目标文件，现在我们可以开始搭建得到这一系列文件所需要的框架，接下来的步骤将主要在 <code>rule</code> 文件夹下进行，并涉及到一些 <code>config.yaml</code> 中参数的补充。</p><p>由于我们想要开发的 Pipeline <strong>能够根据单端和双端数据进行调整</strong>，因此我们现在 <code>config.yaml</code> 文件中添加一个新的参数：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mode:</span> <span class="string">"paired"</span></span><br></pre></td></tr></tbody></table></figure><p>该 mode 参数也可改为 single，后续我们框架中每个 rule 的行为将根据该 mode 的指定发生变化。</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $tutorial_path/rules</span><br><span class="line">touch common.smk # 该文件将用于编写 Python 函数以拓展后续 rule 的动态性</span><br></pre></td></tr></tbody></table></figure><p>根据先前拟定的 Pipeline 流程，我们首先创建一个下载规则文件：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch 01_prefetch.smk</span><br></pre></td></tr></tbody></table></figure><p>下载部分中我们可能需要调整的参数：</p><ol><li>可以同时并行的下载任务数量。该点将通过运行 Snakemake 时指定 <code>--resources</code> 实现。</li><li>下载如果失败（因为网络原因）重试的次数。该点将通过在 <code>config.yaml</code> 中添加参数实现。</li></ol><p><code>config.yaml</code> 新添参数以让下载失败时重新运行，最多运行三次：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">download:</span></span><br><span class="line">  <span class="attr">retry:</span> <span class="number">3</span></span><br></pre></td></tr></tbody></table></figure><p><code>01_prefetch.smk</code> 内容：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">rule download:</span><br><span class="line">    output:</span><br><span class="line">        srafile = <span class="string">"rawfastq/{srr}/{srr}.sra"</span></span><br><span class="line">    log:</span><br><span class="line">        <span class="string">"logs/{srr}_download_prefetch.log"</span></span><br><span class="line">    params:</span><br><span class="line">        srrid = <span class="string">"{srr}"</span></span><br><span class="line">    threads: <span class="number">1</span></span><br><span class="line">    retries: config[<span class="string">"download"</span>][<span class="string">"retry"</span>]</span><br><span class="line">    resources:</span><br><span class="line">        download_slots = <span class="number">1</span> </span><br><span class="line">    shell:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        if [ -e {output.srafile}.lock ]; then</span></span><br><span class="line"><span class="string">            rm {output.srafile}.lock</span></span><br><span class="line"><span class="string">            echo "{output.srafile} lock found but no output file! Deleting..." &gt;&gt; {log}</span></span><br><span class="line"><span class="string">        fi</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        prefetch --max-size 100000000 --progress --output-directory rawfastq {params.srrid} &gt; {log} 2&gt;&amp;1</span></span><br><span class="line"><span class="string">        if [ -e {output.srafile} ]; then</span></span><br><span class="line"><span class="string">            echo "{output.srafile} download finished!" &gt;&gt; {log}</span></span><br><span class="line"><span class="string">        else</span></span><br><span class="line"><span class="string">            mv {output.srafile}* {output.srafile}</span></span><br><span class="line"><span class="string">            echo "{output.srafile} not find! May end with .sralite. Renaming..." &gt;&gt; {log}</span></span><br><span class="line"><span class="string">        fi</span></span><br><span class="line"><span class="string">        """</span></span><br></pre></td></tr></tbody></table></figure><p>逐段解释：</p><ol><li><code>output</code> 段用于<strong>定义该 rule 用于产生什么文件</strong>，Snakemake 会自行创建相应目录（如果不存在）。运行完该 rule 后 Snakemake 会检查是否有产生 <code>output</code> 中指定的输出文件，<strong>如果没有则判断其运行失败</strong>。上例中该段指定了一个输出文件 <code>srafile</code>，其中通配符 <code>{srr}</code> 的识别方式可见下文。</li><li><code>log</code> 段用于<strong>定义该 rule 的日志文件</strong>，可以在相应命令行中将屏幕输出记录在该文件中。<code>params</code> 段用于<strong>定义该 rule 使用的参数</strong>。<code>threads</code> 段用于<strong>定义该 rule 使用的线程数（核数）</strong>。</li><li><code>retries</code> 段用于<strong>定义该 rule 失败后重新运行的次数</strong>，如果不指定则该 rule 失败后即会停止 Pipeline 的运行（正在进行的任务不会中断）。可以在 snakemake 运行时通过 <code>--keep-going</code> 使工作流继续运行其他独立的任务。</li><li><code>resources</code> 段用于<strong>定义该 rule 所占用的资源</strong>，可以在 snakemake 运行时通过 <code>--resources</code> 指定资源总量。以该 rule 为例，每个下载任务在 rule 中指定会占用 1 个下载槽（download_slots），因此可以通过 <code>--resources download_slots=4</code> 分配四个下载槽，使下载任务最多同时进行四个。</li><li><code>shell</code> 段用于<strong>定义该 rule 执行的 shell 命令行</strong>，内容解释如下：<ul><li><code>{output.srafile}</code> 将替换为 output 段的 srafile 字段。首先，让其检查是否存在 <code>.lock</code> 文件（<code>prefetch</code> 在下载过程中会产生的文件，如果不移除则无法重续下载），这一步是<strong>为失败以后重试而准备的</strong>。此后通过 <code>prefetch</code> 下载，将日志输出到 <code>{log}</code> 中。下载结束后进行检查，将后缀统一为 <code>.sra</code>（部分 sra 文件以 <code>.sralite</code> 结尾）。</li></ul></li></ol><p>对于一个 rule 而言，最少仅需要 <code>output</code> 和指令（如 <code>shell</code>、<code>script</code>、<code>run</code>）部分，其他都是可选字段（增强 rule 的功能和可管理性）。</p><p>下载结束后就是我们还需要进行处理，即将 <code>sra</code> 文件提取为 fastq 文件（使用 <code>fasterq-dump</code>），考虑到这也属于原始数据的处理，因此新的规则可以继续添加在 <code>01_prefetch.smk</code> 中，首先我们确定一些要提前确定的参数：</p><ul><li><code>fasterq-dump</code> 和 <code>pigz</code> 使用的线程数。</li></ul><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># config.yaml</span></span><br><span class="line"><span class="attr">download:</span></span><br><span class="line">  <span class="attr">retry:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">extract_threads:</span> <span class="number">6</span></span><br></pre></td></tr></tbody></table></figure><p>此外，这一步中单端时仅产生一个文件，双端时产生两个文件。因此规则需要根据 <code>config</code> 中指定的 <code>mode</code> 进行行为调整：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 01_prefetch.smk</span></span><br><span class="line">rule extract:</span><br><span class="line">    <span class="built_in">input</span>:</span><br><span class="line">        srafile = <span class="string">"rawfastq/{srr}/{srr}.sra"</span></span><br><span class="line">    output:</span><br><span class="line">        fastq_files = <span class="string">"rawfastq/{srr}.fastq.gz"</span> <span class="keyword">if</span> config[<span class="string">"mode"</span>] == <span class="string">"single"</span> <span class="keyword">else</span> [<span class="string">"rawfastq/{srr}_1.fastq.gz"</span>, <span class="string">"rawfastq/{srr}_2.fastq.gz"</span>],</span><br><span class="line">    log:</span><br><span class="line">        <span class="string">"logs/{srr}_extract.log"</span></span><br><span class="line">    params:</span><br><span class="line">        srrid = <span class="string">"{srr}"</span></span><br><span class="line">    threads: config[<span class="string">"download"</span>][<span class="string">"extract_threads"</span>]</span><br><span class="line">    shell:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        fasterq-dump {input.srafile} --progress --details --split-files -v --outdir rawfastq --threads {threads} &gt; {log} 2&gt;&amp;1</span></span><br><span class="line"><span class="string">        pigz -p {threads} rawfastq/{params.srrid}*.fastq</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        if [ "{config[mode]}" == "single" ]; then</span></span><br><span class="line"><span class="string">            if [ -e rawfastq/{params.srrid}_1.fastq.gz ]; then</span></span><br><span class="line"><span class="string">                mv rawfastq/{params.srrid}_1.fastq.gz {output.fastq_files}</span></span><br><span class="line"><span class="string">                echo "Single extract finished! (Renamed _1.fastq.gz)" &gt;&gt; {log}</span></span><br><span class="line"><span class="string">            else</span></span><br><span class="line"><span class="string">                echo "Single extract finished!" &gt;&gt; {log}</span></span><br><span class="line"><span class="string">            fi</span></span><br><span class="line"><span class="string">        else</span></span><br><span class="line"><span class="string">            if [ -e rawfastq/{params.srrid}_1.fastq.gz ] &amp;&amp; [ -e rawfastq/{params.srrid}_2.fastq.gz ]; then</span></span><br><span class="line"><span class="string">                echo "Paired extract finished!" &gt;&gt; {log}</span></span><br><span class="line"><span class="string">            else</span></span><br><span class="line"><span class="string">                echo "Paired extract failed: one or both FASTQ files missing!" &gt;&gt; {log}</span></span><br><span class="line"><span class="string">                exit 1</span></span><br><span class="line"><span class="string">            fi</span></span><br><span class="line"><span class="string">        fi</span></span><br><span class="line"><span class="string">        """</span></span><br></pre></td></tr></tbody></table></figure><p>逐段解释：</p><ol><li><code>input</code> 段<strong>指定该 rule 需要的输入文件</strong>，snakemake 将进行监测直到这些文件出现后再开始运行该 rule。<u>这里涉及到 snakemake 中最重要的一个概念</u>：<ul><li>Snakemake <strong>将依据各个 rule 的 input 和 output 确认它们彼此之间的依赖关系，构建一个工作流的有向无环图</strong>，此后按照图的拓扑顺序运行，确保某一规则在依赖的其他规则结束后才会执行。同样也是在这个过程中，Snakemake 会自动根据目标文件向各规则的 input 和 output 中进行通配符填补（即上述的 <code>{srr}</code>）。以本文拟构建的 Pipeline 为例（<code>snakemake --dag | dot -Tpng &gt; dag.png</code>）：</li></ul></li></ol><img src="/pic2/dag.png" style="zoom:80%;"><ol start="2"><li><p><code>output</code> 段新增了根据 mode 进行的输出文件调整。</p></li><li><p><code>shell</code> 段中，首先通过 <code>fasterq-dump</code> 进行提取，并使用 <code>pigz</code> 进行压缩。此后，如果使用单端模式，则将后缀统一为 <code>[SRR id].fastq.gz</code>；如果使用双端模式，则检查是否两端数据都存在。</p></li></ol><p>到此即产生我们所需要的原始数据，将这些 rule 整合进 <code>Snakefile</code> 中（声明 <code>smk</code> 文件）：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Snakefile</span></span><br><span class="line"><span class="comment"># download and extract SRA files</span></span><br><span class="line">include: <span class="string">"rules/01_prefetch.smk"</span></span><br></pre></td></tr></tbody></table></figure><p>此后 <code>trim_galore</code> 的流程构建也遵循相同的思路（设定参数 —— 编写规则 —— 整合）：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch 02_trimgalore.smk</span><br></pre></td></tr></tbody></table></figure><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># config.yaml</span></span><br><span class="line"><span class="attr">trim:</span></span><br><span class="line">  <span class="attr">threads:</span> <span class="number">8</span></span><br><span class="line">  <span class="attr">param:</span> <span class="string">""</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 02_trimgalore.smk</span></span><br><span class="line">rule trim_galore:</span><br><span class="line">    <span class="built_in">input</span>:</span><br><span class="line">        fastq_files = <span class="string">"rawfastq/{srr}.fastq.gz"</span> <span class="keyword">if</span> config[<span class="string">"mode"</span>] == <span class="string">"single"</span> <span class="keyword">else</span> [<span class="string">"rawfastq/{srr}_1.fastq.gz"</span>, <span class="string">"rawfastq/{srr}_2.fastq.gz"</span>]</span><br><span class="line">    output:</span><br><span class="line">        trimmed_fastq = <span class="string">"trimgalore_result/{srr}_trimmed.fq.gz"</span> <span class="keyword">if</span> config[<span class="string">"mode"</span>] == <span class="string">"single"</span> <span class="keyword">else</span> [<span class="string">"trimgalore_result/{srr}_1_val_1.fq.gz"</span>, <span class="string">"trimgalore_result/{srr}_2_val_2.fq.gz"</span>]</span><br><span class="line">    params:</span><br><span class="line">        option = config[<span class="string">"trim"</span>][<span class="string">"param"</span>]</span><br><span class="line">    log:</span><br><span class="line">        <span class="string">"logs/{srr}_trimgalore.log"</span></span><br><span class="line">    threads: config[<span class="string">"trim"</span>][<span class="string">"threads"</span>]</span><br><span class="line">    shell:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        if [ "{config[mode]}" == "single" ]; then</span></span><br><span class="line"><span class="string">            trim_galore {params.option} --cores {threads} {input.fastq_files} -o trimgalore_result/ &gt; {log} 2&gt;&amp;1</span></span><br><span class="line"><span class="string">        else</span></span><br><span class="line"><span class="string">            trim_galore --paired {params.option} --cores {threads} {input.fastq_files[0]} {input.fastq_files[1]} -o trimgalore_result/ &gt; {log} 2&gt;&amp;1</span></span><br><span class="line"><span class="string">        fi</span></span><br><span class="line"><span class="string">        """</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Snakefile</span></span><br><span class="line"><span class="comment"># Trim fastq</span></span><br><span class="line">include: <span class="string">"rules/02_trimgalore.smk"</span></span><br></pre></td></tr></tbody></table></figure><p>这里将监测先前 <code>extract</code> 规则输出的文件，如果 <code>extract</code> 规则运行完成则启动，并根据单端和双端模式自动调节输入的 fastq 文件和过滤后的 fastq 文件。可以设置的参数有两个，一个是每个 <code>trim_galore</code> 使用的线程数，另一个是 <code>trim_galore</code> 的额外参数。通过<strong>设置一个可以灵活调节的参数项可以让使用者依据自身的情况进行参数设定而不至于受 Pipeline 限制</strong>，例如自行设置最短长度和接头序列等。如果不需要则留空即可。</p><p><code>salmon</code> 的构造流程大致相同，但由于这里使用的通配符将变为样本名，因此这里需要进行一定变动：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch 03_salmon.smk</span><br></pre></td></tr></tbody></table></figure><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># config.yaml</span></span><br><span class="line"><span class="attr">salmon:</span></span><br><span class="line">  <span class="attr">threads:</span> <span class="number">16</span></span><br><span class="line">  <span class="attr">salmon_index:</span> <span class="string">"/path/to/hg38_salmon_index"</span></span><br><span class="line">  <span class="attr">param:</span> <span class="string">"--seqBias --gcBias --dumpEq"</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 03_salmon.smk</span></span><br><span class="line">rule salmon:</span><br><span class="line">    <span class="built_in">input</span>:</span><br><span class="line">        trimmed_fastq = <span class="keyword">lambda</span> wildcards: get_fastq_list(config[<span class="string">"mode"</span>], config[<span class="string">"inputfile"</span>][<span class="string">"groupfile"</span>], wildcards.sample)</span><br><span class="line">    output:</span><br><span class="line">        count_file = <span class="string">"results/{sample}/quant.sf"</span></span><br><span class="line">    params:</span><br><span class="line">        index = config[<span class="string">"salmon"</span>][<span class="string">"salmon_index"</span>],</span><br><span class="line">        samplename = <span class="string">"{sample}"</span>,</span><br><span class="line">        option = config[<span class="string">"salmon"</span>][<span class="string">"param"</span>],</span><br><span class="line">        fastqpath = <span class="keyword">lambda</span> wildcards: get_fastq_path(config[<span class="string">"mode"</span>], config[<span class="string">"inputfile"</span>][<span class="string">"groupfile"</span>], wildcards.sample)</span><br><span class="line">    log:</span><br><span class="line">        <span class="string">"logs/{sample}_salmon.log"</span></span><br><span class="line">    threads: config[<span class="string">"salmon"</span>][<span class="string">"threads"</span>]</span><br><span class="line">    shell:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        if [ "{config[mode]}" == "single" ]; then</span></span><br><span class="line"><span class="string">            salmon quant -i {params.index} -l A -r &lt;(zcat {params.fastqpath}) -p {threads} {params.option} -o results/{params.samplename} &gt; {log} 2&gt;&amp;1</span></span><br><span class="line"><span class="string">        else</span></span><br><span class="line"><span class="string">            salmon quant -i {params.index} -l A -1 &lt;(zcat {params.fastqpath[0]}) -2 &lt;(zcat {params.fastqpath[1]}) -p {threads} {params.option} -o results/{params.samplename} &gt; {log} 2&gt;&amp;1</span></span><br><span class="line"><span class="string">        fi</span></span><br><span class="line"><span class="string">        """</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Snakefile</span></span><br><span class="line"><span class="comment"># Salmon quant</span></span><br><span class="line">include: <span class="string">"rules/03_salmon.smk"</span></span><br></pre></td></tr></tbody></table></figure><p>可以看到这里使用的通配符变为了 <code>{sample}</code>，并且使用了两个自行构造的函数 <code>get_fastq_list</code> 及 <code>get_fastq_path</code>，分别用于获取每个样本中所有 fastq 文件的列表和路径。这两个函数的具体实现在 <code>common.smk</code> 中进行：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_fastq_list</span>(<span class="params">mode, filepath, group</span>):</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Get fastq list for each group.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    group_fastq = {}</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filepath, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            info = line.strip().split()</span><br><span class="line">            <span class="keyword">if</span> info[<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> group_fastq:</span><br><span class="line">                group_fastq[info[<span class="number">1</span>]] = [info[<span class="number">0</span>]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                group_fastq[info[<span class="number">1</span>]].append(info[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">"single"</span>:</span><br><span class="line">        fastq_path_list = [<span class="string">f'trimgalore_result/<span class="subst">{i}</span>_trimmed.fq.gz'</span> <span class="keyword">for</span> i <span class="keyword">in</span> group_fastq[group]]</span><br><span class="line">        <span class="keyword">return</span> fastq_path_list</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">"paired"</span>:</span><br><span class="line">        fastq_path_list1 = [<span class="string">f'trimgalore_result/<span class="subst">{i}</span>_1_val_1.fq.gz'</span> <span class="keyword">for</span> i <span class="keyword">in</span> group_fastq[group]]</span><br><span class="line">        fastq_path_list2 = [<span class="string">f'trimgalore_result/<span class="subst">{i}</span>_2_val_2.fq.gz'</span> <span class="keyword">for</span> i <span class="keyword">in</span> group_fastq[group]]</span><br><span class="line">        fastq_path_list = fastq_path_list1 + fastq_path_list2</span><br><span class="line">        <span class="keyword">return</span> fastq_path_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_fastq_path</span>(<span class="params">mode, filepath, group</span>):</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Get fastq path for each group.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    group_fastq = {}</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filepath, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            info = line.strip().split()</span><br><span class="line">            <span class="keyword">if</span> info[<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> group_fastq:</span><br><span class="line">                group_fastq[info[<span class="number">1</span>]] = [info[<span class="number">0</span>]]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                group_fastq[info[<span class="number">1</span>]].append(info[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">"single"</span>:</span><br><span class="line">        fastq_path_list = [<span class="string">f'trimgalore_result/<span class="subst">{i}</span>_trimmed.fq.gz'</span> <span class="keyword">for</span> i <span class="keyword">in</span> group_fastq[group]]</span><br><span class="line">        <span class="keyword">return</span> <span class="string">' '</span>.join(fastq_path_list)</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">"paired"</span>:</span><br><span class="line">        fastq_path_list1 = [<span class="string">f'trimgalore_result/<span class="subst">{i}</span>_1_val_1.fq.gz'</span> <span class="keyword">for</span> i <span class="keyword">in</span> group_fastq[group]]</span><br><span class="line">        fastq_path_list2 = [<span class="string">f'trimgalore_result/<span class="subst">{i}</span>_2_val_2.fq.gz'</span> <span class="keyword">for</span> i <span class="keyword">in</span> group_fastq[group]]</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">' '</span>.join(fastq_path_list1), <span class="string">' '</span>.join(fastq_path_list2)]</span><br></pre></td></tr></tbody></table></figure><p>这两个函数根据接收到的模式及 <code>group.txt</code> 内容，返回特定 sample 对应的 fastq 文件列表和路径，其中列表传输给 input 使其监视上游依赖 rule 的运行情况，而路径传输给特定参数以在 salmon 定量中对同一样本的测序数据进行合并。</p><p>这里涉及到的其他一些概念：</p><ul><li><strong>wildcards 是一个命名空间，在函数和 lambda 表达式中用于访问通配符。</strong><code>wildcards.sample</code> 只有在规则的输出、输入或其他字段中定义了 <code>{sample}</code> 占位符时才会存在。<strong>结合 lambda 和 wildcards 并通过自定义的处理函数可以获取需要动态规划的输入和输出。</strong>注意，直接在这些函数中给定 <code>{sample}</code> 是不可行的。</li><li>common.smk 同样需要在 Snakefile 中声明，此后该文件中定义的所有函数都可以被其他规则所识别。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Snakefile</span></span><br><span class="line">include: <span class="string">"rules/common.smk"</span></span><br></pre></td></tr></tbody></table></figure><p>到这里，Pipeline 的整体搭建就已完成。</p><h4 id="3、Pipeline-运行"><a href="#3、Pipeline-运行" class="headerlink" title="3、Pipeline 运行"></a>3、Pipeline 运行</h4><p>使用 snakemake 命令行运行 Pipeline：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $tutorial_path</span><br><span class="line">snakemake --core 16 --resources download_slots=4</span><br></pre></td></tr></tbody></table></figure><p>这里 <code>--core</code> 指定 snakemake 总共可以管理的 CPU 核数量，snakemake 将根据每个 rule 占用的 threads 自动分配任务，例如：</p><ul><li><p>如果一个 extract 要占用 8 个 threads，一个 salmon 要占用 4 个 threads。那么 snakemake 可能同时进行一个 extract 任务和两个 salmon 任务，<strong>确保提供的核被最大化利用（前提是这些任务的前置依赖都已经满足）</strong>。</p></li><li><p><code>--resources</code> 指定 snakemake 的其他分配资源，这些都是用户规定的。以上述命令为例，Snakemake 共有 4 个 download_slots 资源，而在需要 download_slots 资源运行的 rule 中，同时运行所需的资源总和将被限定在这个值以内。在该 Pipeline 中即最高只可同时并行四个下载任务。</p></li></ul><p>运行完成后，留意以下文件夹：</p><ul><li><code>log</code> 文件夹中包含各个环节运行时的输出日志文件。</li><li><code>rawfastq</code> 文件夹中为原始测序数据。</li><li><code>trimgalore_result  </code> 文件夹中为过滤后的测序数据。</li><li><code>results</code> 文件夹中为各样本的定量结果。</li></ul><p>实测以上 Pipeline 能够正确运行，如果存在问题请前往 github repository 页面查看完整 Pipeline 并确定出错点：</p><p><a href="https://github.com/JuseTiZ/Blog-Snakemake-pipeline/tree/main/Tutorial-Pipeline">https://github.com/JuseTiZ/Blog-Snakemake-pipeline/tree/main/Tutorial-Pipeline</a></p><h2 id="拓展-Pipeline"><a href="#拓展-Pipeline" class="headerlink" title="拓展 Pipeline"></a>拓展 Pipeline</h2><p>以下内容不再详细解释 Pipeline 中如何具体实现，但原理较简单且具有一定实用性，因此这里做一些简单介绍。</p><h3 id="版本控制"><a href="#版本控制" class="headerlink" title="版本控制"></a>版本控制</h3><p>每个 rule 在运行时都可以指定一个 conda 环境，例如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rule download:</span><br><span class="line">......</span><br><span class="line">    conda:</span><br><span class="line">        <span class="string">"../envs/sratool.yaml"</span></span><br><span class="line">    ......</span><br></pre></td></tr></tbody></table></figure><p><strong>这里的相对路径是以 rule 所在 smk 文件开始寻找的</strong>，你可以在 <code>../envs/sratool.yaml</code> 中填写以下内容：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">sra</span></span><br><span class="line"><span class="attr">dependencies:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">bioconda::sra-tools==3.1.0</span></span><br></pre></td></tr></tbody></table></figure><p>在运行该规则时，Snakemake 会寻找名为 sra 的环境，如果不存在则新建该环境并安装指定依赖（sra-tools）。</p><p>请注意，Snakemake 所有环境操作都是默认使用 <code>mamba</code> 进行的，如果你想使用 <code>conda</code> 进行需要在命令行中指定 <code>--conda-frontend conda</code>（不推荐）。</p><p>另外你也可以在 snakefile 中指定运行这套 pipeline 所需的最低版本，例如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> snakemake.utils <span class="keyword">import</span> min_version</span><br><span class="line">min_version(<span class="string">"6.0"</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="性能监测"><a href="#性能监测" class="headerlink" title="性能监测"></a>性能监测</h3><p>每个 rule 在运行时都可以指定一个 benchmark 文件，例如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rule download:</span><br><span class="line">......</span><br><span class="line">    benchmark:</span><br><span class="line">        <span class="string">"logs/{srr}_download_prefetch.benchmark.txt"</span>,</span><br><span class="line">    ......</span><br></pre></td></tr></tbody></table></figure><p>Snakemake 会自动测量并记录该 rule 的执行时间、CPU 使用率、内存使用情况等信息并储存到 benchmark 文件中。<strong>这对于需要优化工作流和分析性能瓶颈的情况非常有用</strong>。</p><h2 id="再次改进的可能"><a href="#再次改进的可能" class="headerlink" title="再次改进的可能"></a>再次改进的可能</h2><p>不难看出，上述 Pipeline 依然存在很多可以改进的地方以增强 Pipeline 的可读性和可拓展性，但要做到这些可能会在一定程度上提升理解难度，因此如果你认为上述内容不算困难且想要进一步检验自己当前的能力，可以考虑对该 Pipeline 做以下改进：</p><ol><li>将 <code>group.txt</code> 换为 <code>sample.yaml</code>，在 <code>Snakefile</code> 中提取到 <code>config</code> 变量里，据此进行 rule all input 的指定（可通过动态函数进行），并在 salmon 规则中的 input 和 output 里使用更具效率的函数进行动态规划，<code>sample.yaml</code> 示例：</li></ol><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">samples:</span></span><br><span class="line">  <span class="attr">HeLa_Rep1:</span></span><br><span class="line">    <span class="attr">SRR:</span> <span class="string">"SRR25601734|SRR25601735"</span></span><br><span class="line">  <span class="attr">HeLa_Rep2:</span></span><br><span class="line">    <span class="attr">SRR:</span> <span class="string">"SRR25601736|SRR25601737"</span></span><br></pre></td></tr></tbody></table></figure><ol start="2"><li>将涉及单端双端判断的 rule 拆分成两个 rule（一个用于 single-end，一个用于 paired-end），并将 <code>mode</code> 参数从 <code>config.yaml</code> 中移除，使 Pipeline 能够同时处理单端和双端的数据。例如可以通过在上面提到的 <code>sample.yaml</code> 中添加每个样本的数据类型来进行单双端数据的混合处理：</li></ol><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">samples:</span></span><br><span class="line">  <span class="attr">HeLa_Rep1:</span></span><br><span class="line">    <span class="attr">SRR:</span> <span class="string">"SRR25601734|SRR25601735"</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">"paired"</span></span><br><span class="line">  <span class="attr">xxxx:</span></span><br><span class="line">    <span class="attr">SRR:</span> <span class="string">"SRRxxx"</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">"single"</span></span><br></pre></td></tr></tbody></table></figure><ol start="3"><li>添加下游分析部分，例如新建 <code>scripts</code> 文件夹存放自定义的用于提取表达矩阵的脚本，并在 Pipeline 中添加新的规则以对其进行实现（别忘了 rule all 的 input 部分也要进行对应更新）。</li></ol><p>前两者的具体实现可参考 repository 下的 <a href="https://github.com/JuseTiZ/Blog-Snakemake-pipeline/tree/main/ChIP-Pipeline">ChIP Pipeline</a>。</p></body></html>]]></content>
    
    
    <summary type="html">本文为使用 Snakemake 搭建 pipeline 的详细教程，其中包括一个合格 Pipeline 的基本要求、管理方式及具体搭建步骤。也涉及到版本控制和性能监测等方面的内容。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生信" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E4%BF%A1/"/>
    
    
    <category term="生物信息学" scheme="https://biojuse.com/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6/"/>
    
    <category term="生信" scheme="https://biojuse.com/tags/%E7%94%9F%E4%BF%A1/"/>
    
  </entry>
  
  <entry>
    <title>对 bedgraph 文件进行 lowess 平滑操作</title>
    <link href="https://biojuse.com/2024/06/29/%E5%AF%B9%20bedgraph%20%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%20lowess%20%E5%B9%B3%E6%BB%91%E6%93%8D%E4%BD%9C/"/>
    <id>https://biojuse.com/2024/06/29/%E5%AF%B9%20bedgraph%20%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%20lowess%20%E5%B9%B3%E6%BB%91%E6%93%8D%E4%BD%9C/</id>
    <published>2024-06-29T03:30:00.000Z</published>
    <updated>2024-06-29T03:35:25.317Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>在进行一些特定的功能基因组学数据分析时，我们可能需要对 bedgraph 文件中每个 bin 的值进行一定的平滑操作，以降低随机噪声的影响并提供更好的可视化效果。例如：</p><ul><li>Repli-seq / BrdU-seq 中量化得到的 Replication Timing</li><li>OK-seq / Pu-seq 中量化得到的 Replication Fork Directionality</li></ul><p>以下是一个用于对 bedgraph 进行 lowess 平滑操作的 python script：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_args</span>():</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">'Perform lowess smooth on bedgraph.'</span>)</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">'--input'</span>, <span class="string">'-i'</span>, <span class="built_in">help</span>=<span class="string">'Input bedgraph file.'</span>, required=<span class="literal">True</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--output'</span>, <span class="string">'-o'</span>, <span class="built_in">help</span>=<span class="string">'Output smoothed bedgraph file.'</span>, required=<span class="literal">True</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--span'</span>, <span class="built_in">help</span>=<span class="string">'Span size of loess smoothing.'</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, required=<span class="literal">True</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--chr"</span>, required=<span class="literal">True</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">"The chrom to input. e.g. 1-22,X,Y"</span>)</span><br><span class="line">    </span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> args</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_range</span>(<span class="params">value</span>):</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> part <span class="keyword">in</span> value.split(<span class="string">','</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'-'</span> <span class="keyword">in</span> part:</span><br><span class="line">            start, end = part.split(<span class="string">'-'</span>)</span><br><span class="line">            result.extend(<span class="built_in">range</span>(<span class="built_in">int</span>(start), <span class="built_in">int</span>(end) + <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            result.append(<span class="built_in">int</span>(part))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line"></span><br><span class="line">    args = get_args()</span><br><span class="line"></span><br><span class="line">    data = pd.read_csv(args.<span class="built_in">input</span>, sep=<span class="string">'\t'</span>, header=<span class="literal">None</span>, names=[<span class="string">'chrom'</span>, <span class="string">'start'</span>, <span class="string">'end'</span>, <span class="string">'value'</span>])</span><br><span class="line">    data[<span class="string">'midpoint'</span>] = (data[<span class="string">'start'</span>] + data[<span class="string">'end'</span>]) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    span_size = args.span</span><br><span class="line">    smoothed_data = []</span><br><span class="line">    chr_list = [<span class="string">f'chr<span class="subst">{i}</span>'</span> <span class="keyword">for</span> i <span class="keyword">in</span> parse_range(args.<span class="built_in">chr</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> chrom <span class="keyword">in</span> data[<span class="string">'chrom'</span>].unique():</span><br><span class="line">        chrom_data = data[data[<span class="string">'chrom'</span>] == chrom]</span><br><span class="line">        <span class="keyword">if</span> chrom <span class="keyword">not</span> <span class="keyword">in</span> chr_list:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        chrom_length = chrom_data[<span class="string">'end'</span>].iloc[-<span class="number">1</span>] - chrom_data[<span class="string">'start'</span>].iloc[<span class="number">0</span>]</span><br><span class="line">        span_frac = span_size / chrom_length</span><br><span class="line"></span><br><span class="line">        lowess = sm.nonparametric.lowess(chrom_data[<span class="string">'value'</span>], chrom_data[<span class="string">'midpoint'</span>], frac=span_frac)</span><br><span class="line">        smoothed_df = pd.DataFrame(lowess, columns=[<span class="string">'midpoint'</span>, <span class="string">'smoothed_value'</span>])</span><br><span class="line">        chrom_data = chrom_data.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        chrom_data[<span class="string">'smoothed_value'</span>] = smoothed_df[<span class="string">'smoothed_value'</span>]</span><br><span class="line">        smoothed_data.append(chrom_data)</span><br><span class="line"></span><br><span class="line">    smoothed_data = pd.concat(smoothed_data)</span><br><span class="line">    smoothed_data[[<span class="string">'chrom'</span>, <span class="string">'start'</span>, <span class="string">'end'</span>, <span class="string">'smoothed_value'</span>]].to_csv(args.output, sep=<span class="string">'\t'</span>, header=<span class="literal">False</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></tbody></table></figure><p><strong>下载地址：</strong></p><p><a href="https://github.com/JuseTiZ/PyScript-for-CT/blob/main/bedgraph_lowess.py">https://github.com/JuseTiZ/PyScript-for-CT/blob/main/bedgraph_lowess.py</a></p><p><strong>依赖 module 安装：</strong></p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install pandas</span><br><span class="line">pip install statsmodels</span><br></pre></td></tr></tbody></table></figure><p><strong>参数详情：</strong></p><ul><li><code>--input</code> / <code>-i</code>，指定需要进行 lowess 平滑操作的 bedgraph 文件，该文件应当仅具有四列，且第一列染色体编号应以 <code>chr</code> 开头。</li><li><code>--output</code> / <code>-o</code>，指定输出的平滑后 bedgraph 文件，输出的新 bedgraph 中第四列为 LOWESS smooth 后的值。</li><li><code>--span</code>，指定平滑操作时使用的长度，脚本将根据每条染色体的总长确定用于平滑的数据比例。</li><li><code>--chr</code>，指定进行平滑操作的染色体编号，可使用 <code>-</code> 指定数字范围，也可使用逗号分隔，例如 <code>1-22,X,Y</code>。</li></ul><p><strong>应用示例：</strong></p><p>假设目前有一个通过 Repli-seq 计算得到的小鼠 RT(Replication Timing) bedgraph 文件 <code>RT.bedgraph</code>，通过以下命令进行 30w bp 的平滑：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python bedgraph_lowess.py -i RT.bedgraph -o RT.lowess.bedgraph --span 300000 --chr 1-19</span><br></pre></td></tr></tbody></table></figure><p>平滑前后 IGV track 示例：</p><p><img src="/pic2/bedgraphlowess.png"></p><p>请注意，平滑操作<strong>在减少噪音的同时，也损失了部分信息量</strong>，因此请根据自己当前使用的数据进行权衡，合理设置 <code>--span</code> 参数，一般情况下：</p><ul><li>数据的分辨率越高，该参数指定的值应当越低。反之亦然。</li><li>对信息精细程度的要求越高，该参数指定的值应当越低。反之亦然。</li></ul><p>例如对于某些 OK-seq 数据而言只需要 60kb 左右的 span size 即可。请根据研究需求或者数据来源文章指定恰当的值。</p></body></html>]]></content>
    
    
    <summary type="html">在某些特别的 seq 数据处理中，我们可能要对得到的 bedgraph 文件进行平滑以消除噪音影响。本文介绍了一种基于 python 的平滑实现方法。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="杂项" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E6%9D%82%E9%A1%B9/"/>
    
    
  </entry>
  
  <entry>
    <title>获取基因组上 intron 区域及对应链信息的方法</title>
    <link href="https://biojuse.com/2024/06/09/%E8%8E%B7%E5%8F%96%E5%9F%BA%E5%9B%A0%E7%BB%84%E4%B8%8A%20intron%20%E5%8C%BA%E5%9F%9F%E5%8F%8A%E5%AF%B9%E5%BA%94%E9%93%BE%E4%BF%A1%E6%81%AF%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <id>https://biojuse.com/2024/06/09/%E8%8E%B7%E5%8F%96%E5%9F%BA%E5%9B%A0%E7%BB%84%E4%B8%8A%20intron%20%E5%8C%BA%E5%9F%9F%E5%8F%8A%E5%AF%B9%E5%BA%94%E9%93%BE%E4%BF%A1%E6%81%AF%E7%9A%84%E6%96%B9%E6%B3%95/</id>
    <published>2024-06-09T12:30:00.000Z</published>
    <updated>2024-06-09T12:46:13.128Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><p>前置条件：</p><ul><li>在环境变量中可调用的 <code>bedtools</code></li></ul><p>本文参考：</p><blockquote><p><strong>Get intronic and intergenic sequences based on gff file</strong> from Biostars</p><p><a href="https://www.biostars.org/p/112251/">https://www.biostars.org/p/112251/</a></p></blockquote><p>本文可满足的需求：</p><ul><li>得到特定区域的 bed 文件（例如 exon / intron 等）。</li><li>在得到区域信息的同时进行链信息的区分。</li></ul><h3 id="下载基因组注释文件（gtf）"><a href="#下载基因组注释文件（gtf）" class="headerlink" title="下载基因组注释文件（gtf）"></a>下载基因组注释文件（gtf）</h3><p>以人类最新版本的 gencode 注释为例：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_46/gencode.v46.basic.annotation.gtf.gz</span><br></pre></td></tr></tbody></table></figure><p>也可以选择已有的 gtf 文件进行。</p><h3 id="得到-transcript-和-exon-区域信息"><a href="#得到-transcript-和-exon-区域信息" class="headerlink" title="得到 transcript 和 exon 区域信息"></a>得到 transcript 和 exon 区域信息</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">GTF="/path/to/gencode.v46.basic.annotation.gtf.gz" # 改为自己实际的注释文件路径，建议使用压缩格式以兼容以下命令</span><br><span class="line">BASENAME=$(basename "$GTF" .gtf.gz)</span><br><span class="line"></span><br><span class="line">TRANSCRIPT_BED="${BASENAME}.transcript.bed"</span><br><span class="line">FORWARD_TRANS_BED="${BASENAME}.transcript.fors.bed"</span><br><span class="line">BACKWARD_TRANS_BED="${BASENAME}.transcript.bacs.bed"</span><br><span class="line">DOUBLE_TRANS_BED="${BASENAME}.transcript.doubletrans.bed"</span><br><span class="line">EXON_BED="${BASENAME}.exon.bed"</span><br><span class="line">INTRON_BED="${BASENAME}.intron.bed"</span><br><span class="line">CDS_BED="${BASENAME}.cds.bed"</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">得到 transcript 区域</span></span><br><span class="line">awk '$3=="transcript" {print $1, $4-1, $5, $7}' OFS='\t' &lt;(zcat $GTF) &gt; $TRANSCRIPT_BED</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">得到 exon 区域</span></span><br><span class="line">awk '$3=="exon" {print $1, $4-1, $5, $7}' OFS='\t' &lt;(zcat $GTF) &gt; $EXON_BED</span><br></pre></td></tr></tbody></table></figure><p>以上命令将得到 <code>.transcript.bed</code> 和 <code>.exon.bed</code> 结尾的文件，里面包含基因组上所有 transcript 和 exon 的信息。</p><h3 id="不关注链信息时"><a href="#不关注链信息时" class="headerlink" title="不关注链信息时"></a>不关注链信息时</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">合并排序 transcript 区域</span></span><br><span class="line">bedtools sort -i $TRANSCRIPT_BED | bedtools merge -i - &gt; $TRANSCRIPT_BED.tmp </span><br><span class="line">mv $TRANSCRIPT_BED.tmp $TRANSCRIPT_BED</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">合并排序 exon 区域</span></span><br><span class="line">bedtools sort -i $EXON_BED | bedtools merge -i - &gt; $EXON_BED.tmp </span><br><span class="line">mv $EXON_BED.tmp $EXON_BED</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在 transcript 中排除 exon 得到 intron</span></span><br><span class="line">bedtools subtract -a $TRANSCRIPT_BED -b $EXON_BED &gt; $INTRON_BED</span><br></pre></td></tr></tbody></table></figure><h3 id="关注链信息时"><a href="#关注链信息时" class="headerlink" title="关注链信息时"></a>关注链信息时</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">得到正负链信息</span></span><br><span class="line">awk '$4=="+"' $TRANSCRIPT_BED | bedtools sort -i - | bedtools merge -i - | awk 'OFS="\t"{print $0, "+"}' &gt; $FORWARD_TRANS_BED</span><br><span class="line">awk '$4=="-"' $TRANSCRIPT_BED | bedtools sort -i - | bedtools merge -i - | awk 'OFS="\t"{print $0, "-"}' &gt; $BACKWARD_TRANS_BED</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">找出冲突区域（正负链上都存在 transcript）</span></span><br><span class="line">bedtools intersect -a $FORWARD_TRANS_BED -b $BACKWARD_TRANS_BED &gt; $DOUBLE_TRANS_BED</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">排除 transcript 冲突区域</span></span><br><span class="line">bedtools subtract -a &lt;(cat $FORWARD_TRANS_BED $BACKWARD_TRANS_BED | bedtools sort -i -) -b $DOUBLE_TRANS_BED &gt; $TRANSCRIPT_BED</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">排除 exon 冲突区域</span></span><br><span class="line">awk '$3=="exon" {print $1, $4-1, $5, $7}' OFS='\t' &lt;(zcat $GTF) | bedtools sort -i - | bedtools merge -i - &gt; $EXON_BED</span><br><span class="line">bedtools intersect -a $TRANSCRIPT_BED -b $EXON_BED &gt; $EXON_BED.tmp</span><br><span class="line">mv $EXON_BED.tmp $EXON_BED</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">得到包含链信息的 intron</span></span><br><span class="line">bedtools subtract -a $TRANSCRIPT_BED -b $EXON_BED &gt; $INTRON_BED</span><br></pre></td></tr></tbody></table></figure><h3 id="相关的用途"><a href="#相关的用途" class="headerlink" title="相关的用途"></a>相关的用途</h3><p>通过上述命令，同理也可以获得例如 CDS 或 UTR 的 bed 文件。下游分析中，我们可以通过这些 bed 文件筛选在特定区域中的突变进行相关探索：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bedtools intersect -a input.vcf -b CDS.bed &gt; input.cds.vcf</span><br></pre></td></tr></tbody></table></figure><p>将 <code>input.vcf</code> 和 <code>CDS.bed</code> 请换为实际路径即可得到所有在 CDS 区域中的 variants。</p></body></html>]]></content>
    
    
    <summary type="html">通过基因组注释文件得到基因组 intron 区域的 bed 文件，同时标注链信息。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生信" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E4%BF%A1/"/>
    
    
    <category term="生物信息学" scheme="https://biojuse.com/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6/"/>
    
    <category term="生信" scheme="https://biojuse.com/tags/%E7%94%9F%E4%BF%A1/"/>
    
    <category term="基因组" scheme="https://biojuse.com/tags/%E5%9F%BA%E5%9B%A0%E7%BB%84/"/>
    
  </entry>
  
  <entry>
    <title>Ensembl VEP plugins 的使用方法（Alphamissense、dbNSFP 等）</title>
    <link href="https://biojuse.com/2024/06/09/Ensembl%20VEP%20plugins%20%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%EF%BC%88Alphamissense%E3%80%81dbNSFP%20%E7%AD%89%EF%BC%89/"/>
    <id>https://biojuse.com/2024/06/09/Ensembl%20VEP%20plugins%20%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%EF%BC%88Alphamissense%E3%80%81dbNSFP%20%E7%AD%89%EF%BC%89/</id>
    <published>2024-06-09T11:30:00.000Z</published>
    <updated>2024-06-10T02:58:46.759Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>以下是相关的官网链接，如果想要得到除本文内容以外更全面深入的了解，建议跳转相关链接进行查阅：</p><blockquote><p>VEP plugins documentation:</p><p><a href="https://grch37.ensembl.org/info/docs/tools/vep/script/vep_plugins.html">https://grch37.ensembl.org/info/docs/tools/vep/script/vep_plugins.html</a></p><p>VEP install documentation:</p><p><a href="https://grch37.ensembl.org/info/docs/tools/vep/script/vep_download.html">https://grch37.ensembl.org/info/docs/tools/vep/script/vep_download.html</a></p></blockquote><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="安装-VEP"><a href="#安装-VEP" class="headerlink" title="安装 VEP"></a>安装 VEP</h3><p>此处强烈建议使用 Docker 或者 Singularity 直接拉取 VEP 镜像进行分析，因为在 VEP image 中已经内置好所有的 plugin，无需自己手动下载。</p><p>考虑到 Docker 对权限的要求更高，因此这里以普适性更强的 Singularity 为例（Docker 用户有需求可通过文章开头链接前往官网进行参考）：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">拉取 VEP image</span></span><br><span class="line">singularity pull --name vep.sif docker://ensemblorg/ensembl-vep</span><br></pre></td></tr></tbody></table></figure><p>之后就可以通过 <code>singularity</code> 运行 VEP：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">singularity exec vep.sif vep --help</span><br></pre></td></tr></tbody></table></figure><p>直接安装的方法：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/Ensembl/ensembl-vep.git</span><br><span class="line">cd ensembl-vep</span><br><span class="line">perl INSTALL.pl</span><br></pre></td></tr></tbody></table></figure><p><code>INSTALL.pl</code> 可以通过指定一系列参数来进行自定义安装，这里介绍其中主要需要注意的几个：</p><ul><li><code>--CACHE_VERSION</code> 选择特定的 Ensembl 版本，目前最新为 112，请根据自己正在使用的注释版本确定。</li><li><code>--CACHEDIR</code> 下载数据库的存储路径，默认为 <code>$HOME/.vep</code>，可自行修改。</li><li><code>--PLUGINS</code> 指定下载的插件（plugins），通过逗号分隔。也可通过 <code>--PLUGINS all</code> 使其下载全部插件。</li></ul><p>由于 <code>INSTALL.pl</code> 需要一定的依赖项，因此遇到安装报错时可前往其 <a href="https://grch37.ensembl.org/info/docs/tools/vep/script/vep_download.html">documentation</a> 查看是否存在依赖项缺失，此外 API 和数据库版本不同时可能存在兼容问题。但目前我使用最新版本的 VEP 进行旧版本数据库的注释是不存在任何问题的。</p><h3 id="下载数据库"><a href="#下载数据库" class="headerlink" title="下载数据库"></a>下载数据库</h3><p>通过 <code>INSTALL.pl</code> 安装的另一个弊端就是可能下载速度非常慢，因此这里介绍一个自行安装数据库的方法，对于使用 <code>singularity</code> 或者已经配置好 <code>vep</code> 的情况，可以使用以下命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir vep_data</span><br><span class="line">cd vep_data</span><br><span class="line">wget -c ftp://ftp.ensembl.org/pub/release-112/variation/indexed_vep_cache/homo_sapiens_vep_112_GRCh38.tar.gz</span><br><span class="line">tar zxf homo_sapiens_vep_112_GRCh38.tar.gz</span><br></pre></td></tr></tbody></table></figure><p>使用其他版本时，可将上述网址中的 <code>112</code> 替换为其他版本号例如 <code>110</code> 等。</p><p>之后在运行 <code>vep</code> 时可以通过 <code>--cache_version</code> 指定需要使用的版本号。</p><h3 id="下载-plugins-需要的文件"><a href="#下载-plugins-需要的文件" class="headerlink" title="下载 plugins 需要的文件"></a>下载 plugins 需要的文件</h3><p>这里仅介绍 AlphaMissense 和 dbNSFP 两个插件的数据库构建方法，原因如下：</p><ul><li>AlphaMissense 对于非同义突变的影响预测具有目前最先进的性能。但由于其开发团队仅提供了 GENCODE V33（对应 Ensembl V98），因此很多更新版本注释中的突变可能没法找到其对应的 AlphaMissense 参数。且其 github 上仅提供模型架构而未提供训练后的权重信息，因此无法自行预测。但其结果依然是极具参考价值的。</li><li>dbNSFP 是一个集成数据库，里面包含了众多 Variant Effect Predictor 的结果，包括但不限于 CADD、LINSIGHT、ESM1b、EVE、AlphaMissense 等各种 score，安装了该插件的效果等同于安装许多其他插件，从效率上讲极具价值。如果选择安装该数据库则可以考虑跳过 AlphaMissense，注意该数据库文件较大请注意剩余存储。</li></ul><img src="/pic2/dbNSFP.png" style="zoom:80%;"><p>一些需要注意的事项：</p><ul><li>插件下载好不代表可以直接使用，因为它需要基于对应数据库才能运行。</li><li>AlphaMissense 是 vep 112 版本新发布的插件，不清楚旧版本 API 是否与其兼容。</li><li>dbNSFP 中<strong>仅包含非同义突变的注释信息</strong>，因此其仅会对那些存在 missense_variant 的位点进行注释。如果你还需要统计同义突变或者内含子突变的 score（例如 CADD 等软件包含其他类型突变的影响），那么使用 dbNSFP 可能不是最好的选择。</li><li>dbNSFP 本身使用的注释版本可能与 VEP 具有冲突，这可能导致相关的预测结果在不同版本间存在冲突，详见 <a href="https://github.com/Ensembl/VEP_plugins/issues/626">issue#626</a>。</li><li>建议将所有数据库下载到同一目录下的不同子目录中，方便进行管理和可能需要的路径绑定。</li></ul><h4 id="AlphaMissense-安装"><a href="#AlphaMissense-安装" class="headerlink" title="AlphaMissense 安装"></a>AlphaMissense 安装</h4><p>更多细节见：<a href="https://grch37.ensembl.org/info/docs/tools/vep/script/vep_plugins.html#alphamissense">https://grch37.ensembl.org/info/docs/tools/vep/script/vep_plugins.html#alphamissense</a></p><p>请自行选择安装该数据库的路径 <code>[PATH]</code>，然后运行以下命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd [PATH]</span><br><span class="line">gsutil -m cp \</span><br><span class="line">  "gs://dm_alphamissense/AlphaMissense_hg19.tsv.gz" \</span><br><span class="line">  "gs://dm_alphamissense/AlphaMissense_hg38.tsv.gz" \</span><br><span class="line">  .</span><br></pre></td></tr></tbody></table></figure><p>此后对数据库进行构建，以使用 hg38 版本为例：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tabix -s 1 -b 2 -e 2 -f -S 1 AlphaMissense_hg38.tsv.gz</span><br></pre></td></tr></tbody></table></figure><p>建立好索引后，在运行 VEP 时可以通过以下命令进行 Alphamissense score 注释：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">singularity exec /path/to/vep.sif -i variations.vcf --plugin AlphaMissense,file=/full/path/to/AlphaMissense_hg38.tsv.gz</span><br></pre></td></tr></tbody></table></figure><p>将 <code>vep.sif</code> 和 <code>AlphaMissense_hg38.tsv.gz</code> 的路径替换为自己的路径（详细运行示例可见下文）。</p><h4 id="dbNSFP-安装"><a href="#dbNSFP-安装" class="headerlink" title="dbNSFP 安装"></a>dbNSFP 安装</h4><p>更多细节见：<a href="https://grch37.ensembl.org/info/docs/tools/vep/script/vep_plugins.html#dbnsfp">https://grch37.ensembl.org/info/docs/tools/vep/script/vep_plugins.html#dbnsfp</a></p><p>dbNSFP 每个版本分为 <code>a</code> <code>c</code> 两个类型，其中 <code>a</code> 适用于 academic use，<code>c</code> 适用于 <code>commercial use</code>。后者中不包含以下 effect score:</p><blockquote><p>Polyphen2, VEST, REVEL, ClinPred, CADD, LINSIGHT, GenoCanyon</p></blockquote><p>以下部分以 <code>a</code> 类型为例，该文章编写时 dbNSFP 的最新版本为 v4.8，若有变动请见其 <a href="https://sites.google.com/site/jpopgen/dbNSFP">documentation</a> 页面。</p><p>请自行选择安装该数据库的路径 <code>[PATH]</code>，然后运行以下命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd [PATH]</span><br><span class="line">wget https://dbnsfp.s3.amazonaws.com/dbNSFP4.8a.zip</span><br></pre></td></tr></tbody></table></figure><p>下载后，通过以下命令进行数据库的构建：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">version=4.8a</span><br><span class="line">unzip dbNSFP${version}.zip</span><br><span class="line">zcat dbNSFP${version}_variant.chr1.gz | head -n1 &gt; h</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">For hg38</span></span><br><span class="line">zgrep -h -v ^#chr dbNSFP${version}_variant.chr* | sort -k1,1 -k2,2n - | cat h - | bgzip -c &gt; dbNSFP${version}_grch38.gz</span><br><span class="line">tabix -s 1 -b 2 -e 2 dbNSFP${version}_grch38.gz</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">For hg19</span></span><br><span class="line">zgrep -h -v ^#chr dbNSFP${version}_variant.chr* | awk '$8 != "." ' | sort -k8,8 -k9,9n - | cat h - | bgzip -c &gt; dbNSFP${version}_grch37.gz</span><br><span class="line">tabix -s 8 -b 9 -e 9 dbNSFP${version}_grch37.gz</span><br></pre></td></tr></tbody></table></figure><p>通过以下命令查看 dbNSFP 可以进行哪些 score 的注释：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat h | tr '\t' '\n'</span><br></pre></td></tr></tbody></table></figure><p>以上命令会打印出所有可以进行注释的列，通过在参数中指定这些列进行相应的注释，以 AlphaMissense 为例：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> h | <span class="built_in">tr</span> <span class="string">'\t'</span> <span class="string">'\n'</span> | grep AlphaMissense</span></span><br><span class="line">AlphaMissense_score</span><br><span class="line">AlphaMissense_rankscore</span><br><span class="line">AlphaMissense_pred</span><br></pre></td></tr></tbody></table></figure><p>在运行 VEP 时可以通过以下命令进行 Alphamissense score 注释：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">singularity exec /path/to/vep.sif -i variations.vcf --plugin dbNSFP,file=/path/to/dbNSFP${version}_grch38.gz,AlphaMissense_score</span><br></pre></td></tr></tbody></table></figure><p>如果要进行其他注释，则将 <code>AlphaMissense_score</code> 替换为对应的列名（即文件 <code>h</code> 中包含的那些名称）。如果要使用所有列，则指定 <code>ALL</code> 即可。</p><p>将 <code>vep.sif</code> 和 <code>dbNSFP${version}_grch38.gz</code> 的路径替换为自己的路径。</p><h3 id="进行注释"><a href="#进行注释" class="headerlink" title="进行注释"></a>进行注释</h3><p>以下是一个 dbNSFP 注释的示例：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">singularity exec -B /path/to/database:/path/to/database /path/to/vep.sif \</span><br><span class="line"> vep --dir /path/to/vep_data \</span><br><span class="line">    --cache --cache_version [version] --offline --format vcf --vcf --force_overwrite --assembly GRCh38 \</span><br><span class="line">    --input_file [input vcf] \</span><br><span class="line">    --output_file [output vcf] \</span><br><span class="line">    --plugin dbNSFP,/path/to/database/dbNSFP${version}_grch38.gz,AlphaMissense_score,CADD_raw,phyloP100way_vertebrate</span><br></pre></td></tr></tbody></table></figure><p>注意事项：</p><ul><li><code>-B</code>：该参数用于将目录挂载到 singularity 中，不是必选项。但是如果在运行中，各个插件文件的<u>路径指定正确却依然返回找不到文件</u>时，则需要通过 -B 将插件文件的目录挂载到 singularity 容器中进行访问。比如如果 dbNSFP 的目录在 <code>/database/dbNSFP</code> 中，则可以通过 <code>-B /database:/database</code> 将其目录挂载到容器的相同位置上，从而进行访问。</li><li>将 <code>/path/to/vep.sif</code> / <code>/path/to/vep_data</code> / <code>[input vcf]</code> / <code>[output vcf]</code> 更改为自己的实际路径。</li><li><code>[version]</code> 指定下载的注释版本，如 <code>112</code>。</li><li>dbNSFP 插件中的文件路径改为自己的实际路径。</li><li>要指定更多 dbNSFP 的列，只需要通过逗号作为分隔符添加即可。</li></ul><p>或者你仅想进行 AlphaMissense 的注释：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">singularity exec -B /path/to/database:/path/to/database /path/to/vep.sif \</span><br><span class="line"> vep --dir /path/to/vep_data \</span><br><span class="line">    --cache --cache_version [version] --offline --format vcf --vcf --force_overwrite --assembly GRCh38 \</span><br><span class="line">    --input_file [input vcf] \</span><br><span class="line">    --output_file [output vcf] \</span><br><span class="line">    --plugin AlphaMissense,file=/path/to/database/AlphaMissense_hg38.tsv.gz</span><br></pre></td></tr></tbody></table></figure><p>请根据自己的实际需求选择 <code>hg38</code> 或 <code>hg19</code> 版本。</p><h3 id="多进程并行"><a href="#多进程并行" class="headerlink" title="多进程并行"></a>多进程并行</h3><p>不难察觉到，VEP 的运行速度是非常慢的，如果想对基因组的所有 variants 进行注释可能需要花费大量的时间，因此可以通过以下几种方法降低所需时间：</p><ol><li>仅选择特定区域的 variants，例如只取出那些落在 CDS 区域的 variants 等。</li><li>通过多进程进行并行注释。</li></ol><p>关于如何选择仅在 CDS 区域的 variants 可见博客另一篇文章，这里仅提如何实现多进程并行。</p><p>首先，假设所有 variants 都在同一个文件里，那么一个简单的方法是将其拆分为不同染色体的 variants，然后对每一条染色体的 variants 进行并行注释：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">vcffile="example.vcf" # 请将该处的 vcf 替换为真实的 vcf 文件名称</span><br><span class="line">outname=$(basename "$vcffile" .vcf)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">保存 vcf header</span></span><br><span class="line">grep '^#' "$vcffile" &gt; header.vcf</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">输出每条染色体的 vcf</span></span><br><span class="line">awk -v outname="$outname" 'BEGIN {OFS="\t"} !/^#/ {print &gt; outname"."$1".vcf.tmp"}' "$vcffile"</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">整合 header 和 vcf</span></span><br><span class="line">mkdir -p subvcf</span><br><span class="line">for i in *.vcf.tmp;</span><br><span class="line">do</span><br><span class="line">    cat header.vcf "$i" &gt; ./subvcf/"$(basename "$i" .tmp)"</span><br><span class="line">    rm "$i"</span><br><span class="line">done</span><br></pre></td></tr></tbody></table></figure><p>拆分步骤如上所示，并行请根据实际情况决定方案，例如通过 slurm 调度系统或者 parallel 命令等实现。此处以 parallel 为例：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ls ./subvcf/*.vcf | parallel -j [num] \</span><br><span class="line">'singularity exec -B /path/to/database:/path/to/database /path/to/vep.sif \</span><br><span class="line"> vep --dir /path/to/vep_data \</span><br><span class="line">    --cache --cache_version [version] --offline --format vcf --vcf --force_overwrite --assembly GRCh38 \</span><br><span class="line">    --input_file {} \</span><br><span class="line">    --output_file {.}.anno.vcf \</span><br><span class="line">    --plugin AlphaMissense,file=/path/to/database/AlphaMissense_hg38.tsv.gz'</span><br></pre></td></tr></tbody></table></figure><p>请将 <code>-j</code> 后的 <code>[num]</code> 替换为希望的并行作业数，最终的注释 vcf 文件将在 <code>subvcf</code> 中以 <code>.anno.vcf</code> 结尾。</p><img src="/pic2/vep_example1.png" style="zoom:80%;"><p>以上方案依然存在两个问题：</p><p>①、不同染色体上的 variants 数量差异很大。</p><p>②、这样仅能做到最高同时并行 <code>染色体数</code> 个任务。</p><p>因此，也可以通过将文件拆分为 variants 数量相等的若干个文件进行并行，以下是一个示例：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">vcffile="example.vcf"</span><br><span class="line">outname=$(basename "$vcffile" .vcf)</span><br><span class="line">num_splits=10  # 希望的拆分数量</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">保存 vcf header</span></span><br><span class="line">grep '^#' "$vcffile" &gt; header.vcf</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">计算突变数量</span></span><br><span class="line">total_variants=$(grep -v '^#' "$vcffile" | wc -l)</span><br><span class="line">variants_per_file=$(( (total_variants + num_splits - 1) / num_splits ))</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">输出子 vcf 文件</span></span><br><span class="line">mkdir -p subvcf</span><br><span class="line">split_count=1</span><br><span class="line">variant_count=0</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用 awk 按突变数量进行拆分（By GPT4.0）</span></span><br><span class="line">awk -v header="header.vcf" -v outname="$outname" -v variants_per_file="$variants_per_file" -v split_count="$split_count" -v variant_count="$variant_count" '</span><br><span class="line">BEGIN {</span><br><span class="line">    while ((getline &lt; header) &gt; 0) {</span><br><span class="line">        header_lines[++header_line_count] = $0</span><br><span class="line">    }</span><br><span class="line">    close(header)</span><br><span class="line">}</span><br><span class="line">!/^#/ {</span><br><span class="line">    if (variant_count % variants_per_file == 0) {</span><br><span class="line">        if (split_count &gt; 1) close(output_file)</span><br><span class="line">        output_file = sprintf("./subvcf/%s.split%d.vcf", outname, split_count)</span><br><span class="line">        for (i = 1; i &lt;= header_line_count; i++) {</span><br><span class="line">            print header_lines[i] &gt; output_file</span><br><span class="line">        }</span><br><span class="line">        split_count++</span><br><span class="line">    }</span><br><span class="line">    print &gt;&gt; output_file</span><br><span class="line">    variant_count++</span><br><span class="line">}</span><br><span class="line">' "$vcffile"</span><br></pre></td></tr></tbody></table></figure><p>以上命令将把文件分为拆分为 10 个子 vcf 文件，并行的方法同之前所述。如果需要拆分为更多的子文件以设置更高的并行数量，仅需调整 <code>num_splits</code> 即可。</p><img src="/pic2/vep_example2.png" style="zoom:80%;"><h3 id="注释结果解释"><a href="#注释结果解释" class="headerlink" title="注释结果解释"></a>注释结果解释</h3><p>注释后，header 中会出现相应的说明字段，以 AlphaMissence 注释为例：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">##INFO=&lt;ID=CSQ,Number=.,Type=String,Description="Consequence annotations from Ensembl VEP. Format: Allele|Consequence|IMPACT|SYMBOL|Gene|Feature_type|Feature|BIOTYPE|EXON|INTRON|HGVSc|HGVSp|cDNA_position|CDS_position|Protein_position|Amino_acids|Codons|Existing_variation|DISTANCE|STRAND|FLAGS|SYMBOL_SOURCE|HGNC_ID|am_class|am_pathogenicity"&gt;</span><br><span class="line">##am_class=The AlphaMissense thresholds are: 'Likely benign' if score &lt; 0.34, 'Likely pathogenic' if score &gt; 0.564, 'ambiguous' otherwise -- see doi.org/10.1126/science.adg7492 for details; column from /data/alphamissense/AlphaMissense_hg38.tsv.gz</span><br><span class="line">##am_pathogenicity=Continuous AlphaMissense score between 0 and 1 which can be interpreted as the predicted probability of the variant being pathogenic; column from /data/alphamissense/AlphaMissense_hg38.tsv.gz</span><br></pre></td></tr></tbody></table></figure><p>这里 <code>INFO</code> 中多出的 <code>CSQ</code> 为 Ensemble VEP 的注释结果，其中以 <code>|</code> 分隔所有注释信息，相应位置上对应的注释说明可见 header 说明。此外：</p><ul><li>一个 variant 可能落在多个转录本中，因此对应的 <code>CSQ</code> 会出现多条结果（以 <code>,</code> 分隔）。</li><li>也有可能该 variant 并不在某个 score 的注释区域内（例如 AlphaMissense 仅注释非同义突变），此时对应位置将为空。</li></ul><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>之前在 <a href="https://www.nature.com/articles/s41588-023-01465-0">esm1b</a> 的文章里看到他们用的就是 dbNSFP 来评估各个 VEP method 的表现，想一想先前我还傻楞地去一个一个下载，不禁感慨世界上有很多节省时间的方式，只是需要多花些时间、多长点见识才能了解到。</p><p>也希望这篇文章能帮其他人少走点弯路。</p></body></html>]]></content>
    
    
    <summary type="html">本文将介绍如何使用 VEP 的各个插件进行全面的 variant effect 注释，同时也涉及到数据下载、相关版本注意事项和多进程并行方法等。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生信" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E4%BF%A1/"/>
    
    
    <category term="生物信息学" scheme="https://biojuse.com/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6/"/>
    
    <category term="生信" scheme="https://biojuse.com/tags/%E7%94%9F%E4%BF%A1/"/>
    
  </entry>
  
  <entry>
    <title>fasterq-dump 下载 SRA 文件时报错的解决方法（err cmn_iter）</title>
    <link href="https://biojuse.com/2024/05/10/fasterq-dump%20%E4%B8%8B%E8%BD%BD%20SRA%20%E6%96%87%E4%BB%B6%E6%97%B6%E6%8A%A5%E9%94%99%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%EF%BC%88err%20cmn_iter%EF%BC%89/"/>
    <id>https://biojuse.com/2024/05/10/fasterq-dump%20%E4%B8%8B%E8%BD%BD%20SRA%20%E6%96%87%E4%BB%B6%E6%97%B6%E6%8A%A5%E9%94%99%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%EF%BC%88err%20cmn_iter%EF%BC%89/</id>
    <published>2024-05-10T03:00:00.000Z</published>
    <updated>2024-06-27T07:35:12.744Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>这几天下载 SRA，遇到的错误有：</p><ul><li><p><code>fasterq-dump.3.1.0 err: cmn_iter.c cmn_read_uint8_array</code></p></li><li><p><code>fasterq-dump.3.1.0 err: cmn_iter.c cmn_read_String</code></p></li></ul><p>以前使用 <code>fasterq-dump</code> 时只是偶然出现这些问题，重新下载也都能解决，但最近有些 SRA 一直下载失败，经过调查发现这种错误在大文件下载时出现异常频繁，且多次下载并没法有效解决问题，因此需要一个替代的方法。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="Prefetch"><a href="#Prefetch" class="headerlink" title="Prefetch"></a>Prefetch</h4><p>由于 <code>fasterq-dump</code> 直接通过 HTTP 下载得到 fastq 文件，该过程很可能由于一些问题中断从而导致下载失败。因此可以通过更稳定的 <code>prefetch</code> 先得到 sra 文件，再通过 <code>fasterq-dump</code> 提取 fastq 文件。</p><blockquote><p>fasterq-dump fetches SRR on the fly via HTTP and there could be fatal errors during the transfer.<br>prefetch eliminates transfer problems.</p></blockquote><p>具体操作：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">prefetch --max-size 100000000 [SRR <span class="built_in">id</span>]</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">fasterq-dump [SRR <span class="built_in">id</span>]/[SRR <span class="built_in">id</span>].sra --progress --details --split-files -v --threads [number of threads]</span></span><br></pre></td></tr></tbody></table></figure><p>使用 <code>prefetch</code> 下载的好处有两点：</p><ol><li>sra 文件占用的空间较小，因此下载速度更快。</li><li>下载如果因为某种原因中断，仍可以通过相同的命令进行断点重连。</li></ol><p>以下是一个使用 slurm 调度系统进行批量下载和读取的示例，没有调度系统的朋友也可以直接参考命令进行多下载任务并行：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#SBATCH -J download</span></span><br><span class="line"><span class="comment">#SBATCH -o download.out</span></span><br><span class="line"><span class="comment">#SBATCH -e download.err</span></span><br><span class="line"><span class="comment">#SBATCH -N 1</span></span><br><span class="line"><span class="comment">#SBATCH -n 4</span></span><br><span class="line"></span><br><span class="line">output_dir=<span class="string">"./rawfastq"</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$output_dir</span></span><br><span class="line"><span class="built_in">mkdir</span> -p tmp</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download</span></span><br><span class="line"><span class="built_in">cat</span> download.list | parallel -j 4 --tmpdir ./tmp <span class="string">"prefetch --max-size 100000000 --progress --output-directory <span class="variable">$output_dir</span> {} &gt;<span class="variable">$output_dir</span>/prefetch_{}.log 2&gt;&amp;1"</span></span><br></pre></td></tr></tbody></table></figure><p>其中 <code>download.list</code> 为一行一个 SRR id 的文件。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#SBATCH -J extract</span></span><br><span class="line"><span class="comment">#SBATCH -o extract.out</span></span><br><span class="line"><span class="comment">#SBATCH -e extract.err</span></span><br><span class="line"><span class="comment">#SBATCH -N 1</span></span><br><span class="line"><span class="comment">#SBATCH -n 16</span></span><br><span class="line"></span><br><span class="line">output_dir=<span class="string">"./rawfastq"</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$output_dir</span></span><br><span class="line"><span class="built_in">mkdir</span> -p tmp</span><br><span class="line"></span><br><span class="line"><span class="comment"># Extract</span></span><br><span class="line"><span class="built_in">cat</span> download.list  | parallel -j 4 --tmpdir ./tmp <span class="string">"fasterq-dump <span class="variable">$output_dir</span>/{}/{}.sra* --progress --details --split-files -v --outdir <span class="variable">$output_dir</span> --threads 4 &gt;<span class="variable">$output_dir</span>/fasterq-dump_{}.log 2&gt;&amp;1"</span></span><br><span class="line"><span class="comment"># gzip Compress</span></span><br><span class="line"><span class="built_in">ls</span> <span class="variable">$output_dir</span>/*fastq | parallel -j 16 gzip {}</span><br></pre></td></tr></tbody></table></figure><p>这里使用 <code>.sra*</code> 作为后缀的原因是有时下载的 sra 文件其尾缀可能为 <code>.sralite</code>，具体差别可见 <a href="https://www.ncbi.nlm.nih.gov/sra/docs/sra-data-formats/">SRA Data Formats</a>。</p><p>若想加速提取过程，可以根据自身情况调整并行数量和 <code>fasterq-dump</code> 使用的线程数量。以上方法<strong>经实测非常稳定，对于大文件而言也不会出现报错</strong>。</p><h4 id="ascp"><a href="#ascp" class="headerlink" title="ascp"></a>ascp</h4><p>有时 <code>prefetch</code> 下载速度极缓慢，因此选择速度更快很多的 <code>ascp</code> 也是很好的替代方案。</p><p>conda 下载：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">有 mamba 则用 mamba</span></span><br><span class="line">conda install -c hcc aspera-cli -y</span><br></pre></td></tr></tbody></table></figure><p>在 <a href="https://www.ebi.ac.uk/ena/browser/home">ENA Browser</a> 搜索对应的 Accession Number，</p><img src="/pic2/ENAbroser.png" style="zoom:80%;"><p>勾选 <code>fastq_aspera</code> 后下载 TSV：</p><img src="/pic2/ENAbroser2.png" style="zoom:50%;"><p>根据 <code>fastq_ftp</code> 列，制成以下类似文件：</p><figure class="highlight txt"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/vol1/fastq/ERR418/003/ERR4181783/ERR4181783_1.fastq.gz</span><br><span class="line">/vol1/fastq/ERR418/003/ERR4181783/ERR4181783_2.fastq.gz</span><br></pre></td></tr></tbody></table></figure><p>假设其命名为 <code>download.list</code>，使用以下命令下载：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ascp -QT -k 1 -l 100m -P33001 -i /path/to/asperaweb_id_dsa.openssh --mode recv --user era-fasp --host fasp.sra.ebi.ac.uk --file-list ./download.list [output path]</span><br></pre></td></tr></tbody></table></figure><p>参数详解：</p><ul><li><code>-Q</code>：启用较少详细信息的输出模式（quiet mode）。</li><li><code>-T</code>：启用文件时间戳保留。文件传输完成后，目标文件的时间戳将与源文件相同。</li><li><code>-k 1</code>：断点续传。</li><li><code>-l 100m</code>：限制传输速率，最大传输速率为 100 Mbps。</li><li><code>-P 33001</code>：用于连接的端口号，<code>33001</code> 是 Aspera 使用的默认端口号。</li><li><code>-i /path/to/asperaweb_id_dsa.openssh</code>：指定私钥文件的路径，用于身份验证。位于<strong>安装 ascp 的环境</strong>目录中的 <strong>etc</strong> 下。</li></ul><img src="/pic2/ENAbroser3.png" style="zoom:80%;"><ul><li><code>--mode recv</code>：指定传输模式。<code>recv</code> 表示接收文件（下载）。</li><li><code>--user era-fasp</code>：指定连接使用的用户名。</li><li><code>--host fasp.sra.ebi.ac.uk</code>：指定连接的主机名或 IP 地址。</li><li><code>--file-list ./download.list</code>：指定包含待传输文件列表的文件。</li><li><code>[output path]</code>：存储下载文件的路径，改为自己的实际路径。</li></ul><p>不过需要注意，<code>ascp</code> 下载的数据有时会出现问题，进而导致下游分析如 <code>trim_galore</code> 在过滤 reads 时出错，以下是一些报错示例：</p><blockquote><p>cutadapt: error: Line xxx in FASTQ file is expected to start with ‘xxx’, but found ‘xxx’</p></blockquote><p>可以通过 <code>gunzip</code> 命令检查是否是由于 fastq 文件存在问题：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">gunzip -t SRRxxx.fastq.gz</span></span><br><span class="line"></span><br><span class="line">gzip: SRRxxx.fastq.gz: invalid compressed data--crc error</span><br><span class="line"></span><br><span class="line">gzip: SRRxxx.fastq.gz: invalid compressed data--length error</span><br></pre></td></tr></tbody></table></figure><p>以上情况<strong>可能并不是因为下载过程中的网络问题</strong>，而是 <code>ascp</code> 下载的文件本身存在问题，经实测某些文件下载不存在问题的话，不管下载多少次都不会出错。而某些文件如果下载后存在问题，那么不管下载多少次都会存在问题，这同时也体现了 <code>prefetch</code> 的特点 —— 慢但稳定。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>本文解决方案皆来自 sra-tools Github issue:</p><blockquote><ul><li><p><a href="https://github.com/ncbi/sra-tools/issues/214">fasterq-dump err #214</a></p></li><li><p><a href="https://github.com/ncbi/sra-tools/issues/545">Fasterq-dump failing consistently #545</a></p></li></ul></blockquote></body></html>]]></content>
    
    
    <summary type="html">本文将介绍当使用 fasterq-dump 下载序列不成功时其他的可行替代方法（prefetch 和 ascp） ，同时也提到 ascp 下载 fastq 文件时可能出现的问题和报错。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生信" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E4%BF%A1/"/>
    
    
  </entry>
  
  <entry>
    <title>提取 bwa 比对中唯一比对 reads（uniquely mapped reads）的方法</title>
    <link href="https://biojuse.com/2024/05/03/%E6%8F%90%E5%8F%96%20bwa%20%E6%AF%94%E5%AF%B9%E4%B8%AD%E5%94%AF%E4%B8%80%E6%AF%94%E5%AF%B9%20reads%EF%BC%88uniquely%20mapped%20reads%EF%BC%89%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <id>https://biojuse.com/2024/05/03/%E6%8F%90%E5%8F%96%20bwa%20%E6%AF%94%E5%AF%B9%E4%B8%AD%E5%94%AF%E4%B8%80%E6%AF%94%E5%AF%B9%20reads%EF%BC%88uniquely%20mapped%20reads%EF%BC%89%E7%9A%84%E6%96%B9%E6%B3%95/</id>
    <published>2024-05-03T13:15:00.000Z</published>
    <updated>2024-05-06T06:42:08.274Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>本文由 Juse 基于以下资料进行撰写：</p><blockquote><p><a href="https://www.seqanswers.com/forum/bioinformatics/bioinformatics-aa/49537-how-to-extract-uniquely-mapped-reads-from-bam-sam-produced-by-bwa-mem">How to extract uniquely mapped reads from bam/sam produced by bwa-mem?</a> from SEQanswers</p><p><a href="https://www.biostars.org/p/256448/">Obtaining uniquely mapped reads from BWA mem alignment (filtering by q score does not seem to do the trick in my case)</a> from Biostars</p><p><a href="https://bioinformatics.stackexchange.com/questions/508/obtaining-uniquely-mapped-reads-from-bwa-mem-alignment">Obtaining uniquely mapped reads from BWA mem alignment</a> from StackExchange</p></blockquote><p>在此感谢 community 中各位大佬的无私分享。</p><h3 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h3><p>在某些文章中，有时我们会看到作者提及到在后续分析中，ta 只考虑了 <u><strong>uniquely mapping reads</strong></u>（后文统称唯一比对）。从直觉上出发，使用唯一比对的思路不难理解 —— 这些比对更加精确，并使后续的分析具有更好的解释性。</p><p>目前来说，得到唯一比对的最主流方法是直接通过 MAPQ 进行过滤，但这并不能得到严格意义上的唯一比对，因为部分 reads 在经过该过滤后仍然具有多重比对。不过这里需要明确一点：通过 MAPQ 过滤后仍存在的多重比对数量其实已经很少，如 Biostars 中的帖主所示：</p><blockquote><p>For the one sample I am using to test out commands, I filtered my data so I only have properly paired reads and mapped reads (in the case of my unpaired reads) and also filtered by q score (I get nearly the same results at q of 10, 20, or 30, so it’s not an issue of changing the q score level), merged all my bam files, and obtained 193,998 reads total for that sample. When I open the bam file in Geneious and count the number of reads for each locus they add up to 194, 050. I realize the difference between these two numbers is an incredibly small number of reads.</p></blockquote><p>因此，<u><strong>仅使用 MAPQ 进行过滤看上去也是可行的</strong></u>（如果你能接受极小一部分 “可能具有多个比对” 的 reads）。并且严格意义上说，唯一比对这一概念本身就具有局限性 —— 比对时所使用的参数如果过于宽松，那么具有多重比对的 reads 数量也会大大增加。</p><p>不过，如果实在想要避免多重比对带来的影响，可以根据 bwa 比对中的一些 Alignment Tags 来进行过滤，在 bwa 中，拥有多重比对的 reads 会出现以下一些 Tag：</p><ul><li><code>XA</code>，该 flag 中描述了对应 read 的 Alternative hits。</li><li><code>SA</code>，该 flag 用于标识次要的比对结果（Supplementary Alignment）。</li></ul><p>这两个 flag 的格式非常相似，其中 XA 是 bwa 等一部分比对软件使用的 flag。</p><p>因此，如果要过滤出唯一比对，<u><strong>只需要对这两个 flag 进行筛选</strong></u>：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sambamba view -t 12 -h -f bam -F <span class="string">"mapping_quality &gt;= 1 and not (unmapped or secondary_alignment) and not ([XA] != null or [SA] != null)"</span> [input bam] -o [output bam]</span></span><br></pre></td></tr></tbody></table></figure><p>经过该过滤后，将仅剩下在基因组上有且仅有唯一比对位置的 alignment。</p></body></html>]]></content>
    
    
    <summary type="html">本文介绍了使用 sambamba 提取 bwa 比对中唯一比对 reads 的方法，同时对其中的思路进行了探讨展开。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="杂项" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E6%9D%82%E9%A1%B9/"/>
    
    
  </entry>
  
  <entry>
    <title>bismark 分析 Bisulfite-Seq 数据的流程示例</title>
    <link href="https://biojuse.com/2024/04/20/bismark%20%E5%88%86%E6%9E%90%20Bisulfite-Seq%20%E6%95%B0%E6%8D%AE%E7%9A%84%E6%B5%81%E7%A8%8B%E7%A4%BA%E4%BE%8B/"/>
    <id>https://biojuse.com/2024/04/20/bismark%20%E5%88%86%E6%9E%90%20Bisulfite-Seq%20%E6%95%B0%E6%8D%AE%E7%9A%84%E6%B5%81%E7%A8%8B%E7%A4%BA%E4%BE%8B/</id>
    <published>2024-04-20T03:05:00.000Z</published>
    <updated>2024-04-27T07:29:08.084Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>以下是相关的官网链接和参考资料，如果想要得到除本文内容以外更全面深入的了解，建议直接跳转相关链接进行查阅：</p><blockquote><p>Bismark github:</p><p><a href="https://github.com/FelixKrueger/Bismark">https://github.com/FelixKrueger/Bismark</a></p><p>Bismark documentation:</p><p><a href="https://felixkrueger.github.io/Bismark/">https://felixkrueger.github.io/Bismark/</a></p><p>本文比对及去重等流程均参考自 Bismark documentation</p></blockquote><p>此外，以下内容在本文中不会涉及到，有需要请自行探索：</p><ul><li>数据的质控</li><li>除 Bismark 以外其他软件的安装</li></ul><p>通过以下流程可以得到：</p><ul><li>基因组上 CpG 位点的甲基化信息</li></ul><p>请阅读完全文后再进行相关分析，建议先使用 bismark 的示例数据观察相关命令是否存在问题。</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>下载最新版本的 bismark（至 2024/04/19）：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">wget https://github.com/FelixKrueger/Bismark/archive/refs/tags/v0.24.2.tar.gz</span></span><br></pre></td></tr></tbody></table></figure><p>也可前往 github release 下载指定版本或可能已存在的更新版本：</p><p><a href="https://github.com/FelixKrueger/Bismark/releases">https://github.com/FelixKrueger/Bismark/releases</a></p><p>下载后解压并将对应文件夹置入环境变量：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">tar xzf Bismark-0.24.2.tar.gz</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">echo</span> <span class="string">'export PATH="'</span>$(<span class="built_in">readlink</span> -f Bismark-0.24.2)<span class="string">':$PATH"'</span> &gt;&gt; ~/.bashrc</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">source</span> ~/.bashrc</span></span><br></pre></td></tr></tbody></table></figure><p>由于 bismark 中比对等操作均由 bismark 本身调用比对软件进行，因此在运行后续代码前，请确保正确版本的 <code>Bowtie2</code> 或 <code>HISAT2</code> 已经被置入环境变量中，如果想自行指定，可在后续执行时添加以下参数：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--path_to_bowtie2 &lt;/../../bowtie2&gt; or</span><br><span class="line">--path_to_hisat2 &lt;/../../hisat2&gt;</span><br></pre></td></tr></tbody></table></figure><p>bismark 默认使用 bowtie2，除非特别指定 <code>--hisat2</code>。</p><h3 id="比对"><a href="#比对" class="headerlink" title="比对"></a>比对</h3><p>比对前，使用 bismark 构建基因组 <code>C&gt;T</code> <code>G&gt;A</code> 的索引：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bismark_genome_preparation --verbose --parallel [thread] [path]</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>path</code> 换为基因组序列文件储存的路径，bismark 将自动识别 <code>.fa</code> 及 <code>.fasta</code> 文件并建立索引（压缩格式 <code>.gz</code> 也能识别）。</li><li><code>thread</code> 处填入使用的线程数量，注意最终将会使用 <code>2 * thread</code> 个线程。</li></ul><p>在构建完索引后，就可以通过 bismark 对数据进行比对，在比对前，建议先记录版本信息：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bismark --version &gt; bismark.version 2&gt;&amp;1</span></span><br></pre></td></tr></tbody></table></figure><p>比对命令（单端）：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bismark -L [seed length] -N [mismatch number] --genome [path] --parallel [thread] -o [output_dir] [fastq] &gt; [logfile] 2&gt;&amp;1</span></span><br></pre></td></tr></tbody></table></figure><p>比对命令（双端）：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bismark -L [seed length] -N [mismatch number] --genome [path] --parallel [thread] -o [output_dir] -1 [fastq_R1] -2 [fastq_R2] &gt; [logfile] 2&gt;&amp;1</span></span><br></pre></td></tr></tbody></table></figure><p>相关参数解释：</p><ul><li><code>seed length</code> 指定比对过程中使用的基础片段的长度（默认为 20），最大可设置为 32，该值越小时比对越慢，但同时也增加了召回率。该选项只在 Bowtie2 可用。</li><li><code>mismatch number</code> 指定比对过程中基础片段允许的错配数量（默认为 0）。可设置为 1，设置为 1 时比对会减慢很多，但同样增加了召回率。该选项只在 Bowtie2 可用。</li><li><code>thread</code> 指定并行的实例，注意其不是单纯的线程数，由于一个 <code>bismark</code> 进程就已经使用了多个核，以小鼠基因组为例，该值设置为 4 时将会使用大约 20 个核以及 40GB 左右的 RAM。因此请根据基因组大小、空闲的内存及核数量对该参数进行调整。</li><li><code>path</code> 即构建索引时使用的 <code>path</code>。</li><li><code>output_dir</code> 为输出的目录，<code>fastq</code> 指定测序数据文件，<code>logfile</code> 记录比对过程中的屏幕输出。</li></ul><p><strong>注意！</strong>如果测序数据由 <a href="https://www.illumina.com/science/sequencing-method-explorer/kits-and-arrays/pbat.html">PBAT</a> 建库得到，需要添加 <code>--pbat</code> 参数。此外，bismark 也提供了 <code>--local</code> 参数，指定此参数可以提高该文库类型的 map 比率，但是在 documentation 中 bismark 团队并不推荐这么做。</p><p>比对完成后，请先检查比对情况，相关的比对报告将储存在 <code>output_dir</code> 中以 <code>_report.txt</code> 结尾的文件里，如果是单端数据且使用 Bowtie2 比对，则相关报告文件将以 <code>_bismark_bt2_SE_report.txt</code> 结尾。打开文件，检查其中 <u>Mapping efficiency</u> 一项：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Final Alignment report</span><br><span class="line">======================</span><br><span class="line">Sequences analysed in total:1157278848</span><br><span class="line">Number of alignments with a unique best hit from the different alignments:736261309</span><br><span class="line">Mapping efficiency:63.6%</span><br><span class="line">Sequences with no alignments under any condition:337204612</span><br><span class="line">Sequences did not map uniquely:83812927</span><br><span class="line">Sequences which were discarded because genomic sequence could not be extracted:29</span><br></pre></td></tr></tbody></table></figure><p>如果 Mapping efficiency 在可接受范围内，则可进行后续分析。若该值过低（例如不超过 30%），则需考虑更改过滤参数以提升该值。一个折中的选择是适当提升 <code>seed length</code> 并设置 <code>mismatch number</code> 为 1。</p><p>考虑到比对的用时都较长，因此在第一次比对时就应尽量根据实际情况决定一个好的参数起点，例如：</p><ul><li>测序数据的 reads 长度？是单端还是双端？</li><li>测序数据的质量如何？</li></ul><h3 id="去重"><a href="#去重" class="headerlink" title="去重"></a>去重</h3><p>基因组上相同位置的比对可能是由 PCR 扩增导致的，删除重复的 reads 有助于避免 PCR 引入的序列 bias 并提高后续相关分析的准确性。但请注意，<strong>不建议对目标富集型文库（target enrichment-type library）例如 RRBS 及 amplicon 等进行该操作</strong>，因为在这些类型的文库中，重复序列可能是有意义的。</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">deduplicate_bismark --version &gt; deduplicate_bismark.version 2&gt;&amp;1</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">deduplicate_bismark --output_dir [output_dir] --outfile [prefix] [files] &gt; [logfile] 2&gt;&amp;1</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>output_dir</code> 填入去重后的文件输出目录。</li><li><code>prefix</code> 填入去重后的文件前缀名称。</li><li><code>files</code> 填入需要去重的文件。<code>logfile</code> 填入记录屏幕输出的日志文件。</li></ul><p>如果此处想要将多个比对结果放在一起进行去重，则需指定 <code>--multiple</code> 参数，此时 bismark 将会把所有的输入文件视为单个样本连接在一起并进行去重。</p><p>最后的去重结果将在以 <code>.deduplication_report.txt</code> 结尾的文件中，该文件里将展示共有多少比对被移除。</p><h3 id="统计甲基化信息"><a href="#统计甲基化信息" class="headerlink" title="统计甲基化信息"></a>统计甲基化信息</h3><p>该步使用的 <code>bismark_methylation_extractor</code> 命令具有非常多的可选参数，建议通过 <code>bismark_methylation_extractor --help</code> 查看详细信息，此处给出个人使用的命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bismark_methylation_extractor --version &gt; bismark_methylation_extractor.version 2&gt;&amp;1</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bismark_methylation_extractor --gzip --bedGraph [bamfile] --output_dir [output_dir] --parallel [thread] &gt; [logfile] 2&gt;&amp;1</span></span><br></pre></td></tr></tbody></table></figure><p><code>bamfile</code> 处填入经过上述处理后得到的最终 bam 文件。此处 <code>--gzip</code> 表示输出文件以 gzip 进行压缩，<code>--bedgraph</code> 使 bismark 输出 bedgraph 文件。也可不使用 <code>--bedgraph</code> 参数并通过结果文件自行处理生成。</p><p>通过上述命令将得到以下尾缀结尾的文件：</p><ul><li><code>.bismark.cov.gz</code> 该文件共六列，分别代表：染色体、起始位点（1 坐标）、终止位点、甲基化比例、甲基化的 read count、非甲基化的 read count。</li><li><code>.bedGraph.gz</code> 该文件共四列，分别代表：染色体、起始位点（0 坐标）、终止位点、甲基化比例。</li><li><code>.M-bias.txt</code> 该文件展示了 reads 中每个 position 的平均甲基化水平，可通过画图可视化，偏离水平线可能表示存在 bias [<a href="https://www.nature.com/articles/nrg3273#glossary">Bock 2012, Nat Rev Genet</a>]。</li><li><code>_splitting_report.txt</code> 相关信息的统计汇总。</li></ul><p>通过 <code>.bismark.cov.gz</code> 生成 <code>.bedGraph.gz</code> 的方式：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">zcat xxx.bismark.cov.gz | awk -F<span class="string">'\t'</span> <span class="string">'OFS="\t" {print $1, $2-1, $3, $4}'</span> | gzip &gt; xxx.bedGraph.gz</span></span><br></pre></td></tr></tbody></table></figure><p>此后可以通过 bedgraph 文件生成 bigWig 文件，请注意，由于 <code>bedGraphToBigWig</code> 并没法输入压缩文件，因此你可能需要对上述文件进行解压，或者不使用 <code>gzip</code> 进行压缩：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bedGraphToBigWig xxx.bedGraph genome.chrom.sizes output.bw</span></span><br></pre></td></tr></tbody></table></figure><p>关于 <code>chrom.sizes</code> 文件的获取方式可参考之前的 <a href="https://biojuse.com/2024/04/16/%E9%80%9A%E8%BF%87%20bowtie2%20+%20macs3%20%E5%88%86%E6%9E%90%20ChIP-Seq%20%E6%95%B0%E6%8D%AE/#Signal-P-value-amp-Fold-change-over-control">ChIP-seq 文章</a>。</p><p>以下是 bismark documentation 中给出的示例 M-bias plot（仅为示例，并不代表这种属于正常情况）：</p><img src="https://felixkrueger.github.io/Bismark/images/PBAT_SE_M-bias.jpg"><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>bisulfite-seq 数据的分析也可以通过 <a href="https://github.com/heathsc/gemBS">gemBS</a> Pipeline 直接进行，请注意该 Pipeline 目前仅支持双端数据。</p><p>也不必过多担心参数的选择，大多数情况下可能仅有 <code>bismark</code> 的 <code>-L</code> 和 <code>-N</code> 需要进行调整，如果不知道应该设置多少为宜建议统一使用默认参数，再根据结果进行修改。</p></body></html>]]></content>
    
    
    <summary type="html">本文将介绍如何使用 bismark 分析 Bisulfite-Seq 数据，同时也将提及到一些设定参数时需要注意的事项。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生信" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E4%BF%A1/"/>
    
    
    <category term="生物信息学" scheme="https://biojuse.com/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6/"/>
    
    <category term="生信" scheme="https://biojuse.com/tags/%E7%94%9F%E4%BF%A1/"/>
    
  </entry>
  
  <entry>
    <title>通过 bowtie2 + macs3 分析 ChIP-Seq 数据</title>
    <link href="https://biojuse.com/2024/04/16/%E9%80%9A%E8%BF%87%20bowtie2%20+%20macs3%20%E5%88%86%E6%9E%90%20ChIP-Seq%20%E6%95%B0%E6%8D%AE/"/>
    <id>https://biojuse.com/2024/04/16/%E9%80%9A%E8%BF%87%20bowtie2%20+%20macs3%20%E5%88%86%E6%9E%90%20ChIP-Seq%20%E6%95%B0%E6%8D%AE/</id>
    <published>2024-04-16T03:30:00.000Z</published>
    <updated>2024-07-09T05:05:17.840Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><hr><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>以下是相关的官网链接和参考资料，如果想要得到除本文内容以外更全面深入的了解，建议直接跳转相关链接进行查阅：</p><blockquote><p>MACS3 github:</p><p><a href="https://github.com/macs3-project/MACS">https://github.com/macs3-project/MACS</a></p><p>MACS3 documentation:</p><p><a href="https://macs3-project.github.io/MACS/">https://macs3-project.github.io/MACS/</a></p><p>本文比对及去重等流程参考自以下资料：</p><ul><li><a href="https://hbctraining.github.io/Intro-to-ChIPseq/lessons/03_align_and_filtering.html">Alignment and filtering</a> from Harvard Chan Bioinformatics Core (HBC)</li><li><a href="https://www.encodeproject.org/pipelines/ENCPL612HIG/">Histone ChIP-seq pipeline</a> from ENCODE project (<strong>Version 2.0</strong> with bowtie2)</li></ul><p>本文流程根据以下资料进行过优化：</p><ul><li><a href="https://github.com/biod/sambamba/issues/477">sambamba -F “not duplicate” processed bam still have duplicated</a> (sambamba issue#477) (Update 2024/4/24)</li><li><a href="https://www.nature.com/articles/s41598-019-45839-z">Encode blacklist</a> (Update 2024/4/24)</li></ul></blockquote><p>以下内容在本文中不会涉及到：</p><ul><li>数据的质控</li><li>除 macs3 以外其他软件的安装</li></ul><p>你可以考虑参考以下 github repository 安装用于 ChIP-seq 数据处理的 conda 环境，可补齐下文中需要的上游分析软件：</p><p><a href="https://github.com/JuseTiZ/Blog-Snakemake-pipeline/">https://github.com/JuseTiZ/Blog-Snakemake-pipeline/</a></p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>使用 pip 安装 macs3：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install macs3</span></span><br></pre></td></tr></tbody></table></figure><p>如果安装过程中无法安装依赖 <code>cykhash</code>，可以尝试使用 mamba 进行安装：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">mamba install -c conda-forge cykhash</span></span><br></pre></td></tr></tbody></table></figure><p>安装后可以输入 <code>macs3 --version</code> 查看是否成功安装。</p><h3 id="比对"><a href="#比对" class="headerlink" title="比对"></a>比对</h3><p>比对前，使用 bowtie2 构建基因组的索引：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bowtie2-build genome.fa indexname</span></span><br></pre></td></tr></tbody></table></figure><p>将以上代码中的 <code>genome.fa</code> 替换为自己的基因组文件，<code>indexname</code> 替换为想要设定的索引名称，后续比对将用到 <code>indexname</code>。</p><p>构建后，对数据进行比对得到 sam 文件：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bowtie2 --threads 4 \</span></span><br><span class="line"><span class="language-bash">       -q -x indexname -U chip-seq.fastq.gz \</span></span><br><span class="line"><span class="language-bash">       -S output.sam &gt; bowtie2.log 2&gt;&amp;1</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>--threads</code> 使用的线程数量</li><li><code>-X</code> 指定索引</li><li><code>-U</code> 质控后的测序数据文件</li><li><code>-S</code> 输出的 sam 文件</li></ul><p>将 <code>indexname</code> 替换为之前建立的索引名称，将 <code>chip-seq.fastq.gz</code> 替换为 chip-seq 的数据，如果是双端则需进行一定修改，考虑到 chip-seq 数据一般都是单端数据此处不做拓展。该过程的屏幕输出将记录在 <code>bowtie2.log</code> 中（例如比对率等）。</p><p>以上命令在 slurm 调度系统下的一个并行示例：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#SBATCH -J alignment</span></span><br><span class="line"><span class="comment">#SBATCH -o alignment.out</span></span><br><span class="line"><span class="comment">#SBATCH -e alignment.err</span></span><br><span class="line"><span class="comment">#SBATCH -n 24</span></span><br><span class="line"></span><br><span class="line">input_dir=<span class="string">"./fastq"</span></span><br><span class="line">output_dir=<span class="string">"./bowtie2_alignment"</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="string">"<span class="variable">${output_dir}</span>"</span></span><br><span class="line"></span><br><span class="line">bowtie2 --version &gt; bowtie2.version 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">align_fastq</span></span>() {</span><br><span class="line">    <span class="built_in">local</span> fastq_file=<span class="string">"<span class="variable">$1</span>"</span></span><br><span class="line">    <span class="built_in">local</span> sam_file=<span class="string">"<span class="variable">${output_dir}</span>/<span class="subst">$(basename <span class="string">"<span class="variable">${fastq_file}</span>"</span> .fq.gz)</span>.sam"</span></span><br><span class="line">    <span class="built_in">local</span> log_file=<span class="string">"<span class="variable">${output_dir}</span>/<span class="subst">$(basename <span class="string">"<span class="variable">${fastq_file}</span>"</span> .fq.gz)</span>.log"</span></span><br><span class="line"></span><br><span class="line">    bowtie2 --threads 4 -q -x indexname -U <span class="string">"<span class="variable">$fastq_file</span>"</span> -S <span class="string">"<span class="variable">$sam_file</span>"</span> &gt; <span class="string">"<span class="variable">$log_file</span>"</span> 2&gt;&amp;1</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> -f align_fastq</span><br><span class="line"><span class="built_in">export</span> input_dir</span><br><span class="line"><span class="built_in">export</span> output_dir</span><br><span class="line"></span><br><span class="line">parallel --<span class="built_in">jobs</span> 6 align_fastq ::: <span class="string">"<span class="variable">${input_dir}</span>"</span>/*.fq.gz</span><br></pre></td></tr></tbody></table></figure><p>上述脚本将从当前目录下的 <code>fastq</code> 文件夹里读取所有 <code>.fq.gz</code> 结尾的文件，并通过 <code>bowtie2</code> 进行比对，索引为 <code>indexname</code>。该过程将并行 6 个比对任务，每个比对任务使用 4 个线程，通过 slurm 使用 24 个 CPU。</p><p>请根据个人情况（如空闲内存、集群信息等）进行修改以保证正常运行。在比对结束后，请对相关 log 文件进行检查，确保比对率等信息未出现问题。</p><h3 id="排序及去重"><a href="#排序及去重" class="headerlink" title="排序及去重"></a>排序及去重</h3><p>由于 PCR 过程中可能部分片段有 bias（被过度扩增），将这些片段去重将有助于降低 call peak 时的假阳性。</p><p>单个运行时的示例：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">samtools view -h -S -b -o [output_unsorted_bam] [input_sam_file] <span class="comment"># 转为 bam</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sambamba <span class="built_in">sort</span> -t 4 -o [output_sorted_bam] [input_unsorted_bam] <span class="comment"># 对 bam 排序</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sambamba markdup -t 4 [output_markdup_bam] [input_sorted_bam] <span class="comment"># 标记 duplicate</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sambamba view -h -t 4 -f bam -F <span class="string">"[XS] == null and not unmapped and not duplicate"</span> [input_markdup_bam] &gt; [output_filter_bam] <span class="comment"># 过滤及去重</span></span></span><br></pre></td></tr></tbody></table></figure><p>slurm 调度系统下的批量运行示例：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#SBATCH -J sam2bam</span></span><br><span class="line"><span class="comment">#SBATCH -o sam2bam.out</span></span><br><span class="line"><span class="comment">#SBATCH -e sam2bam.err</span></span><br><span class="line"><span class="comment">#SBATCH -n 24</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define directories</span></span><br><span class="line">input_dir=<span class="string">"./bowtie2_alignment"</span></span><br><span class="line">output_dir=<span class="string">"./bamfiles"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create output directory</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="string">"<span class="variable">${output_dir}</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Record versions</span></span><br><span class="line">samtools --version &gt; samtools.version 2&gt;&amp;1</span><br><span class="line">sambamba --version &gt; sambamba.version 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Function to process each SAM file into BAM format</span></span><br><span class="line"><span class="function"><span class="title">process_sam_to_bam</span></span>() {</span><br><span class="line">    <span class="built_in">local</span> sam_file=<span class="string">"<span class="variable">$1</span>"</span></span><br><span class="line">    <span class="built_in">local</span> base_name=<span class="string">"<span class="variable">${output_dir}</span>/<span class="subst">$(basename <span class="string">"<span class="variable">${sam_file}</span>"</span> .sam)</span>"</span></span><br><span class="line">    <span class="built_in">local</span> unsorted_bam=<span class="string">"<span class="variable">${base_name}</span>.unsorted.bam"</span></span><br><span class="line">    <span class="built_in">local</span> sorted_bam=<span class="string">"<span class="variable">${base_name}</span>.sorted.bam"</span></span><br><span class="line">    <span class="built_in">local</span> marked_bam=<span class="string">"<span class="variable">${base_name}</span>.marked.bam"</span></span><br><span class="line">    <span class="built_in">local</span> final_bam=<span class="string">"<span class="variable">${base_name}</span>.bam"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert SAM to BAM</span></span><br><span class="line">    samtools view -h -S -b -o <span class="string">"<span class="variable">${unsorted_bam}</span>"</span> <span class="string">"<span class="variable">${sam_file}</span>"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Sort BAM file</span></span><br><span class="line">    sambamba <span class="built_in">sort</span> -t 4 -o <span class="string">"<span class="variable">${sorted_bam}</span>"</span> <span class="string">"<span class="variable">${unsorted_bam}</span>"</span></span><br><span class="line">    <span class="built_in">rm</span> <span class="string">"<span class="variable">${unsorted_bam}</span>"</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Mark duplicate</span></span><br><span class="line">    sambamba markdup -t 4 <span class="string">"<span class="variable">${sorted_bam}</span>"</span> <span class="string">"<span class="variable">${marked_bam}</span>"</span></span><br><span class="line">    <span class="built_in">rm</span> <span class="string">"<span class="variable">${sorted_bam}</span>"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Filter BAM file</span></span><br><span class="line">    sambamba view -h -t 4 -f bam -F <span class="string">"[XS] == null and not unmapped and not duplicate"</span> <span class="string">"<span class="variable">${marked_bam}</span>"</span> &gt; <span class="string">"<span class="variable">${final_bam}</span>"</span></span><br><span class="line">    <span class="built_in">rm</span> <span class="string">"<span class="variable">${marked_bam}</span>"</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment"># Export function and variables for parallel execution</span></span><br><span class="line"><span class="built_in">export</span> -f process_sam_to_bam</span><br><span class="line"><span class="built_in">export</span> input_dir</span><br><span class="line"><span class="built_in">export</span> output_dir</span><br><span class="line"></span><br><span class="line"><span class="comment"># Execute processing in parallel</span></span><br><span class="line">parallel --<span class="built_in">jobs</span> 6 process_sam_to_bam ::: <span class="string">"<span class="variable">${input_dir}</span>"</span>/*.sam</span><br></pre></td></tr></tbody></table></figure><p>以上注释均通过 ChatGPT 添加，请确保可以通过环境变量直接调用 <code>samtools</code> 和 <code>sambamba</code>。该脚本主要做了以下工作：</p><ul><li><code>samtools</code> 将 <code>sam</code> 转换为 <code>bam</code> 格式。</li><li><code>sambamba</code> 标记重复 reads。</li><li><code>sambamba</code> 对文件进行排序和索引，此后对其进行去重。</li></ul><p>以下是过滤规则：</p><ul><li><p><code>[XS] == null</code>：</p><ul><li>过滤掉具有 <code>XS</code> 标签的比对，选出比对位置唯一或最佳的比对（primary alignments），即没有其他次优的比对位置。</li></ul></li><li><p><code>not unmapped</code>：</p><ul><li>过滤未成功比对到参考基因组的序列。</li></ul></li><li><p><code>not duplicate</code>：</p><ul><li>过滤掉标记为重复的比对。在使用 PCR 方法扩增样本时，会产生重复的 reads。</li></ul></li></ul><p>通过上述命令，就能得到所有后续分析中 macs3 所需要使用到的 bam 文件。如果想要将不同 replicate 的数据放在一起获得最终 bedGraph 文件，可在该步后对同一处理下所有 bam 文件进行合并。</p><h3 id="Filtering-out-Blacklisted-Regions"><a href="#Filtering-out-Blacklisted-Regions" class="headerlink" title="Filtering out Blacklisted Regions"></a>Filtering out Blacklisted Regions</h3><p>详细可见：<a href="https://www.nature.com/articles/s41598-019-45839-z">The ENCODE Blacklist: Identification of Problematic Regions of the Genome</a></p><p>基因组中部分区域的序列高度重复，此外现有基因组版本也可能存在基因组组装错误所导致的 artifact。这些因素都可能导致 signal 出现异常，从而增加假阳性。解决这一问题的方法之一是过滤掉这些区域中的 alignment。</p><p>一些模式物种的 blacklist 文件可以在以下 github repositories 获得：</p><p><a href="https://github.com/Boyle-Lab/Blacklist/">https://github.com/Boyle-Lab/Blacklist/</a></p><p><a href="https://github.com/dozmorovlab/excluderanges">https://github.com/dozmorovlab/excluderanges</a></p><p>请注意选择正确的基因组版本，如果目标物种的 blacklist 文件并不存在于以上 repositories 中，可以自行通过 Encode Blacklist 软件识别。</p><p>以下是使用 deeptools <code>alignmentSieve</code> 过滤 blacklist 的方法：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">alignmentSieve --blackListFileName blacklist.bed.gz --filterMetrics filterMetricBlacklist.txt -p 8 -b input.bam -o output.bam</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>--blackListFileName</code> 指定 blacklist region file。</li><li><code>-p</code> 指定使用线程数，<code>-b</code> 和 <code>-o</code> 指定输入和输出 bam 文件。</li></ul><p>如果过滤的为 ATAC-seq 数据，则还需添加 <code>--ATACshift</code> 参数。在 <code>filterMetricBlacklist.txt</code> 中可以看到总 reads 数和过滤后留存的 reads 数。</p><p>由于该步骤并非必需，因此个人看需求选择是否加入到 <strong><u>排序及去重</u></strong> 部分的 Pipeline 中。</p><h3 id="macs3-call-peak"><a href="#macs3-call-peak" class="headerlink" title="macs3 call peak"></a>macs3 call peak</h3><p>一般而言，CHIP-seq（染色质免疫沉淀测序）实验中，通常包括 treat 组和 control 组，后者用于提供实验的背景信号，帮助区分特异性的信号和非特异性的背景噪音。通常有两种主要的对照方式：</p><ul><li><strong>Input control</strong>：在这种对照中，不进行抗体沉淀，直接使用同一样本的一部分进行 DNA 提取和测序。</li><li><strong>Mock IP control</strong>：这种对照使用不特异的抗体（如与目标蛋白无关的抗体或前清液）来进行免疫沉淀。</li></ul><p>因此，假设目前已经有了 treat 和 control 的 bam files，那么可以通过以下命令来进行 callpeak：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">macs3 callpeak -t [treat bam] -c [control bam] -f BAM -n [outname] --broad -g [species] -B --broad-cutoff 0.1</span></span><br></pre></td></tr></tbody></table></figure><p>以上命令适用于 <u>broad peak calling on Histone Mark ChIP-seq</u>，如果是其他 ChIP-seq 数据，请根据 github 上的 usage 示例进行相应修改。以 TF ChIP-seq 为例，示例命令如下：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">macs3 callpeak -t [treat bam] -c [control bam] -f BAM -g [species] -n [outname] -B -q 0.01</span></span><br></pre></td></tr></tbody></table></figure><p>由于 macs3 的 <code>bdgdiff</code> 只能处理一个重复下的情况，因此如果单个样本有多个重复，在 <code>callpeak</code> 步骤中可以一起输入（即 <code>-t</code> 和 <code>-c</code> 可以接多个 bam 文件）。或者也可以选择每个重复单独进行 <code>callpeak</code>，然后使用其他工具例如 <code>DESeq2</code> 或 <code>edgeR</code> 进行后续的比较分析。</p><p>这里将对应参数后的输入依据自己的实际情况和需求进行修改，着重提几个参数：</p><ul><li><p><code>-n</code> 输出文件的前缀（可包括输出路径）。</p></li><li><p><code>-g</code> 有效基因组大小，目前 macs3 内置有：</p></li></ul><blockquote><p>  hs for human (2,913,022,398)</p><p>  mm for mouse (2,652,783,500)</p><p>  ce for C. elegans (100,286,401)</p><p>  dm for fruitfly (142,573,017)</p><p>  Default:hs</p></blockquote><ul><li><code>-B</code> 生成 bedgraph 文件，该文件可用于后续计算得到 signal p-value 或 fold change over control 的 bigWig 文件。</li></ul><p>如果分析的物种有效基因组大小在 macs3 中没有内置，且在 deeptools 上也没给出，可以通过以下方法进行计算：</p><ul><li><p>下载 <code>faCount</code>：<code>wget https://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/faCount</code></p></li><li><p>统计物种非 N 碱基数量：<code>faCount genome.fa -summary &gt; genome.faCount.report</code></p></li><li><p>计算：<code>awk 'NR==2 {print $3+$4+$5+$6}' genome.faCount.report</code></p></li></ul><p>将 faCount 后的 <code>genome.fa</code> 替换为自己的基因组文件后，就能得到相应的信息，例如 rheMac10 基因组的非 N 碱基数量为 2936892725。此外也可以 <code>uniquely mappable regions</code> 确定。关于这两种方法的区别，可见 <a href="https://deeptools.readthedocs.io/en/develop/content/feature/effectiveGenomeSize.html">Effective Genome Size</a>。</p><p>由于 macs3 的默认值均根据非 N 碱基数量确定，因此这里只讨论 faCount。</p><p>确定相关参数后，<code>macs3 callpeak</code> 将产生相关的 peak 信息文件，若仅需要 peak 信息则进行到该步即完成任务。不添加 <code>-B</code> 参数时会生成的一些文件：</p><ul><li><code>*_peaks.narrowPeak</code>：包含了每个 peak 的详细信息，包括峰的起始和终止位置、峰的名称、峰的得分、以及峰顶相对于峰起始位置的偏移值。</li><li><code>*_summits.bed</code>：包含了每个 peak 的峰顶位置，即信号最强的点。每一行通常表示一个峰顶，包含染色体、峰顶的起始位置、终止位置（起始位置+1），以及峰的强度值。</li><li><code>*_peaks.xls</code>：详细结果文件，包含产生该文件所使用的命令，各参数的情况，以及峰的各个信息等。</li></ul><h3 id="Signal-P-value-amp-Fold-change-over-control"><a href="#Signal-P-value-amp-Fold-change-over-control" class="headerlink" title="Signal P-value &amp; Fold change over control"></a>Signal P-value &amp; Fold change over control</h3><p>如果在 call peak 步骤中添加了 <code>-B</code> 参数，那么此时应该就已经生成了 treat 组和 control 组的 bed 文件，通过比较这两个 bed 文件，可以得到基因组上不同位置中 peak 的 <code>signal p-value</code> 或者 <code>fold change over control</code>。</p><p>以下是一个运行示例，该例子将产生 Signal P-value 文件，其中第四列为 <code>-log10(pvalue)</code>，因此第四列越大，该处的 peak 越显著：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">macs3 bdgcmp -t treatment.bedGraph -c control.bedGraph -m ppois -p 1.0 -S 1.0 -o output.bedGraph</span></span><br></pre></td></tr></tbody></table></figure><p>各参数含义详细可见 <a href="https://github.com/macs3-project/MACS/blob/master/docs/bdgcmp.md">documentation</a>，这里提几个重要的：</p><ul><li><code>-m</code> 计算第四列 score 时所用的方法，<code>ppois</code> 即泊松 p 值，选择 <code>FE</code> 则能得到 <code>fold change over control</code> 信息。</li><li><code>-p</code> 为每个计数添加 x 个伪计数，尤其在对数转换时可以避免计数为 0 造成的计算问题，此外也有助于减少因样本稀疏导致的统计波动。</li><li><code>-S</code> 设置处理组（treatment）和对照组（control）数据的缩放因子，如果在 call peak 时未指定 <code>--SPMR</code> 参数，则该项设定为 1 即可。</li></ul><p>对于输出后的 bedgraph 文件，可以通过 <code>bedGraphToBigWig</code> 转为 bigWig 文件：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">wget https://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/bedGraphToBigWig <span class="comment"># 下载工具</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bedGraphToBigWig output.bedGraph genome.chrom.sizes output.bw</span></span><br></pre></td></tr></tbody></table></figure><p>每个物种的 <code>genome.chrom.sizes</code> 文件可在相应的 UCSC Genome Browser goldenpath 中得到，如果所分析物种尚未在基因组浏览器上记载，可自行通过 <code>samtools</code> 或 <code>Biopython</code> 得到。例：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">samtools faidx reference.fasta</span><br><span class="line"><span class="built_in">cut</span> -f1,2 reference.fasta.fai &gt; chrom.sizes</span><br></pre></td></tr></tbody></table></figure><p>如果此处存在一个样本下有多个重复的情况，并且已经对每个重复都单独计算得到了 bed 文件，你也可以通过 macs3 的 <code>cmbreps</code> 对不同的重复进行合并。例如以 <code>signal p-value</code> 为例：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">macs3 cmbreps -i replicate1.bedGraph replicate2.bedGraph replicate3.bedGraph -o combined.bedGraph --method fisher</span></span><br></pre></td></tr></tbody></table></figure><p>其通过费舍尔联合概率检验合并 p 值并计算得到新 p 值。</p><p>对于 <code>fold change over control</code>，可将 <code>-m</code> 后的 <code>fisher</code> 换为 <code>mean</code> 进行。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>以上流程已被打包为一个完整的 Snakemake Pipeline，可在以下链接中获取：</p><p><a href="https://github.com/JuseTiZ/Blog-Snakemake-pipeline/tree/main/ChIP-Pipeline">https://github.com/JuseTiZ/Blog-Snakemake-pipeline/tree/main/ChIP-Pipeline</a></p><p>该 Pipeline 的理解和使用可能需要一定的 Snakemake 语法基础。</p><p>考虑到 ChIP-seq 下游分析能弄的东西五花八门，所以这篇文章就只介绍到如何产生结果，对于相关结果的解释和分析就各看需求了。</p></body></html>]]></content>
    
    
    <summary type="html">本文将介绍如何使用 bowtie2 + macs3 分析 ChIP-seq 数据，同时也涉及到报错解决和非模式物种的参数确定等。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生信" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E4%BF%A1/"/>
    
    
    <category term="生物信息学" scheme="https://biojuse.com/tags/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6/"/>
    
    <category term="生信" scheme="https://biojuse.com/tags/%E7%94%9F%E4%BF%A1/"/>
    
  </entry>
  
  <entry>
    <title>远程服务器无 root 权限时 LightGBM GPU 版的安装方法</title>
    <link href="https://biojuse.com/2024/04/02/%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%97%A0%20root%20%E6%9D%83%E9%99%90%E6%97%B6%20LightGBM%20GPU%20%E7%89%88%E7%9A%84%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95/"/>
    <id>https://biojuse.com/2024/04/02/%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%97%A0%20root%20%E6%9D%83%E9%99%90%E6%97%B6%20LightGBM%20GPU%20%E7%89%88%E7%9A%84%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95/</id>
    <published>2024-04-02T13:40:00.000Z</published>
    <updated>2024-04-02T13:01:13.458Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>首先感谢 LightGBM 团队的帮助：</p><p><a href="https://github.com/microsoft/LightGBM/issues/6399">https://github.com/microsoft/LightGBM/issues/6399</a></p><h3 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h3><p>本文适用于在远程服务器（Linux）上安装 LightGBM 的情况。</p><p>不同的编译器版本在遇到报错时的解决方案可能略有不同，以下是我在编译时使用的相关版本：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-- The C compiler identification is GNU 8.5.0</span><br><span class="line">-- The CXX compiler identification is GNU 4.8.5</span><br></pre></td></tr></tbody></table></figure><p>本文默认已在服务器上配置有 CUDA，如果 CUDA 另有路径请通过 <code>-DOpenCL_LIBRARY</code> 和 <code>-DOpenCL_INCLUDE_DIR</code> 指定。</p><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>下载 LightGBM，并进行 <code>CUDA</code> 版本的编译：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive https://github.com/microsoft/LightGBM</span><br><span class="line">cd LightGBM</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake -DUSE_CUDA=1 .. # 指定 CUDA version</span><br><span class="line">make -j4</span><br></pre></td></tr></tbody></table></figure><blockquote><p>注：</p><p>不知道是否属于个人原因，如果我使用 <code>-DUSE_GPU=1</code> 而非 <code>-DUSE_CUDA=1</code>（即安装 GPU version 而非 CUDA version），虽然最后能安装成功，但是在 Jupyter Notebook 中运行 LightGBM GPU 时会直接发生以下错误：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click [here](https://aka.ms/vscodeJupyterKernelCrash) for more info. View Jupyter [log](command:jupyter.viewOutput) for further details.</span><br></pre></td></tr></tbody></table></figure></blockquote><p>在上述 CUDA version 编译过程中，如果在 CMake 时发生以下错误：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x86_64-conda-linux-gnu-cc: error: unrecognized command-line option '-march'</span><br></pre></td></tr></tbody></table></figure><p>解决方案：打开 <code>build</code> 文件夹中的 <code>CMakeCache.txt</code>，搜索 <code>-march</code>，找到类似以下这段的内容：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//Flags used by the C compiler during all build types.</span><br><span class="line">CMAKE_C_FLAGS:STRING=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /public/home/xxxx/mambaforge/envs/kaggle/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /public/home/xxxx/mambaforge/envs/ncsvp/include -march</span><br></pre></td></tr></tbody></table></figure><p>将<strong>最后的</strong> <code>-march</code> 删除再重新运行 <code>cmake -DUSE_CUDA=1 ..</code>，如果成功则继续运行 <code>make -j4</code>。</p><p>如果与上述相关内容有所出入，则可自己根据具体情况尝试修改 <code>march</code> 相关内容。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>编译完成后，你已经可以使用 Linux 下的 LightGBM，如果要继续安装 Python LightGBM，则还需进行以下操作：</p><p>返回到 <code>LightGBM</code> 目录，并安装 Python 接口：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> ../</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sh ./build-python.sh install --precompile</span></span><br></pre></td></tr></tbody></table></figure><p>安装好 LightGBM Module 后，就可以使用 <code>device='CUDA'</code> 调用 GPU 进行加速。</p><p>关于以上流程，更详细内容可以移至文章开头的 github issue 查看。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>其实去年我也尝试过在服务器上安装 LightGBM GPU，但折腾了一段时间并没有搞成功，所以也就放弃了。</p><p>那么为什么现在又弄了呢？这个动机可以追溯到上个月在 Kaggle 上打 PlayGround 竞赛的时候。因为 LightGBM 没有 GPU 加速实在是跑的太慢，而我加入竞赛的时间又比较晚（只剩六七天就结束了），所以我选择了使用自己的电脑来做 XGBoost 和 LightGBM GPU 的 HPO。所幸最后的结果是不错的 —— 最好的结果可以排第八名，可惜选错了 submission<del>（盲目相信 public score 的代价）</del>。</p><p>离题了，如上所述，虽然拿着自己电脑的 2060 跑确实比 CPU 要快很多，但是每天拿着一个游戏本跑来跑去，还要早去课室为它霸占充电位以及忍受它狂躁的风扇声和令人发指的续航能力，实在是让人有些心力交瘁。看着服务器上三台 3090，我还是下定决心在服务器上也安装一个。</p><p>所幸最后安装成功，在这里记录下自己走的弯路，希望能帮助别人省些时间……</p></body></html>]]></content>
    
    
    <summary type="html">本文将介绍如何在 Linux 系统上无 root 权限的情景下安装 LightGBM GPU，从而实现使用服务器显卡进行 GPU 加速。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>全基因组 CpG (or nonCpG) 位点 bed 文件的获取方式</title>
    <link href="https://biojuse.com/2024/03/18/%E5%85%A8%E5%9F%BA%E5%9B%A0%E7%BB%84%20CpG%20(or%20nonCpG)%20%E4%BD%8D%E7%82%B9%20bed%20%E6%96%87%E4%BB%B6%E7%9A%84%E8%8E%B7%E5%8F%96%E6%96%B9%E5%BC%8F/"/>
    <id>https://biojuse.com/2024/03/18/%E5%85%A8%E5%9F%BA%E5%9B%A0%E7%BB%84%20CpG%20(or%20nonCpG)%20%E4%BD%8D%E7%82%B9%20bed%20%E6%96%87%E4%BB%B6%E7%9A%84%E8%8E%B7%E5%8F%96%E6%96%B9%E5%BC%8F/</id>
    <published>2024-03-18T07:00:00.000Z</published>
    <updated>2024-03-20T08:16:30.929Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><a href="https://en.wikipedia.org/wiki/CpG_site">CpG 位点</a>是基因组上的一种特殊序列，它在 DNA 的结构和功能中起着重要作用。其组成和甲基化影响着基因组上各区域的调控，是表观遗传学领域中重要的研究目标。</p><p>本文中的脚本主要用于提取基因组中 CpG 位点的位置，以方便一些着重于 CpG 位点的下游分析进行。此外在部分场景下，我们可能希望排除 CpG 位点进行分析（因为其独特的性质例如高突变率等），因此该脚本也可产生 AT&amp;CG(nonCpG) 位点信息文件。</p><p>需要准备的文件和条件有：</p><ul><li>基因组序列文件</li><li>Biopython 库（<code>pip install biopython</code>）</li></ul><p>可能需要准备的前置条件：</p><ul><li>已被放置于 <code>$PATH</code> 环境变量中的 <code>bedtools</code></li></ul><p>脚本内容：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> Bio <span class="keyword">import</span> SeqIO</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_args</span>():</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">"Use to extract AT/CG(nonCpG) &amp; CpG sites."</span>)</span><br><span class="line">    </span><br><span class="line">    input_group = parser.add_argument_group(<span class="string">"Input"</span>)</span><br><span class="line">    input_group.add_argument(<span class="string">"-g"</span>, <span class="string">"--genome"</span>, required=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">"The path of genome sequence file."</span>)</span><br><span class="line">    input_group.add_argument(<span class="string">"-c"</span>, <span class="string">"--chr"</span>, required=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">"The target chromosome to extract."</span>)</span><br><span class="line"></span><br><span class="line">    option_group = parser.add_argument_group(<span class="string">"Optional parameters"</span>)</span><br><span class="line">    option_group.add_argument(<span class="string">"--onlycpg"</span>, action=<span class="string">"store_true"</span>, <span class="built_in">help</span>=<span class="string">"Only extract CpG sites."</span>)</span><br><span class="line">    option_group.add_argument(<span class="string">"--merge"</span>, action=<span class="string">"store_true"</span>, <span class="built_in">help</span>=<span class="string">"Use bedtools to merge bed files."</span>)</span><br><span class="line">    option_group.add_argument(<span class="string">"--gzip"</span>, action=<span class="string">"store_true"</span>, <span class="built_in">help</span>=<span class="string">"Use gzip to compress output."</span>)</span><br><span class="line">    option_group.add_argument(<span class="string">"--nosoftmask"</span>, action=<span class="string">"store_true"</span>, <span class="built_in">help</span>=<span class="string">"Remove soft-masked where repeats are in lower-case text."</span>)</span><br><span class="line"></span><br><span class="line">    output_group = parser.add_argument_group(<span class="string">"Output"</span>)</span><br><span class="line">    output_group.add_argument(<span class="string">"-n"</span>, <span class="string">"--name"</span>, <span class="built_in">help</span>=<span class="string">"The name of output file."</span>)</span><br><span class="line">    output_group.add_argument(<span class="string">"-p"</span>, <span class="string">"--path"</span>, <span class="built_in">help</span>=<span class="string">"The path of output file."</span>, default=<span class="string">'./'</span>)</span><br><span class="line">    </span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> args.name:</span><br><span class="line">        args.name = os.path.splitext(os.path.basename(args.genome))[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> args</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_command</span>(<span class="params">command</span>):</span><br><span class="line">    result = subprocess.run(command, shell=<span class="literal">True</span>, stdout=subprocess.PIPE, stderr=subprocess.PIPE)</span><br><span class="line">    <span class="keyword">if</span> result.returncode != <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"Error when executing command: <span class="subst">{command}</span>\n<span class="subst">{result.stderr.decode()}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    args = get_args()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> gzip.<span class="built_in">open</span>(args.genome, <span class="string">"rt"</span>) <span class="keyword">if</span> args.genome.endswith(<span class="string">".gz"</span>) <span class="keyword">else</span> <span class="built_in">open</span>(args.genome, <span class="string">"r"</span>) <span class="keyword">as</span> genome_file:</span><br><span class="line">        chr_num = <span class="string">"chr"</span> + args.<span class="built_in">chr</span>.replace(<span class="string">"chr"</span>, <span class="string">""</span>)</span><br><span class="line">        cpg_bed_name = os.path.join(args.path, <span class="string">f"<span class="subst">{args.name}</span>_<span class="subst">{chr_num}</span>_CpGsites.bed.tmp"</span>)</span><br><span class="line">        atcg_bed_name = os.path.join(args.path, <span class="string">f"<span class="subst">{args.name}</span>_<span class="subst">{chr_num}</span>_ATnonCpGsites.bed.tmp"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(cpg_bed_name, <span class="string">"w"</span>) <span class="keyword">as</span> cpg_bed, <span class="built_in">open</span>(atcg_bed_name, <span class="string">"w"</span>) <span class="keyword">as</span> atcg_bed:</span><br><span class="line">            <span class="keyword">for</span> record <span class="keyword">in</span> SeqIO.parse(genome_file, <span class="string">"fasta"</span>):</span><br><span class="line">                <span class="keyword">if</span> record.<span class="built_in">id</span> != args.<span class="built_in">chr</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> args.nosoftmask:</span><br><span class="line">                    seq = <span class="built_in">str</span>(record.seq).upper()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    seq = <span class="built_in">str</span>(record.seq)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> i, nuc <span class="keyword">in</span> <span class="built_in">enumerate</span>(seq):</span><br><span class="line">                    <span class="keyword">if</span> nuc == <span class="string">"N"</span>:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="keyword">if</span> nuc <span class="keyword">in</span> [<span class="string">"A"</span>, <span class="string">"T"</span>] <span class="keyword">and</span> <span class="keyword">not</span> args.onlycpg:</span><br><span class="line">                        atcg_bed.write(<span class="string">f"<span class="subst">{chr_num}</span>\t<span class="subst">{i}</span>\t<span class="subst">{i+<span class="number">1</span>}</span>\n"</span>)</span><br><span class="line">                    <span class="keyword">elif</span> nuc <span class="keyword">in</span> [<span class="string">"C"</span>, <span class="string">"G"</span>]:</span><br><span class="line">                        <span class="keyword">if</span> (nuc == <span class="string">"C"</span> <span class="keyword">and</span> i + <span class="number">1</span> &lt; <span class="built_in">len</span>(seq) <span class="keyword">and</span> seq[i + <span class="number">1</span>] == <span class="string">"G"</span>) <span class="keyword">or</span> (nuc == <span class="string">"G"</span> <span class="keyword">and</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> seq[i - <span class="number">1</span>] == <span class="string">"C"</span>):</span><br><span class="line">                            cpg_bed.write(<span class="string">f"<span class="subst">{chr_num}</span>\t<span class="subst">{i}</span>\t<span class="subst">{i+<span class="number">1</span>}</span>\n"</span>)</span><br><span class="line">                        <span class="keyword">elif</span> <span class="keyword">not</span> args.onlycpg:</span><br><span class="line">                            atcg_bed.write(<span class="string">f"<span class="subst">{chr_num}</span>\t<span class="subst">{i}</span>\t<span class="subst">{i+<span class="number">1</span>}</span>\n"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> bed_file <span class="keyword">in</span> [cpg_bed_name, atcg_bed_name] <span class="keyword">if</span> <span class="keyword">not</span> args.onlycpg <span class="keyword">else</span> [cpg_bed_name]:</span><br><span class="line">            outfile = bed_file.replace(<span class="string">".tmp"</span>, <span class="string">""</span>)</span><br><span class="line">            <span class="keyword">if</span> args.merge:</span><br><span class="line">                command = <span class="string">f"bedtools merge -i <span class="subst">{bed_file}</span> &gt; <span class="subst">{outfile}</span> &amp;&amp; rm <span class="subst">{bed_file}</span>"</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                command = <span class="string">f"mv <span class="subst">{bed_file}</span> <span class="subst">{outfile}</span>"</span></span><br><span class="line">            process_command(command)</span><br><span class="line">            <span class="keyword">if</span> args.gzip:</span><br><span class="line">                process_command(<span class="string">f"gzip -f <span class="subst">{outfile}</span>"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> args.onlycpg:</span><br><span class="line">            os.remove(atcg_bed_name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></tbody></table></figure><p>将上述内容保存在一个名为 <code>get_sites.py</code> 的文件内，或者点击<a href="https://github.com/JuseTiZ/PyScript-for-CT/blob/main/get_sites.py">此处</a>下载。</p><p>运行示例（以 hg38 人类基因组为例，提取其在 21 号染色体上的 CpG 位点位置信息）：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">python get_sites.py -g hg38.fa -c chr21 --merge --gzip -n GRCh38</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">python get_sites.py -g [sequence file] -c [chr] --merge --gzip -n [outname]</span></span><br></pre></td></tr></tbody></table></figure><p>最后将会得到名为 <code>GRCh38_chr21_ATnonCpGsites.bed.gz</code> 和 <code>GRCh38_chr21_CpGsites.bed.gz</code> 的文件，以下是各参数说明：</p><ul><li><code>-g</code> 指定参考基因组序列文件，<code>.gz</code> 压缩格式可以被自动识别。</li><li><code>-c</code> 指定要提取位点信息的染色体，需和序列文件中的序列 id 一致。</li><li><code>--merge</code> 在提取完位点信息后，调用 <code>bedtools</code> 对输出文件进行合并，将每行 1bp 的 bed 文件合并为区间。该选项要求 <code>bedtools</code> 可以直接调用，否则会出现错误。bedtools 的安装可以直接通过 <code>conda install -c bioconda bedtools</code> 实现。</li><li><code>--gzip</code> 在提取完位点信息后（如果指定了 <code>--merge</code> 则在合并后），对输出文件进行压缩。</li><li><code>-n</code> 指定输出文件的前缀。如果命令中未给定，则默认为序列文件的前缀。</li><li><code>-p</code> 指定输出文件的路径。如果命令中未给定，则默认为运行脚本时所处的路径。</li></ul><p>此外，如果在运行脚本时指定了 <code>--onlycpg</code>，则仅产生 CpG 位点位置文件。如果在运行脚本时指定了 <code>--nosoftmask</code>，则跳过<a href="https://grch37.ensembl.org/info/genome/genebuild/assembly_repeats.html">软掩蔽区域</a>。</p><p>因为该脚本针对特定染色体进行，因此可以批量运行脚本以同时处理多条染色体，示例：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="keyword">for</span> i <span class="keyword">in</span> {1..23};</span></span><br><span class="line">do</span><br><span class="line">python get_sites.py -g hg38.fa -c chr"$i" --merge --gzip -n GRCh38 &amp;</span><br><span class="line">done</span><br></pre></td></tr></tbody></table></figure><p>请保证计算机或服务器有足够的 CPU 和内存进行批量运行，此处更建议使用 <code>parallel</code> 来进行并行数量控制。如果在服务器上有作业调度系统，也可以合理利用，例如 slurm 的作业数组等，以下是一个 slurm 作业数组运行的示例：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --job-name=getsites</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --nodelist=yournode</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --ntasks=1</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --array=1-23%10</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --mail-user=youremail</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --mail-type=END</span></span><br><span class="line"></span><br><span class="line">python get_sites.py -g hg38.fa -c chr"$SLURM_ARRAY_TASK_ID" --merge --gzip -n GRCh38</span><br></pre></td></tr></tbody></table></figure><p>产生结果示例：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">zcat GRCh38_chr21_CpGsites.bed.gz | <span class="built_in">head</span> -n 5</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">chr21   5010008 5010010</span><br><span class="line">chr21   5010053 5010055</span><br><span class="line">chr21   5010215 5010217</span><br><span class="line">chr21   5010331 5010333</span><br><span class="line">chr21   5010335 5010337</span><br></pre></td></tr></tbody></table></figure><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>虽说这半年已经写了非常多的 script，但是翻来翻去感觉都属于针对性过强的类型，除了我大概率也没有人能用得上。因此最后只能选定本文中这个短小精悍<del>（没啥难度）</del>的脚本作为分享<del>（水文）</del>，或许能帮到有需要的人。</p><p>此外也还有一个和本文类似的脚本，主要用来提取全基因组上所有四倍简并位点的位置信息。之后如果有需要的话会补充在这篇文章里面。</p></body></html>]]></content>
    
    
    <summary type="html">在制作 bw 文件时我们可能需要用到特定的位点，本文将介绍获得基因组上 CpG 位点信息的方法。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="杂项" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E6%9D%82%E9%A1%B9/"/>
    
    
  </entry>
  
  <entry>
    <title>Clash for windows 中让某些域名绕过代理</title>
    <link href="https://biojuse.com/2024/02/20/Clashforwindow/"/>
    <id>https://biojuse.com/2024/02/20/Clashforwindow/</id>
    <published>2024-02-20T13:00:00.000Z</published>
    <updated>2024-07-26T01:47:01.718Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>众所周知，Clash for windows 的作者已经删库跑路许久，而在使用这个软件到现在我也终于忍不住想要解决某些域名绕过代理的问题。我已经受够了每次进 web of science 查文献亦或是在 Science 官网上看文章都要关掉代理才能进去的痛楚了。</p><p>在搜索网上的解决方案时，部分文章说可以在 Settings&gt;System Proxy&gt;Bypass Domain/IPNet 中设置相应域名解决，但是我发现我新加域名进入后我的梯子就会直接整个没用。当然这发生在大约半年前的时候，那时的我选择了将这件事暂时搁置，直到现在。</p><p>这次再次寻找解决方法，终于找到个适用于我的，趁这个机会写在此处，希望能帮到其他有需求的朋友，当然也有可能某天这篇文章会因为不可抗力消失。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>首先点开 <code>Profiles</code>，选择自己的梯子右键，点开 <code>Parsers</code> 并选择 <code>Edit Parsers</code>。</p><p><img src="/pic2/cfw1.png"></p><p>将里面的内容处理成以下格式：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parsers: # array</span><br><span class="line">    - url: 梯子 url</span><br><span class="line">      yaml:</span><br><span class="line">        prepend-rules:</span><br><span class="line">        - DOMAIN-SUFFIX,www.science.org,DIRECT</span><br></pre></td></tr></tbody></table></figure><p><img src="/pic2/cfw2.png"></p><p>将 url 后的内容换成自己的梯子 url，注意<strong>不要破坏缩进，yaml 和 url 需要对齐才能正常发挥作用</strong>，后面可以通过以下格式在 prepend-rules 下添加域名：</p><ul><li>DOMAIN-SUFFIX,域名,DIRECT</li></ul><p>对于 Web of science，我填入以下几个规则后成功让 Web of science 绕过代理，但不清楚其中最关键的是哪些（亦或是都需要）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- DOMAIN-SUFFIX,snowplow.apps.clarivate.com,DIRECT</span><br><span class="line">- DOMAIN-SUFFIX,access.clarivate.com,DIRECT</span><br><span class="line">- DOMAIN-SUFFIX,www.webofscience.com,DIRECT</span><br><span class="line">- DOMAIN-SUFFIX,snowplow-collector.userintel.prod.sp.aws.clarivate.net,DIRECT</span><br><span class="line">- DOMAIN-SUFFIX,www.webofknowledge.com,DIRECT</span><br></pre></td></tr></tbody></table></figure><p>有需求的朋友可以将上面这段复制到 <code>prepend-rules</code> 之后。</p></body></html>]]></content>
    
    
    <summary type="html">绕过域名代理，让不需要梯子的网站实现不翻梯子自由。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="杂项" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E6%9D%82%E9%A1%B9/"/>
    
    
  </entry>
  
  <entry>
    <title>通俗易懂的贝叶斯深度神经网络介绍及代码实现</title>
    <link href="https://biojuse.com/2024/01/20/%E9%80%9A%E4%BF%97%E6%98%93%E6%87%82%E7%9A%84%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    <id>https://biojuse.com/2024/01/20/%E9%80%9A%E4%BF%97%E6%98%93%E6%87%82%E7%9A%84%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</id>
    <published>2024-01-20T13:40:00.000Z</published>
    <updated>2024-06-12T08:54:19.365Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>最近在课题组里的一些任务涉及到了把模型的架构改成贝叶斯层，让模型输出的结果能够体现不确定性。恶补相关知识的同时正好也能记录下笔记发博客，一举两得。</p><p>这篇文章只做概念介绍，不做深入剖析，追求原理的话敬请走下文中相关链接前往原论文。</p><h2 id="贝叶斯神经网络是什么"><a href="#贝叶斯神经网络是什么" class="headerlink" title="贝叶斯神经网络是什么"></a>贝叶斯神经网络是什么</h2><p>或许你在很多其他的地方听过<strong>贝叶斯</strong>这个词汇。以下是一些生物学中的例子：</p><ul><li>系统发育学中的贝叶斯建树（Mrbayes、Phylobayes）。</li><li>群体遗传学（种群遗传学）中分析种群结构、估计种群历史等（Structure、ABC）。</li><li>在某些基因组比对的工具里也有用到贝叶斯方法（bwa）。</li></ul><p>贝叶斯方法的大致原理是结合先验知识和实验数据来估计参数或测试假设，这个过程涉及到随机采样，因此其他方法不同，应用贝叶斯法得到的结果往往不会是完全一致的，也就是说，可能每次跑出来的结果之间会相似，但不会完全相同。</p><p>这里就提及到了贝叶斯方法的一个宝贵性质 —— <strong>不确定性</strong>。</p><p>如果有模型搭建经验的话，你应该知道，对于传统的神经网络架构而言，在你训练好模型后，如果给它相同的输入，那么一般而言它都会产生一致的输出（因为模型内部的<strong>权重是确定的</strong>）。但是有些时候，我们可能想知道：</p><ul><li>我们模型输出的结果到底多可靠？它的不确定性有多大？波动范围大概是多少？</li></ul><p>这时候，贝叶斯神经网络就发挥了其优势。</p><h3 id="贝叶斯神经网络如何引入不确定性"><a href="#贝叶斯神经网络如何引入不确定性" class="headerlink" title="贝叶斯神经网络如何引入不确定性"></a>贝叶斯神经网络如何引入不确定性</h3><p>我深知过于详尽的数学公式和逻辑论证会让人头昏眼花（数学佬除外），所以我并不打算以过于专业的角度进行解释。如果你对本文涉及到的贝叶斯神经网络个中原理感兴趣，可以去观摩相关的文章：</p><p><a href="https://arxiv.org/abs/1505.05424">Weight Uncertainty in Neural Networks</a></p><p>简单来说，贝叶斯神经网络中，每个神经元（或者说节点）之间连接的<strong>权重</strong>并<strong>不是一个固定的值</strong>，而是一个<strong>分布</strong>。假设在一个传统的神经网络架构中，一个权重的值为 0.2，那么这个权重在贝叶斯神经网络中的对应值可能就变为了 0.1-0.3。</p><p>只要能够理解上述这点，那么其输出为什么具有不确定性也就不难理解了。在贝叶斯神经网络的前向传播中，权重将会从一个分布中进行采样，因此不同的运行里我们得到的结果会不同，以上面的权重为例，或许这一次模型采样的权重为 0.15，而下一次就变成了 0.25。</p><p>这里同样引申出另一个重要的点：贝叶斯神经网络如何得到<strong>结果的不确定性</strong>？不难看出，它引入不确定性是通过权重的随机采样实现的，但是它每次运行输出的结果依然是个确定的值而不是一个分布（因为权重在采样后都是确定的某个值）。不过正如上面所说，因为每次运行得到的结果都会不同，因此我们可以通过<strong>多次预测</strong>得到一组预测结果，<strong>这组预测结果</strong>的平均值和标准差等信息即我们所需要的。</p><h3 id="贝叶斯神经网络的优势和不足"><a href="#贝叶斯神经网络的优势和不足" class="headerlink" title="贝叶斯神经网络的优势和不足"></a>贝叶斯神经网络的优势和不足</h3><p>贝叶斯神经网络（BNNs）与传统神经网络相比，优势如下：</p><p><strong>不确定性估计</strong>：贝叶斯神经网络能够提供预测的不确定性估计。这是因为 BNNs 使用概率分布而非单一值来表示权重，从而能够在给出预测时同时给出关于这些预测的不确定性信息。</p><p>这一优势同时导致了其他方面的优势：</p><ul><li><strong>减少过拟合</strong>。这其实不难理解，正如被广泛使用的 dropout 一样，两者有一个共通之处 —— 随机性，dropout 体现在丢弃神经元的随机性上，而贝叶斯神经网络体现在权重取值的随机性上。</li><li><strong>自适应（鲁棒）性</strong>。通过对权重的不确定性进行建模，可以更好地适应数据分布的变化，从而在面对数据或环境变化时更加 robust。</li></ul><p>举一些实际例子：</p><ul><li><p>与传统神经网络相比，贝叶斯神经网络在处理对抗性攻击时表现出更高的鲁棒性 (Uchendu, Campoy, Menart &amp; Hildenbrandt, 2021)。</p></li><li><p>在经济领域，贝叶斯神经网络在预测任务中的表现优于传统神经网络。这在预测乳金融市场动态等方面得到了应用和验证 (Magris, Shabani &amp; Iosifidis, 2022)。</p></li></ul><p>当然，不确定性是一把双刃剑，并且它不一定会让模型表现得更加出色（免费午餐警告⚠），一些明显的缺点如下：</p><ul><li><strong>更高的计算成本和更长的训练时间</strong>。这一点在后文的代码实现中能够得到更清晰的解释。</li><li><strong>内存需求</strong>。需要存储权重的分布而不是单个权重值。</li><li><strong>可解释性问题</strong>。尽管贝叶斯神经网络提供预测的不确定性估计，但它们<strong>本身的结构和决策过程仍然是黑盒</strong>的。</li></ul><p>关于贝叶斯神经网络的介绍，我强烈推荐以下博客文章：</p><p><a href="https://towardsdatascience.com/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6">A Gentle Introduction to Bayesian Deep Learning</a></p><p>这里面稍微涉及到了一些基础的数学知识，理解上并不困难，也是从更易于理解的角度出发进行介绍。</p><h2 id="贝叶斯神经网络的代码实现"><a href="#贝叶斯神经网络的代码实现" class="headerlink" title="贝叶斯神经网络的代码实现"></a>贝叶斯神经网络的代码实现</h2><p>在很久之前贝叶斯神经网络需要自己通过 coding 实现，但现在已经有造好的轮子，所以我们只需要搬过来用即可。</p><blockquote><p>2024/05/20</p><p>经与博后师兄讨论，确定了一个更好用的 module：</p><p><a href="https://github.com/IntelLabs/bayesian-torch">https://github.com/IntelLabs/bayesian-torch</a></p><p>该 module 中关于 KL divergence 的 loss 参数设定不需要自行调整，且支持直接将传统架构转为贝叶斯架构，并提供了不确定性量化的方法。</p><p>安装方法（使用 pip）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install bayesian-torch</span><br></pre></td></tr></tbody></table></figure><p>使用方法：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> bayesian_torch.models.dnn_to_bnn <span class="keyword">import</span> dnn_to_bnn, get_kl_loss</span><br><span class="line"></span><br><span class="line">const_bnn_prior_parameters = {</span><br><span class="line">        <span class="string">"prior_mu"</span>: <span class="number">0.0</span>,</span><br><span class="line">        <span class="string">"prior_sigma"</span>: <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">"posterior_mu_init"</span>: <span class="number">0.0</span>,</span><br><span class="line">        <span class="string">"posterior_rho_init"</span>: -<span class="number">3.0</span>,</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"Reparameterization"</span>,  <span class="comment"># Flipout or Reparameterization</span></span><br><span class="line">        <span class="string">"moped_enable"</span>: <span class="literal">False</span>,  <span class="comment"># True to initialize mu/sigma from the pretrained dnn weights</span></span><br><span class="line">        <span class="string">"moped_delta"</span>: <span class="number">0.5</span>,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">model = torchvision.models.resnet18()</span><br><span class="line">dnn_to_bnn(model, const_bnn_prior_parameters)</span><br></pre></td></tr></tbody></table></figure><p>以上方法将创造一个全新的贝叶斯神经网络模型。此外，如果你已经拥有了一个预训练好的模型，则可以通过将 <code>moped_enable</code> 改为 <code>True</code> 从而使贝叶斯神经网络基于该模型已有权重设置先验和初始化变分参数（该操作有利于大模型的收敛）：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">const_bnn_prior_parameters = {</span><br><span class="line">        <span class="string">"prior_mu"</span>: <span class="number">0.0</span>,</span><br><span class="line">        <span class="string">"prior_sigma"</span>: <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">"posterior_mu_init"</span>: <span class="number">0.0</span>,</span><br><span class="line">        <span class="string">"posterior_rho_init"</span>: -<span class="number">3.0</span>,</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"Reparameterization"</span>,  <span class="comment"># Flipout or Reparameterization</span></span><br><span class="line">        <span class="string">"moped_enable"</span>: <span class="literal">True</span>,  <span class="comment"># True to initialize mu/sigma from the pretrained dnn weights</span></span><br><span class="line">        <span class="string">"moped_delta"</span>: <span class="number">0.5</span>,</span><br><span class="line">}</span><br><span class="line">    </span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">dnn_to_bnn(model, const_bnn_prior_parameters)</span><br></pre></td></tr></tbody></table></figure><p>训练、预测和不确定性量化的相关代码详情请见该 module 的 <a href="https://github.com/IntelLabs/bayesian-torch">github 页面</a>。与下文介绍的 blitz 相比，这里的 bayesian-torch 只需要更简单的命令就可以实现传统神经网络到贝叶斯神经网络的迁移，并且具有更全面的功能和参数设定。</p></blockquote><h3 id="blitz-安装"><a href="#blitz-安装" class="headerlink" title="blitz 安装"></a>blitz 安装</h3><p>这里会用到 Python 的 <code>blitz</code> module，其具体源码和 document 可见：</p><p><a href="https://github.com/piEsposito/blitz-bayesian-deep-learning">https://github.com/piEsposito/blitz-bayesian-deep-learning</a></p><p>pip 安装命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install blitz-bayesian-pytorch</span><br></pre></td></tr></tbody></table></figure><p>或者你可以使用 conda 完成:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge blitz-bayesian-pytorch</span><br></pre></td></tr></tbody></table></figure><p>亦或者你可以通过 clone 其 GitHub 仓库完成：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda create -n blitz python=3.9</span><br><span class="line">conda activate blitz</span><br><span class="line">git clone https://github.com/piEsposito/blitz-bayesian-deep-learning.git</span><br><span class="line">cd blitz-bayesian-deep-learning</span><br><span class="line">pip install .</span><br></pre></td></tr></tbody></table></figure><h3 id="运行范例"><a href="#运行范例" class="headerlink" title="运行范例"></a>运行范例</h3><p>这里使用经典的 LeNet 和 MNIST 数据集进行贝叶斯神经网络和传统神经网络的比较，以下代码将根据 blitz github repository 中的示例代码进行补充和修改，原代码可见：</p><p><a href="https://github.com/piEsposito/blitz-bayesian-deep-learning/blob/master/blitz/examples/bayesian_LeNet_mnist.py">https://github.com/piEsposito/blitz-bayesian-deep-learning/blob/master/blitz/examples/bayesian_LeNet_mnist.py</a></p><p>首先 import 需要的库：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dsets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> blitz.modules <span class="keyword">import</span> BayesianLinear, BayesianConv2d</span><br><span class="line"><span class="keyword">from</span> blitz.losses <span class="keyword">import</span> kl_divergence_from_nn</span><br><span class="line"><span class="keyword">from</span> blitz.utils <span class="keyword">import</span> variational_estimator</span><br></pre></td></tr></tbody></table></figure><p>下载并加载数据：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = dsets.MNIST(root=<span class="string">"./data"</span>,</span><br><span class="line">                             train=<span class="literal">True</span>,</span><br><span class="line">                             transform=transforms.ToTensor(),</span><br><span class="line">                             download=<span class="literal">True</span></span><br><span class="line">                            )</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset,</span><br><span class="line">                                           batch_size=<span class="number">64</span>,</span><br><span class="line">                                           shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_dataset = dsets.MNIST(root=<span class="string">"./data"</span>,</span><br><span class="line">                             train=<span class="literal">False</span>,</span><br><span class="line">                             transform=transforms.ToTensor(),</span><br><span class="line">                             download=<span class="literal">True</span></span><br><span class="line">                            )</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                           batch_size=<span class="number">64</span>,</span><br><span class="line">                                           shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><p>构建贝叶斯神经网络和传统神经网络：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@variational_estimator</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BayesianCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = BayesianConv2d(<span class="number">1</span>, <span class="number">6</span>, (<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">        self.conv2 = BayesianConv2d(<span class="number">6</span>, <span class="number">16</span>, (<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">        self.fc1   = BayesianLinear(<span class="number">256</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2   = BayesianLinear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3   = BayesianLinear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = F.relu(self.conv1(x))</span><br><span class="line">        out = F.max_pool2d(out, <span class="number">2</span>)</span><br><span class="line">        out = F.relu(self.conv2(out))</span><br><span class="line">        out = F.max_pool2d(out, <span class="number">2</span>)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = F.relu(self.fc1(out))</span><br><span class="line">        out = F.relu(self.fc2(out))</span><br><span class="line">        out = self.fc3(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">lenet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, (<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, (<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">        self.fc1   = nn.Linear(<span class="number">256</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2   = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3   = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = F.relu(self.conv1(x))</span><br><span class="line">        out = F.max_pool2d(out, <span class="number">2</span>)</span><br><span class="line">        out = F.relu(self.conv2(out))</span><br><span class="line">        out = F.max_pool2d(out, <span class="number">2</span>)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = F.relu(self.fc1(out))</span><br><span class="line">        out = F.relu(self.fc2(out))</span><br><span class="line">        out = self.fc3(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure><ul><li><code>BayesianCNN</code> 类前的 <code>@variational_estimator</code> 是一种装饰器，它用于拓展包含贝叶斯神经网络的类的功能（例如后面会提到的 <code>.sample_elbo</code>），同时与原先的 <code>nn.Module</code> 适配。</li><li><code>nn.Conv2d</code> 层对应 <code>BayesianConv2d</code>，<code>nn.Linear</code> 对应 <code>BayesianLinear</code>。</li></ul><p>贝叶斯神经网络训练及评估：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Bayesian train &amp; test</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">classifier = BayesianCNN().to(device)</span><br><span class="line">optimizer = optim.Adam(classifier.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">best_accuracy = <span class="number">0</span></span><br><span class="line">waiting = <span class="number">0</span></span><br><span class="line">iteration = <span class="number">0</span></span><br><span class="line">stop = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> stop:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> i, (datapoints, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss = classifier.sample_elbo(</span><br><span class="line">            inputs=datapoints.to(device),</span><br><span class="line">            labels=labels.to(device),</span><br><span class="line">            criterion=criterion,</span><br><span class="line">            sample_nbr=<span class="number">3</span>,</span><br><span class="line">            complexity_cost_weight=<span class="number">1</span> / <span class="number">50000</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># print(loss)</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        iteration += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> iteration % <span class="number">250</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(loss)</span><br><span class="line">            correct = <span class="number">0</span></span><br><span class="line">            total = <span class="number">0</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">                    images, labels = data</span><br><span class="line">                    outputs = classifier(images.to(device))</span><br><span class="line">                    _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">                    total += labels.size(<span class="number">0</span>)</span><br><span class="line">                    correct += (predicted == labels.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">                accuracy = <span class="number">100</span> * correct / total</span><br><span class="line">                <span class="built_in">print</span>(</span><br><span class="line">                    <span class="string">"Iteration: {} | Accuracy of the network on the 10000 test images: {} %"</span>.<span class="built_in">format</span>(</span><br><span class="line">                        <span class="built_in">str</span>(iteration), <span class="built_in">str</span>(<span class="number">100</span> * correct / total)</span><br><span class="line">                    )</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> accuracy &gt; best_accuracy + <span class="number">0.005</span>:</span><br><span class="line">                    best_accuracy = accuracy</span><br><span class="line">                    waiting = <span class="number">0</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    waiting += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> waiting &gt;= <span class="number">4</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f"The best accuracy is <span class="subst">{best_accuracy}</span> for bayesian nn."</span>)</span><br><span class="line">                    stop = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure><p>这里把一百个 epoch 全跑完太消时，所以我使用了简陋的早停策略提前终止训练，需要关注的参数主要为：</p><ul><li><code>.sample_elbo</code>：这是 blitz 中特有的计算 loss 的功能，这里的 <code>sample_nbr</code> 可以当作计算多少次来进行一次梯度更新（因为每次计算都会有不一样的结果），而 <code>complexity_cost_weight</code> 相当于对模型的复杂性进行惩罚，<strong>该值越大，惩罚的力度越高（根据权重的先验分布和后验分布）</strong> 。它的源码如下：</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sample_elbo</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                inputs,</span></span><br><span class="line"><span class="params">                labels,</span></span><br><span class="line"><span class="params">                criterion,</span></span><br><span class="line"><span class="params">                sample_nbr,</span></span><br><span class="line"><span class="params">                complexity_cost_weight=<span class="number">1</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="string">""" Samples the ELBO Loss for a batch of data, consisting of inputs and corresponding-by-index labels</span></span><br><span class="line"><span class="string">            The ELBO Loss consists of the sum of the KL Divergence of the model</span></span><br><span class="line"><span class="string">             (explained above, interpreted as a "complexity part" of the loss)</span></span><br><span class="line"><span class="string">             with the actual criterion - (loss function) of optimization of our model</span></span><br><span class="line"><span class="string">             (the performance part of the loss).</span></span><br><span class="line"><span class="string">            As we are using variational inference, it takes several (quantified by the parameter sample_nbr) Monte-Carlo</span></span><br><span class="line"><span class="string">             samples of the weights in order to gather a better approximation for the loss.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            inputs: torch.tensor -&gt; the input data to the model</span></span><br><span class="line"><span class="string">            labels: torch.tensor -&gt; label data for the performance-part of the loss calculation</span></span><br><span class="line"><span class="string">                    The shape of the labels must match the label-parameter shape of the criterion (one hot encoded or as index, if needed)</span></span><br><span class="line"><span class="string">            criterion: torch.nn.Module, custom criterion (loss) function, torch.nn.functional function -&gt; criterion to gather</span></span><br><span class="line"><span class="string">                        the performance cost for the model</span></span><br><span class="line"><span class="string">            sample_nbr: int -&gt; The number of times of the weight-sampling and predictions done in our Monte-Carlo approach to</span></span><br><span class="line"><span class="string">                        gather the loss to be .backwarded in the optimization of the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(sample_nbr):</span><br><span class="line">        outputs = self(inputs)</span><br><span class="line">        loss += criterion(outputs, labels)</span><br><span class="line">        loss += self.nn_kl_divergence() * complexity_cost_weight</span><br><span class="line">    <span class="keyword">return</span> loss / sample_nbr</span><br></pre></td></tr></tbody></table></figure><details class="toggle" style="border: 1px solid  "><summary class="toggle-button" style="background-color:  ;color:  \#49b1f5">相关概念介绍（非必要内容）</summary><div class="toggle-content"><p>KL 散度是一种衡量两个概率分布差异的方法，关于该概念的理解，可参见：</p><p><a href="https://zhuanlan.zhihu.com/p/100676922?utm_id=0">Kullback-Leibler(KL)散度介绍 from 知乎</a></p><p>在贝叶斯神经网络中，使用 KL 散度作为损失函数的一部分是为了实现变分推断。在这个背景下，网络的权重被视为随机变量，遵循某种概率分布。变分推断的目标是寻找一个简单的分布（变分后验分布），以近似复杂的真实后验分布。</p><p>贝叶斯神经网络的损失函数通常包含两部分：</p><ol><li><strong>性能损失</strong>（Performance Loss）：这部分通常是传统神经网络中使用的损失函数（如上述代码中的交叉熵），用于衡量模型预测与实际情况之间的差距。</li><li><strong>复杂度损失</strong>（Complexity Loss）：这部分是 KL 散度，用于衡量变分后验分布与先验分布之间的差异。KL 散度越大，表示变分后验分布与先验分布差异越大，模型复杂度越高。</li></ol><p>那么为什么变分后验分布与先验分布差异越大，模型复杂度越高？</p><p>这与贝叶斯推断的本质有关，在贝叶斯方法中，先验分布代表模型权重的初始假设，这时我们通常选择较为简单或宽泛的分布（例如没有信息的均匀分布或分布较广的正态分布等）。在进行训练后，训练数据会对参数估计产生影响，从而导致权重的后验分布产生变化。</p><p>这个情况下，后验分布与先验分布相差显著时，模型可能捕捉到了数据中的复杂特征和模式，从而提高了其拟合能力。然而，这也可能意味着模型捕捉到了数据中的噪声，导致过拟合。因此，上述代码中引入了新的超参数 <code>complexity_cost_weight</code> 对这一点进行了限制，以起到一定的正则化作用，保证模型的泛化能力。同时也不难看出，<strong>错误的先验决定也会影响到模型的训练和泛化</strong>。</p><p>关于后验分布更新的示例，可以看以下文章：</p><p><a href="https://zhuanlan.zhihu.com/p/266563061">贝叶斯教你如何正确抛硬币 from 知乎</a></p><p>以抛硬币为例，<strong>正面向上的概率即</strong> <strong>P(正面向上)</strong> 的先验分布可能是 Beta(1,1)，即一个均匀分布（没有任何信息量），之后我们根据观测数据来更新这个分布得到后验概率分布：</p><ol><li>第一次抛出正面，后验分布变为 Beta(2,1)。</li><li>第二次抛出正面，后验分布变为 Beta(3,1)，此时的概率分布已变为向右倾斜（即正面向上的概率<strong>趋于</strong>更高）。</li><li>假设之后又抛了数次，总共出现正面 552 次，反面 539 次。</li><li>此时后验分布为 Beta(553, 540)，这个分布是轻微向右倾斜，但峰值很接近 0.5 且很窄。</li></ol><p><img src="/pic2/bata_distri.png"></p><p>关于贝塔分布的介绍，可见：<a href="https://zhuanlan.zhihu.com/p/69606875">深入理解Beta分布 from 知乎</a></p><p>你可以使用以下代码进行复现：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> beta</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">553</span>]</span><br><span class="line">b = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">540</span>]</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="built_in">len</span>(a), figsize=(<span class="built_in">len</span>(a)*<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, (suba, subb) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(a, b)):</span><br><span class="line"></span><br><span class="line">    x = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">    y = beta.pdf(x, suba, subb)</span><br><span class="line"></span><br><span class="line">    axes[i].plot(x, y, <span class="string">'r-'</span>, lw=<span class="number">2</span>, alpha=<span class="number">0.6</span>, label=<span class="string">'beta pdf'</span>)</span><br><span class="line">    axes[i].set_xlabel(<span class="string">'x'</span>)</span><br><span class="line">    axes[i].set_xlim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    axes[i].set_title(<span class="string">f'Beta(<span class="subst">{suba}</span>, <span class="subst">{subb}</span>)'</span>)</span><br></pre></td></tr></tbody></table></figure></div></details><p>需要注意的是，<code>complexity_cost_weight</code> 的值将对结果产生<strong>很大的</strong>影响，在 blitz 的其他示例代码中可以看到这个值一般设为 <code>1/batchsize</code>，但是在 MNIST 数据集中，它使用了 <code>1/50000</code> 作为该超参数的值。经过实测，如果使用 <code>1/batchsize</code>，模型的准确率将在 10% 上下浮动，说明惩罚过严重，导致权重的后验分布无法灵活适应数据。</p><p>针对该超参数的选择，blitz 的开发者没有给出 guideline<del>（很多相关 issue 发布多年都没有回复…..）</del>。但从该库源码里 KL 散度的计算中可以看出，模型越复杂（贝叶斯层越多或神经元越多），KL 散度越高，最后的计算复杂度惩罚项越大。因此<strong>对于复杂的模型而言，调低该值或许是一个不错的选择</strong>。</p><p>训练和预测结果：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor(4.0596, device='cuda:0', grad_fn=&lt;DivBackward0&gt;)</span><br><span class="line">Iteration: 250 | Accuracy of the network on the 10000 test images: 94.26 %</span><br><span class="line">......</span><br><span class="line">The best accuracy is 98.75 for bayesian nn.</span><br></pre></td></tr></tbody></table></figure><p>再换成传统的 Lenet 试试：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Lenet train &amp; test</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">classifier = lenet().to(device)</span><br><span class="line">optimizer = optim.Adam(classifier.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">best_accuracy = <span class="number">0</span></span><br><span class="line">waiting = <span class="number">0</span></span><br><span class="line">iteration = <span class="number">0</span></span><br><span class="line">stop = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> stop:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> i, (datapoints, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        datapoints = datapoints.cuda()</span><br><span class="line">        labels = labels.cuda()</span><br><span class="line"></span><br><span class="line">        pred = classifier(datapoints)</span><br><span class="line">        loss = criterion(pred, labels)</span><br><span class="line">        <span class="comment"># print(loss)</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        iteration += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> iteration % <span class="number">250</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(loss)</span><br><span class="line">            correct = <span class="number">0</span></span><br><span class="line">            total = <span class="number">0</span></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">                    images, labels = data</span><br><span class="line">                    outputs = classifier(images.to(device))</span><br><span class="line">                    _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">                    total += labels.size(<span class="number">0</span>)</span><br><span class="line">                    correct += (predicted == labels.to(device)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">                accuracy = <span class="number">100</span> * correct / total</span><br><span class="line">                <span class="built_in">print</span>(</span><br><span class="line">                    <span class="string">"Iteration: {} | Accuracy of the network on the 10000 test images: {} %"</span>.<span class="built_in">format</span>(</span><br><span class="line">                        <span class="built_in">str</span>(iteration), <span class="built_in">str</span>(<span class="number">100</span> * correct / total)</span><br><span class="line">                    )</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> accuracy &gt; best_accuracy + <span class="number">0.005</span>:</span><br><span class="line">                    best_accuracy = accuracy</span><br><span class="line">                    waiting = <span class="number">0</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    waiting += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> waiting &gt;= <span class="number">4</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">f"The best accuracy is <span class="subst">{best_accuracy}</span> for lenet nn."</span>)</span><br><span class="line">                    stop = <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor(0.2020, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">Iteration: 250 | Accuracy of the network on the 10000 test images: 91.49 %</span><br><span class="line">......</span><br><span class="line">The best accuracy is 99.01 for lenet nn.</span><br></pre></td></tr></tbody></table></figure><p>从结果上看，贝叶斯神经网络并没有取得比传统神经网络更好的表现，但是两者仅相差几个千分点，也可能是训练的随机性造成的。不过也能看出，并不是越复杂的网络架构就一定会发挥的更好。</p><p>以上代码并没法看出预测的不确定性，接下来将以 blitz 的另一个示例展示其结果的波动：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> blitz.modules <span class="keyword">import</span> BayesianLinear</span><br><span class="line"><span class="keyword">from</span> blitz.utils <span class="keyword">import</span> variational_estimator</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X, y = fetch_california_housing(return_X_y=<span class="literal">True</span>)</span><br><span class="line">X = StandardScaler().fit_transform(X)</span><br><span class="line">y = StandardScaler().fit_transform(np.expand_dims(y, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X,</span><br><span class="line">                                                    y,</span><br><span class="line">                                                    test_size=<span class="number">.1</span>,</span><br><span class="line">                                                    random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_train, y_train = torch.tensor(X_train).<span class="built_in">float</span>(), torch.tensor(y_train).<span class="built_in">float</span>()</span><br><span class="line">X_test, y_test = torch.tensor(X_test).<span class="built_in">float</span>(), torch.tensor(y_test).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line"><span class="meta">@variational_estimator</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BayesianRegressor</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#self.linear = nn.Linear(input_dim, output_dim)</span></span><br><span class="line">        self.blinear1 = BayesianLinear(input_dim, input_dim*<span class="number">2</span>)</span><br><span class="line">        self.blinear2 = BayesianLinear(input_dim*<span class="number">2</span>, output_dim)</span><br><span class="line">        <span class="comment"># self.blinear1 = nn.Linear(input_dim, input_dim*2)</span></span><br><span class="line">        <span class="comment"># self.blinear2 = nn.Linear(input_dim*2, output_dim)</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x_ = self.blinear1(x)</span><br><span class="line">        x_ = F.relu(x_)</span><br><span class="line">        <span class="keyword">return</span> self.blinear2(x_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">regressor = BayesianRegressor(<span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">optimizer = optim.Adam(regressor.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">ds_train = torch.utils.data.TensorDataset(X_train, y_train)</span><br><span class="line">dataloader_train = torch.utils.data.DataLoader(ds_train, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">ds_test = torch.utils.data.TensorDataset(X_test, y_test)</span><br><span class="line">dataloader_test = torch.utils.data.DataLoader(ds_test, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> i, (datapoints, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader_train):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        loss = regressor.sample_elbo(inputs=datapoints,</span><br><span class="line">                           labels=labels,</span><br><span class="line">                           criterion=criterion,</span><br><span class="line">                           sample_nbr=<span class="number">3</span>,</span><br><span class="line">                           complexity_cost_weight=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># loss = criterion(regressor(datapoints), labels)</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(loss.item(), end=<span class="string">'\r'</span>)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'Epoch <span class="subst">{epoch+<span class="number">1</span>}</span> finished!'</span>)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    <span class="built_in">print</span>(regressor(X_test).detach().numpy()[<span class="number">0</span>:<span class="number">3</span>])</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[-1.203408  ]</span><br><span class="line"> [-0.32957393]</span><br><span class="line"> [ 1.9057552 ]]</span><br><span class="line">[[-1.2015951 ]</span><br><span class="line"> [-0.32578745]</span><br><span class="line"> [ 1.9102848 ]]</span><br><span class="line">[[-1.2165544 ]</span><br><span class="line"> [-0.33425388]</span><br><span class="line"> [ 1.9049363 ]]</span><br></pre></td></tr></tbody></table></figure><p>可以看到，三次运行输出的结果都是不一致的，通过以下命令我们可以获得预测的均值和标准差：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">preds = [regressor(X_test) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>)]</span><br><span class="line">preds = torch.stack(preds)</span><br><span class="line">means = preds.mean(axis=<span class="number">0</span>).detach().numpy().squeeze()</span><br><span class="line">stds = preds.std(axis=<span class="number">0</span>).detach().numpy().squeeze()</span><br></pre></td></tr></tbody></table></figure><p>这里需要注意，<u>对于表格数据先需进行标准化，具体原因见注意事项</u>。</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>以上就是使用 blitz 实现贝叶斯神经网络的整体流程，同样地，其他的网络架构可以通过将 nn 层替换为对应的贝叶斯层实现：</p><ul><li>BayesianLinear</li><li>BayesianConv1d</li><li>BayesianConv2d</li><li>BayesianConv3d</li><li>BayesianLSTM</li><li>BayesianGRU</li><li>BayesianEmbedding</li></ul><p>使用贝叶斯神经网络时，有几点需要注意：</p><ol><li>新增的超参数中，<code>complexity_cost_weight</code> 非常重要，<strong>该值的选择可能会极大影响模型的表现</strong>。当模型越复杂时，该值应当越低。</li><li>要获得结果的不确定性需多次运行模型，这<strong>对于模型过于复杂的任务来说可能极为耗时</strong>。因此需综合考虑自身需求和时间成本。</li><li>对于计算机视觉领域中的图片分类问题等，不需要对输入数据进行特别处理。但对于结构化数据（例如上述的 <code>fetch_california_housing</code> 数据集），需要注意特征的尺度是否一致，如果不一致则需要对其进行<strong>标准化处理</strong>。由于贝叶斯层在初始化时，对于每个权重的处理是一致的，因此特征的尺度不同将极大影响后续权重的更新，例如尺度更小的特征可能得到更大的权重，同时相应的复杂度惩罚也就越严重。对于图片分类，每个通道的尺度相同，因此不存在这方面的问题。</li></ol><h2 id="一些有趣的问题"><a href="#一些有趣的问题" class="headerlink" title="一些有趣的问题"></a>一些有趣的问题</h2><ul><li>问 ChatGPT 相同的问题时，它会给出不同的回答。这是为什么？是因为它的模型架构中引入了贝叶斯方法吗？</li><li>除了通过对权重进行采样引入正则化以外，贝叶斯神经网络还在其他哪些地方体现出了可以提升模型表现的性质？</li><li>你认为分类问题更需要贝叶斯神经网络还是回归问题更需要？为什么？</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li>Robustness of Bayesian Neural Networks to White-Box Adversarial Attacks</li><li>Bayesian Bilinear Neural Network for Predicting the Mid-price Dynamics in Limit-Order Book Markets</li></ul></body></html>]]></content>
    
    
    <summary type="html">本文将介绍贝叶斯神经网络相较于传统神经网络有什么优势以及如何通过 Python 实现，还有一些使用时的注意事项。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="深度学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>dN/dS 的计算及其核心理念</title>
    <link href="https://biojuse.com/2023/12/30/dNdS%20%E7%9A%84%E8%AE%A1%E7%AE%97%E5%8F%8A%E5%85%B6%E6%A0%B8%E5%BF%83%E7%90%86%E5%BF%B5/"/>
    <id>https://biojuse.com/2023/12/30/dNdS%20%E7%9A%84%E8%AE%A1%E7%AE%97%E5%8F%8A%E5%85%B6%E6%A0%B8%E5%BF%83%E7%90%86%E5%BF%B5/</id>
    <published>2023-12-30T12:30:00.000Z</published>
    <updated>2024-08-13T10:00:48.639Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p><strong>dN/dS (ω)</strong> 是一个在进化生物学领域常出现的词，也是我最近分析的时候着重关注的一个对象。这几天翻了一些文献和书籍，对它的计算和设计理念有了一个全新的了解，所以有了这篇博客。</p><p>在阅读这篇文章前，你可能需要一定的相关知识基础，例如，你应该已经知道以下概念：</p><ul><li>密码子、简并位点、同义突变、非同义突变。</li><li>中性理论、正选择、纯化选择。</li></ul><h2 id="dN-dS-是什么"><a href="#dN-dS-是什么" class="headerlink" title="dN/dS 是什么"></a>dN/dS 是什么</h2><p>这里的 S 即 <strong>Synonymous</strong>，N 即 <strong>Nonsynonymous</strong>。</p><p>dN 指的是<strong>非同义替换率</strong>，dS 指的是<strong>同义替换率</strong>。但我认为另一个知乎博主文章里的说法会更加直观一些：</p><ul><li>dN: 每个<strong>非同义位点</strong>上发生的非同义突变的数量。</li><li>dS: 每个<strong>同义位点上</strong>发生的同义突变的数量。</li></ul><h3 id="计算原理"><a href="#计算原理" class="headerlink" title="计算原理"></a><strong>计算原理</strong></h3><p>首先，简单来说，dN 和 dS 的计算涉及到四个变量，分别为<strong>同义突变数量（S）、非同义突变数量（N）以及同义位点数量（Ssite）和非同义位点数量（Nsite）</strong>。</p><img src="/pic2/dndscalcu.png" height="100px"><p>前两个变量 N 和 S 非常好理解，所以这里需要重点关注的是后两个变量 Nsite 和 Ssite，即每种类型的位点数量是什么？它们是如何得到的？但在这之前，我们先简单回顾一下一些常见的突变类型。</p><h3 id="突变类型"><a href="#突变类型" class="headerlink" title="突变类型"></a>突变类型</h3><p>我们都知道，一个密码子在发生突变时，其性质可能发生以下几种变化，以密码子 TTT 为例（其编码苯丙氨酸 Phe）：</p><ul><li><strong>错义突变</strong>：由编码一个氨基酸变为编码另一个氨基酸，例如 TTT(苯丙氨酸)&gt;ATT(异亮氨酸)。</li><li><strong>同义突变</strong>：突变过后，仍编码相同氨基酸，例如 TTT(苯丙氨酸)&gt;TTC(苯丙氨酸)。同义突变通常发生在密码子的第三个碱基上（有时第一个碱基也可以，例如 AGA&gt;CGA 皆编码精氨酸），此时我们称这个位点具有简并性。需要注意的是，在某些情况下，<strong>终止密码子突变为终止密码子</strong>（例如 TAA&gt;TAG）<strong>也被视为一种同义突变</strong>。</li></ul><p>除了这一些可能的突变以外，还有一些其他的情况，例如，TAT(酪氨酸) 可能变为一个终止密码子（TAA），这时我们称其为<strong>无义突变</strong>，在某些情景下，这也可以被称为<strong>截断突变</strong> (protein-truncating variant，PTV)。</p><p>以上突变类型并不涵盖全部情况，但这些突变是大多 dN/dS 算法的关注对象。</p><p>部分情况下，错义突变和无义突变会<strong>被统一视为非同义突变</strong>，当然在一些算法中，无义突变并不被考虑。这也取决于具体的研究目的和需要。</p><img src="/pic2/maxresdefault.jpg" height="400px"><h3 id="位点类型"><a href="#位点类型" class="headerlink" title="位点类型"></a>位点类型</h3><p>现在我们知道，一个位点的突变可能带来的结果也不同，这也是计算 Ssite 和 Nsite 的关键所在。</p><p>首先对于任何情况下的 Nsite 和 Ssite 计算结果，我们都有：</p><img src="/pic2/cds_ns.png" height="100px"><p>例如，对于一个长度为 3 的密码子，它所计算出来的 Nsite 和 Ssite 总和也必定为 3。</p><p>接下来就是计算过程，最简单的计算即<strong>遍历每个位点的每种突变情况可能导致的突变类型</strong>，对于 TTT，我们可以列举出它的所有可能突变情况，这里直接用杨子恒《计算分子进化》中的一张图作为示例：</p><img src="/pic2/ttt.png" height="500px"><p>我们先仅考虑 <code>k=1</code> 一栏，<code>k=2</code> 在后文中我们会拓展讨论。</p><p>其中，对于一个密码子的每一个位点，它都可以突变成除它自身之外的碱基，因此每个位点都有三个 <code>Target codon</code>，其中同义突变的情况仅有 <code>TTT&gt;TTC</code>，而其他都为非同义突变。</p><p>因此，这里共有 9 个情况，其中同义突变占 1 个，非同义突变占 8 个。因此，同义位点数量为 <code>1/9 * 3</code>（3 即密码子长度），非同义位点数量为 <code>8/9 * 3</code>。最后则分别为表中的 1/3 和 8/3。</p><h3 id="突变计数及-dN-dS-计算"><a href="#突变计数及-dN-dS-计算" class="headerlink" title="突变计数及 dN/dS 计算"></a>突变计数及 dN/dS 计算</h3><p>知道位点数量后，我们就可以计算了，依然以一个最简单的情况作为示例，假设我们有一个包含九个 <code>TTT</code> 的 CDS：</p><ul><li>TTTTTTTTTTTTTTTTTTTTTTTTTTT</li></ul><p>通过计算我们可以知道，它具有 3 个同义位点和 24 个非同义位点，假设这里在两个密码子上我们观察到了一个同义突变和一个非同义突变：</p><ul><li>TT<strong>C</strong>TTTTTTTTTTTTTTTTTTTTTT<strong>A</strong>T</li></ul><p>那么此时，其 dN = 1/24，dS = 1/3，计算得到 dN/dS = 1/8。</p><p>当然，在突变计数步骤中也可能出现更加复杂的情况，例如同一个密码子上出现了两个突变，例如 TTT 变为了 TCA，这种情况就要考虑多条进化途径：</p><ul><li>TTT &gt; TTA &gt; TCA（苯丙氨酸 &gt; 亮氨酸 &gt; 丝氨酸），这条途径涉及到两个非同义突变。</li><li>TTT &gt; TCT &gt; TCA（苯丙氨酸 &gt; 丝氨酸 &gt; 丝氨酸），这条途径涉及到一个非同义突变和一个同义突变。</li></ul><p>对于这种情况，常见的做法有对不同的途径赋予合适的权重等，更多细节如果看客感兴趣可自行深入了解。</p><h3 id="dN-dS-应该是多少"><a href="#dN-dS-应该是多少" class="headerlink" title="dN/dS 应该是多少"></a>dN/dS 应该是多少</h3><p>在最简单的 dN/dS 计算中（前文中的 <code>k=1</code>），其中有一个隐性的假设：我们认为每一个位点上每一个突变情况发生的概率都是<strong>相等</strong>的。</p><p>那么，我们继续以 TTT 作为例子，如果每一个突变情况发生的概率都是<strong>相等</strong>的，理想情况下（<strong>无选择压力</strong>）我们会获得的 dN/dS 值会是多少？</p><p>很简单，发生同义突变的概率是 1/9，发生非同义突变的概率是 8/9，假设发生突变的数量为 x，则通过计算和约分后我们可以发现，<strong>dN/dS 值应该是 1</strong>。</p><p>当然，在现实条件中，每一个突变情况发生的概率并不一定相等，例如，转换（transition）发生的概率就要比颠换（transversion）更高，而很多同义突变都是转换事件，因此如果我们忽略转换和颠换之间的发生概率差异，就会导致 Ssite 低估和 Nsite 的高估，从而导致计算的 dN/dS 偏低。</p><p>对于上面这种情况，一种可行的方法就是在计算转换事件时，给它提供的位点数量进行相应加权，也就有了这个表格中 <code>k=2</code> 的情况：</p><img src="/pic2/ttt.png" height="500px"><p>相似的事情还发生在<strong>密码子偏好</strong>等因素上，而很多方法也针对这些点进行了改进。例如<a href="https://www.sciencedirect.com/science/article/pii/S0092867417311364">一篇 17 年的 cell 文章中</a>，作者使用 192 个 rate parameters (包括不同的替换类型和链属性，有些类似 3-mer 的突变率模型) 来计算 dN/dS，并发现其他方法要么高估要么低估了 dN/dS。</p><p>所以，dN/dS 有一个很关键的设计理念，就是<strong>在完全没有选择的情况下，这个值应当等于 1</strong>，即同义替代率和非同义替代率应当是相等的。这个概念对于后续的一系列引申也很重要。</p><h2 id="dN-dS-有什么用"><a href="#dN-dS-有什么用" class="headerlink" title="dN/dS 有什么用"></a>dN/dS 有什么用</h2><p>现在我们能够知道，如果完全考虑了不同替换发生概率的差异并针对它们矫正 Nsite 和 Ssite 的计算，那么 dN/dS 在没有选择的情况下应该是等于 1 的。</p><p>在很多地方我们也可以看到这么一个结论：</p><ul><li>dN/dS &lt; 1，表明存在<strong>纯化选择</strong>。</li><li>dN/dS = 1，表明处在<strong>中性选择</strong>下。</li><li>dN/dS &gt; 1，表明存在<strong>正选择</strong>。</li></ul><p>也就是说，dN/dS 还可以帮助我们确认一个基因所处的状态，并据此推断出其选择的机制，为我们理解其进化过程提供信息。</p><p>那么为什么说 dN/dS = 1 时，基因处于中性选择下呢？这里首先我们要明确一个概念，<strong>中性选择</strong>并不意味着不存在选择，只是它对生物体的适应性和生存几率没有正面或负面的影响，因此它并不因自然选择而频率增加或下降，在种群内的频率变化全由遗传漂变决定。当然为了方便理解，可以将其视为和 “没有选择” 等效。</p><p>同样，中性（neutral）的概念也是如此，一个中性突变就是对生物既无益处也无害处的突变（亦或是带来的益害相抵消）。</p><p>然后要明确一个概念，<code>dN/dS = 1</code> 推导出中性选择的前提假设是 —— <strong>同义突变都是中性的</strong>。</p><p>不妨设想一下，如果同义突变和非同义突变都受到相同强度的强纯化选择，导致其突变数量减少 50%，那么最后算出来的 dN/dS 依然等于 1。</p><p>那么这个假设到底成不成立呢？很显然不成立。这个假设主要的根基在于<strong>同义突变并不会改变蛋白质的组成和结构</strong>，因此从蛋白的角度出发它不会对生物体的表型产生影响。但是在转录翻译阶段，已经有很多研究发现，<strong>不同密码子就算编码同一氨基酸，也会存在效率和质量上的不同</strong>（这也是密码子偏好的原因之一），所以大多数同义突变其实归根结底是<strong>有害</strong>的（可见 <a href="https://www.nature.com/articles/s41586-022-04823-w">Shen et al. 2022, Nature</a>）。</p><p>但是这影不影响我们使用同义突变作为中性的 Proxy？其实这是个很有争议的问题，因为即使同义突变有害，也很难去量化同义突变到底 “多有害”。今年 4 月份，针对得出 “大多数同义突变都有害” 的文章有人产生了<a href="https://www.nature.com/articles/s41586-023-05865-4">质疑</a>。同样，很多其他的研究表明同义突变可能产生影响的概率比非同义小得多。</p><p>违反一个假设的严重性，一要看<strong>违反的程度</strong>，二要看<strong>理论对此的鲁棒性</strong>。就以我个人目前进行的分析来看，同义突变受到的影响应该较为轻微。从各种使用 dN/dS 进行选择推断的分析来看，似乎其结果都 “合乎情理”。当然，具体情况如何也有待更详尽的探讨。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实这篇博客最重要的点还是理清为什么会以 1 这个值作为 dN/dS 的 “分界线”，其实理解了 dN/dS 的计算目的，也就能明白这么做的理由。</p><p>这里再总结一下该文章中的一些重点：</p><p>①、计算 Nsite 和 Ssite 时，对于突变率高的突变类型，需要对其进行一定的加权以平衡其对 dN 和 dS 计算的贡献。以往的计算中，<code>转换-颠换比率</code> 和 <code>密码子偏好</code> 等都是基于该点考虑对计算进行修正。</p><p>②、基于突变发生概率矫正后计算的 dN/dS 可以作为选择情况的参考。如果同义突变的中性假设是正确的或者几近正确的，则当 dN/dS 等于 1 时我们可以认为该蛋白编码基因处于中性选择下。</p><p>如果你对代码更感兴趣，并且想知道具体的计算步骤，可以尝试看一看 Biopython 中相应脚本的源码：</p><p><a href="https://github.com/biopython/biopython/blob/master/Bio/codonalign/codonseq.py">https://github.com/biopython/biopython/blob/master/Bio/codonalign/codonseq.py</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li>Ziheng Yang, Computational Molecular Evolution.</li><li>Martincorena et al., Universal Patterns of Selection in Cancer and Somatic Tissues.</li><li>Shen et al., Synonymous mutations in representative yeast genes are mostly strongly non-neutral.</li><li>Dhindsa et al., A minimal role for synonymous variation in human disease.</li></ol></body></html>]]></content>
    
    
    <summary type="html">我们需要知道 dN/dS 是怎么算的，也需要知道它为什么这么算。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="杂项" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E6%9D%82%E9%A1%B9/"/>
    
    
  </entry>
  
  <entry>
    <title>slurm 作业数组的正确使用姿势</title>
    <link href="https://biojuse.com/2023/12/13/slurm%20%E4%BD%9C%E4%B8%9A%E6%95%B0%E7%BB%84%E7%9A%84%E6%AD%A3%E7%A1%AE%E4%BD%BF%E7%94%A8%E5%A7%BF%E5%8A%BF/"/>
    <id>https://biojuse.com/2023/12/13/slurm%20%E4%BD%9C%E4%B8%9A%E6%95%B0%E7%BB%84%E7%9A%84%E6%AD%A3%E7%A1%AE%E4%BD%BF%E7%94%A8%E5%A7%BF%E5%8A%BF/</id>
    <published>2023-12-13T13:00:00.000Z</published>
    <updated>2024-02-27T09:49:57.284Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>最近一次想要批量运行任务前试着运行了一个个例，发现占用的内存意外的比较多，所以一口气把所有提交到服务器上显然不太妥当。由于之前在服务器上用 slurm 仅限于 <code>srun</code> 而从来没有尝试过 <code>sbatch</code>，所以就对这方面的知识进行了一番恶补，正好也能补充十二月份的博客内容。</p><h3 id="如何确定内存是否充足"><a href="#如何确定内存是否充足" class="headerlink" title="如何确定内存是否充足"></a>如何确定内存是否充足</h3><p>对于当前节点，使用 <code>top</code> 和 <code>htop</code> 就能得到相关的信息，但对于其他节点，如果没有登录权限，那么这种实时监控的手段就没法派上用场了。</p><p>这时候，可以使用 <code>free -h</code>  命令将节点在该时间点的内存使用情况信息打印出来：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">srun -w nodexx free -h</span></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           487G        144G        2.7G        162M        340G        340G</span><br><span class="line">Swap:           49G        685M         49G</span><br></pre></td></tr></tbody></table></figure><p>重点关注 available 部分，可以看到当前该节点还是剩余了非常多可用内存的。不过并不是内存越多可以运行的命令就越多，这也跟剩余的空闲 CPU 数有关，关于剩余的 CPU 数可以直接使用 <code>scontrol show node nodexx </code> 来查看：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">scontrol show node nodexx | grep <span class="string">'CPUTot'</span></span></span><br><span class="line">   CPUAlloc=45 CPUTot=52 CPULoad=75.96</span><br></pre></td></tr></tbody></table></figure><p>可以看到已经分配的 CPU 有多少个（CPUAlloc）。</p><h3 id="如何确定任务要多少内存"><a href="#如何确定任务要多少内存" class="headerlink" title="如何确定任务要多少内存"></a>如何确定任务要多少内存</h3><p>可以先只投递单个任务，若该任务投递到当前终端所在节点，则直接使用 <code>top -u username</code> 就能进行查看。</p><p>如果该任务投递到其他节点上，那么可以使用 <code>ps</code> 命令查看相关情况：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">srun -w nodexx ps -u username -o pid,vsz,rss,<span class="built_in">comm</span></span></span><br><span class="line">   PID    VSZ   RSS COMMAND</span><br><span class="line">  6382 34042040 9706368 xxxx</span><br></pre></td></tr></tbody></table></figure><p>这里我们可以着重关注 RSS（常驻内存集大小，即进程实际使用的物理内存大小），该例子中这个命令占用了约 9.7 GB（9706368 KB） 的内存，以此为准，若我们剩余了 100 GB 的内存，那就还能支撑我们运行大约十个这样的命令（理想情况）。但实际情况肯定没那么简单，所以最好留出一些余量以避免性能下降或内存交换。</p><h3 id="使用-sbatch-提交作业"><a href="#使用-sbatch-提交作业" class="headerlink" title="使用 sbatch 提交作业"></a>使用 sbatch 提交作业</h3><p>确定了自己大约还能投多少任务后，就可以针对其编写相关的 shell 脚本了。</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --job-name=xxx</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --output=xxx_%A_%a.out</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --error=xxx_%A_%a.err</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --partition=xxx</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --ntasks=1</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --array=1-10%5</span></span><br><span class="line"></span><br><span class="line">FILES=(*txt)</span><br><span class="line">INDEX=$(($SLURM_ARRAY_TASK_ID - 1))</span><br><span class="line">FILE=${FILES[$INDEX]}</span><br><span class="line"></span><br><span class="line">echo $FILE</span><br></pre></td></tr></tbody></table></figure><p>以上述例子为例，以下几个标头较为重要：</p><ul><li><p><code>#SBATCH --output=xxx_%A_%a.out</code> 这里的 <code>%A</code> 和 <code>%a</code> 分别表示作业数组的 ID 和数组中任务的索引，添加后不同作业的 out 和 err 会分开产生而不会混在一起。</p></li><li><p><code>#SBATCH --partition=xxx</code> 该处的填写可根据 <code>sinfo</code> 的信息进行，例如：</p></li></ul><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sinfo</span></span><br><span class="line">PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST</span><br><span class="line">all*         up   infinite      3    mix nodexx,nodexx,nodexx</span><br></pre></td></tr></tbody></table></figure><p>此时可填写 <code>all</code>，slurm 会根据该 partition 中 node 的使用情况自动分配任务。如果没有 partition 可以换成 <code>#SBATCH --nodelist=xxx</code> 填写想要投递的节点名称。</p><ul><li><code>#SBATCH --array=1-10%5</code> 这行定义了作业数组的范围（1-10，共十个）和同时运行的作业数量（%5），这里还有其他的设计技巧，例如：<ul><li><code>1-100:10</code> 表示从 1 到 100，每隔 10 个一个作业。</li><li><code>1,2,5-7</code> 表示作业 1, 2, 5, 6, 和 7。</li><li>slurm 为每个作业数组提供了特定的环境变量，如示例脚本中的 <code>$SLURM_ARRAY_TASK_ID</code></li></ul></li></ul><p>以上脚本会按照以下逻辑进行：</p><ol><li><code>FILE</code> 变量储存当前文件夹下所有 <code>txt</code> 结尾的文件。</li><li>通过 <code>$SLURM_ARRAY_TASK_ID</code> 定义索引变量 <code>INDEX</code> 并调取出 <code>FILE</code> 中对应索引的文件。</li><li>打印出对应文件的文件名（<code>echo</code>），结果将出现在 out 文件中。</li></ol><p>运行该脚本的方法（假设该脚本名字为 <code>test.sh</code>）：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sbatch test.sh</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sbatch --mail-user=xxx@xxx --mail-type=BEGIN,END test.sh <span class="comment"># 开始和结束后发邮件通知</span></span></span><br></pre></td></tr></tbody></table></figure><p>上述代码中，发邮件通知也可以通过在脚本中添加以下代码完成：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --mail-user=xxx@xxx</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SBATCH --mail-type=BEGIN,END</span></span><br></pre></td></tr></tbody></table></figure><p>投递任务后，如果想要关注相关任务的状态，可以使用 <code>squeue -u username</code> 进行。</p><p>当然，如果需要提交的作业不能仅仅通过索引来区分，而是<strong>需要执行完全不同的命令</strong>时，使用作业数组可能不是最合适的选择。这时候对每个任务单独编写脚本并提交更能满足实际需求。</p></body></html>]]></content>
    
    
    <summary type="html">一种使用sbatch进行作业并发的小技巧，同时介绍了如何确定内存使用状况。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="杂项" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E6%9D%82%E9%A1%B9/"/>
    
    
  </entry>
  
  <entry>
    <title>DNA 语言模型 GPN 在突变效应预测中的应用及表现</title>
    <link href="https://biojuse.com/2023/11/24/DNA%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20GPN%20%E5%9C%A8%E7%AA%81%E5%8F%98%E6%95%88%E5%BA%94%E9%A2%84%E6%B5%8B%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E5%8F%8A%E8%A1%A8%E7%8E%B0/"/>
    <id>https://biojuse.com/2023/11/24/DNA%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20GPN%20%E5%9C%A8%E7%AA%81%E5%8F%98%E6%95%88%E5%BA%94%E9%A2%84%E6%B5%8B%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E5%8F%8A%E8%A1%A8%E7%8E%B0/</id>
    <published>2023-11-24T15:00:00.000Z</published>
    <updated>2024-02-29T03:18:47.295Z</updated>
    
    <content type="html"><![CDATA[<html><head></head><body><p>众所周知，语言模型已经在诸多领域得到了广泛应用。受深度学习狂潮影响深远的生物学领域里，出色的语言模型应用不断涌出。以 DeepMind 为例，其所开发的 Alphafold、Alphamissense 等都是基于语言模型实现的。</p><p>这篇文章将分享一个最近发表的 DNA 语言模型（GPN，Genomic Pre-trained Network）和它的变体 GPN-MSA，该模型与之前的 DNABERT 等 DNA 语言模型不同，其使用<strong>单个碱基</strong>作为 token 进行训练，并在无监督的突变效应预测中取得了良好的成效。</p><p>相关论文链接见文章结尾，本文章将重点介绍两个模型的设计和它们之间的区别，并写出一些很值得学习的地方。</p><h3 id="GPN"><a href="#GPN" class="headerlink" title="GPN"></a>GPN</h3><p>GPN 是一种自监督训练的 DNA 语言模型，与 GPN-MSA 不同的地方在于，它可以仅依靠未经过比对的数个基因组序列而不使用多序列比对（MSA）进行训练，此外它使用的为单纯的 DNA 序列而未使用到任何功能基因组信息，因此可以说 GPN 的泛用性是非常强大的，因为对于很多类群中的生物来说，获得跨物种的 MSA 和全面的功能基因组数据并不是容易的事情。</p><p>GPN 的模型设计如下：</p><p><img src="/pic2/pnas.2311219120fig01.jpg"></p><p>该模型的训练架构类似于 Transformer，其中 DNA 的输入遵守如下规则：</p><ul><li>在基因组上使用 <code>512bp</code> 的窗口 &amp; <code>256bp</code> 的步长进行采样（类似于滑动窗口），并使用反向互补序列作为数据增强手段（DNA 语言模型中常见的方法）。</li><li>该采样并不针对全基因组，而是着重选择了特定区域（如外显子、启动子）以及和这些区域等量的其他随机窗口。</li><li>对于重复区域进行了损失权重的调整，以改善模型表现。</li></ul><p>对于输入的 DNA，其随机掩码 15% 的碱基，并传入给类 Transformer 模块。值得注意的是，与 Transformer 不同，这里用空洞卷积替代了多头注意力机制，文章的理由如下：</p><ul><li>卷积神经网络比用注意力机制收敛地更快。</li><li>对于该规模的 DNA 序列而言，卷积神经网络的局部模式识别可能较注意力机制的全局模式识别更加有用。</li><li>卷积的线性复杂度对于其向更长序列的推广是有益的。</li></ul><p>对于最后一点，具体来说，如果我们使用的为卷积神经网络，那么不论是处理 512 碱基还是 1k 个碱基，卷积层对每个碱基的计算成本是固定的。所以，处理后者大概需要的计算量是前者的两倍。</p><p>相比之下，注意力机制涉及到序列中所有元素之间的关系，其复杂度是<strong>序列长度的平方</strong>。在处理长序列时，这会导致计算成本急剧增加。为了处理长序列，模型可能需要将序列分成更小的块，并分别对它们进行处理，然后再将结果整合起来。这种分块处理通常需要在块之间有重叠部分，以捕捉块边界处的上下文信息，这使得整个处理过程变得复杂。</p><p><img src="/pic2/006C3FgEgy1gsqyd2q43kj60tj09iwlm02.jpg"></p><p>输入的掩码序列经过 25 个类 Transformer 块后被处理成包含上下文信息的 embedding，该 embedding 则被输入到后续的分类层中计算特定位置上四种碱基的概率，模型的损失函数为预测为正确碱基的概率的负 log 值。</p><p>此后，Benegas 等人利用该模型进行了一系列的分析工作，所得结果大致如下：</p><ul><li>在无监督情况下，模型也能有效识别部分基因组区域。使用 embedding 训练逻辑斯蒂回归分类模型后，在 CDS 上实现了高准确率（96%），但对于 ncRNA 和基因间区，模型的表现较为糟糕（51% 和 67%）。</li></ul><div class="hide-block"><button type="button" class="hide-button" style="">图 2    </button><div class="hide-content"><p><img src="/pic2/pnas.2311219120fig02.jpg"></p></div></div><ul><li>通过单独掩盖基因组上各个位置的碱基，模型鉴定出了一些可以在数据库中匹配到的基序和一些尚未匹配但在其他研究中有被提及或者序列特征较特殊的基序。</li></ul><div class="hide-block"><button type="button" class="hide-button" style="">图 3    </button><div class="hide-content"><p><img src="/pic2/pnas.2311219120fig03.jpg"></p></div></div><ul><li>通过以 <code>P(突变碱基)/P(参考碱基)</code> 的 <code>log</code> 值作为突变效应预测的分数，模型实现了良好的预测性能，对于那些因为与 functional variant 连锁的 neutral variants，模型也能实现有效的判别。</li></ul><div class="hide-block"><button type="button" class="hide-button" style="">图 4 5 6    </button><div class="hide-content"><p><img src="/pic2/fig456.png"></p></div></div><p>可以看出，GPN 的模型设计其实也并不复杂，但这里采取的一些方法令人较为受益。例如，不同于以往的研究，GPN 将注意力机制换为传统的卷积层并在单个基因组上取得了良好的学习效果。此外 GPN 使用了一种巧妙的无监督方法，将预测的不同碱基概率之比巧妙地转换为 variant effect 的 score。虽然这些想法来源自不同的研究，但具体的实现也要研究者巧妙地把它们组合起来。</p><h3 id="GPN-MSA"><a href="#GPN-MSA" class="headerlink" title="GPN-MSA"></a>GPN-MSA</h3><p>如果说 GPN 是一个轻巧的、适用于多数非模式物种的工具，那么 GPN-MSA 就是专门设计用于人类基因组的语言模型。与 GPN 不同，GPN-MSA 舍弃了卷积层重回正统的 Transformer 架构（RoFormer），同时它的输入数据也由单个基因组变成了九十种脊椎动物的 MSA。</p><p>不难看出，在这种情况下，更关注全局信息的 Transformer 会有更好的表现。这也告诉我们，对于不同的数据，灵活修改并使用合适的模型架构是非常重要的。</p><p><img src="/pic2/F1.large.jpg"></p><p>它的数据处理大致如下：</p><ul><li>MSA 处理，其首先下载了一百种脊椎动物的多序列比对，对其进行一定处理后，去除了与人类亲缘关系最接近的十种灵长类动物。</li><li>训练输入的数据为长度 <code>128bp</code> 步长 <code>64bp</code> 的窗口，与 GPN 相同，这里有选择性地对窗口进行了选择：①、专注于功能重要的区域，通过保守性分数 PhastCons 选择所有窗口中平均保守性分数最高的 5% 和剩余窗口随机选择 0.1%；②、降低了重复序列的权重并提高保守序列的权重（同样通过 PhastCons 进行）。</li><li>该模型同样使用反向互补序列作为数据增强手段，同样掩码 15% 的核苷酸进行训练。与 GPN 不同的是，该模型在数据增强中，还将非保守区域的参考碱基随机替换为其他碱基（在计算 loss 时），此外该模型使用 21 号染色体作为早停法的测试 loss 参考，且保留 22 号染色体作为进一步的测试（但在论文中并未使用到）。</li></ul><p>去除灵长类动物的原因文章并没有详细提及，但是可能原因也较好理解。首先这些生物与人类的基因组序列相似性可能很高，因此它们可能没法突出那些更广泛进化范围内保守的区域。此外，高度的相似性也意味着数据的冗余信息可能较多，因此去除将有利于模型更有效地提取特征。</p><p>另外，文章还进行了消融实验，以确定不同的模型架构对于最终模型表现的影响，具体的尝试可见论文的方法部分，此处贴出原文内容：</p><blockquote><ul><li>w/o MSA: the model is only trained on the human sequence, without access to other species.</li><li>MSA frequency: variants are scored using the log-likelihood ratio of observed frequencies in the MSA column, with a pseudocount of 1.</li><li>Train on 50% most conserved: expand the training region from the smaller 5% most conserved to a larger set with less overall conservation.</li><li>Include closest primates: do not filter out from the MSA the 10 primates closest to human.</li><li>Don’t upweight conserved: do not upweight the loss function on conserved elements.</li><li>Don’t replace non-conserved: do not replace the reference in non-conserved positions with random nucleotides when computing the loss function.</li></ul><p>Modeling the single human sequence instead of the MSA has by far the biggest impact. Using the column-wide MSA frequencies as predictor also shows a large decrease in performance. Including primate species close to human, or training on less conserved regions, have a moderate impact on performance. Finally, of relatively minor impact are removing the upweighting of conserved elements or removing the data augmentation procedure of replacing nucleotides in non-conserved positions.</p></blockquote><p>目前 Biorxiv 论文上研究人员仅将该模型在疾病预测中的表现进行了展示，在突变致病性预测中，GPN-MSA 超越了基于功能基因组数据的 CADD 以及保守性分数 PhyloP，在多个数据集（Clinvar、COSMIC）里取得了 SOTA。</p><p><img src="/pic2/F2.large.jpg"></p><p>当然，GPN-MSA 在利用多序列比对信息的同时，也失去了其在比对质量较差的非编码区上的表现机会，在文章的最后作者提到了一些未来的方向：</p><ul><li>整合群体遗传变异信息，而非依靠单个基因组。</li><li>整合 DNA 序列和功能基因组学信息。</li></ul><p>最后，我个人认为这个工作对我的启发还是非常大的，归根结底这个模型设计其实并不算非常复杂，但是很多思路缺一不可。此外这个架构还可以用在多个其他领域的下游分析中，它还有很多利用价值等待我们挖掘……</p><hr><p>文章链接如下：</p><blockquote><p>DNA language models are powerful predictors of genome-wide variant effects</p><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10622914/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10622914/</a></p><p>GPN-MSA: an alignment-based DNA language model for genome-wide variant effect prediction</p><p><a href="https://www.biorxiv.org/content/10.1101/2023.10.10.561776v1.full">https://www.biorxiv.org/content/10.1101/2023.10.10.561776v1.full</a></p></blockquote></body></html>]]></content>
    
    
    <summary type="html">本文探讨了 DNA 语言模型 GPN 及其变体 GPN-MSA 在预测基因突变效应中的应用和性能。</summary>
    
    
    
    <category term="学习" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="文献阅读" scheme="https://biojuse.com/categories/%E5%AD%A6%E4%B9%A0/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="文献" scheme="https://biojuse.com/tags/%E6%96%87%E7%8C%AE/"/>
    
    <category term="基因组学" scheme="https://biojuse.com/tags/%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6/"/>
    
  </entry>
  
</feed>
